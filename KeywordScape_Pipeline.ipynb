{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8457278b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INTERACTIVE DOCUMENT CREATION RELATED IMPORTS\n",
    "import json\n",
    "import sys\n",
    "import os \n",
    "import subprocess\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sumy.summarizers.text_rank import TextRankSummarizer\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from pybtex.database.input import bibtex\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4160d169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLASS RELATED IMPORTS\n",
    "from abc import ABC, abstractmethod\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Any\n",
    "import unittest\n",
    "import math\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6397094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAP CREATION RELATED IMPORTS\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import spacy\n",
    "import umap \n",
    "import hdbscan\n",
    "import matplotlib.pyplot as plt\n",
    "from bertopic import BERTopic\n",
    "import transformers\n",
    "# IMPORTANT: ERROR FIX FOR huggingface transformers -> disable iprogress logging -> https://stackoverflow.com/questions/66644432/use-huggingface-transformers-without-ipywidgets\n",
    "import logging\n",
    "transformers.logging.get_verbosity = lambda: logging.NOTSET\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba017f1e",
   "metadata": {},
   "source": [
    "# Core Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6321446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INTERACTIVE DOCUMENT CREATION\n",
    "### pdf folder \n",
    "pdf_files_folder =  './dataset/' \n",
    "# folder to the json file that contains the parsed pdfs -> parsed by allenai science parser library: https://github.com/allenai/science-parse/tree/master/cli\n",
    "allen_ai_parsed_output_json_file_path =  'dataset.json' \n",
    "# command to call the allen ai science parser jar file from the console with the respective path\n",
    "allen_ai_jar_command = 'java -Xmx6g -jar ./pip_stuff/science-parse-cli-assembly-2.0.3.jar' + ' ' + pdf_files_folder + ' ' + '-f' + ' ' + allen_ai_parsed_output_json_file_path\n",
    "# spacy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "# number of keywords per doc, OUT OF USE, we use percentage instead to be adaptive to document size\n",
    "#number_of_keywords_per_document = 20\n",
    "# percentage of document words extracted and used as keywords, we determine the number of keywords by the LENGTH of the document, NOT the number of different words\n",
    "# -> that means, out of a document with a length of 1000 words we extract the 'keyword_percentage_per_document' number of keywords (e.g. 10 percent => 100 keywords)\n",
    "keyword_percentage_per_document = 5 # 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "312676df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAP CREATION \n",
    "allenai_specter_model_dir = \"\" # SET YOUR PATH HERE\n",
    "allenai_scibert_model_dir = \"\" # SET YOUR PATH HERE\n",
    "\n",
    "paragraph_max_length = 350 \n",
    "# preprocessed interactive_document_corpus file path for MAP CREATION\n",
    "interactive_document_corpus_full_file_path = 'corpus_full.txt'\n",
    "# preprocessed interactive_document_corpus file path for FRONTEND USAGE\n",
    "interactive_document_corpus_file_path = 'corpus.txt' \n",
    "# path for the base map points\n",
    "interactive_document_corpus_base_map_points_file_path =  'corpus_base_map_points.txt'\n",
    "# path for the extracted points of the document corpus \n",
    "interactive_document_corpus_corpus_points_file_path = 'corpus_points.txt'\n",
    "# path for the topic_corpus which is the extracted topic model\n",
    "interactive_document_corpus_topic_corpus_file_path = 'topic_corpus.txt'\n",
    "# DEPRECATED -> now done all in one in the main corpus.txt files\n",
    "# path for the extracted points for the paragraph vectors\n",
    "#interactive_document_corpus_paragraph_map_points_file_path = 'paragraph_points.txt'\n",
    "# path for the topic points of the topic map\n",
    "#interactive_document_corpus_topic_map_points_file_path =  'topic_map_points.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5dc548c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUSTOM VARIABLES\n",
    "# recovery news data set for covid 19 news comparison, from here: https://github.com/apurvamulay/ReCOVery\n",
    "recovery_news_data_csv_file_path = 'datasets/covid_news_dataset/recovery-news-data.csv' \n",
    "\n",
    "# STOPWORDS LIST\n",
    "STOPWORDS_LIST = [\"et\", \"al\", \".\", \"k\", \"t\", \"m\", \"r\", \"p\", \"qa\"] # scientific document specific stopwords to be ignored in keyword extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0835862",
   "metadata": {},
   "source": [
    "# Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b3ac1d",
   "metadata": {},
   "source": [
    "## Interactive Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "146efed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENTITY\n",
    "class InteractiveDocument(BaseModel):\n",
    "    # Object Attributes\n",
    "    uri: str = None\n",
    "    title: str = None\n",
    "    timestamp: str = None\n",
    "    rating: str = None\n",
    "    annotation: str = None\n",
    "    tags: List[str] = None\n",
    "    authors: List[str] = None\n",
    "    summary: str = None\n",
    "    keywords: List[str] = None\n",
    "    document_vector: List[int] = None\n",
    "    paragraphs: List[Any] = []\n",
    "    content: dict = {}\n",
    "    # additional specific properties\n",
    "    doi: str = None\n",
    "        \n",
    "# REPOSITORY\n",
    "class InteractiveDocumentRepository(ABC):\n",
    "    @abstractmethod\n",
    "    def test(self):\n",
    "        pass\n",
    "    \n",
    "# REPOSITORY IMPLEMENTATION\n",
    "class InteractiveDocumentRepositoryImpl(InteractiveDocumentRepository):\n",
    "    # Constructor / if needed initialize iunstance attributes here\n",
    "    def __init__(self, interactive_document_data_object: InteractiveDocument):\n",
    "        self.data_object = interactive_document_data_object\n",
    "        print('{0} initialized.'.format(self.__class__.__name__))\n",
    "    \n",
    "    # methods\n",
    "    def test(self):\n",
    "        print('test called.')\n",
    "\n",
    "# TEST\n",
    "class TestInteractiveDocument(unittest.TestCase):\n",
    "\n",
    "    @classmethod\n",
    "    def setUpClass(self):\n",
    "        # set all things up for the test series here\n",
    "        pass\n",
    "\n",
    "    @classmethod\n",
    "    def tearDownClass(self):\n",
    "        # tear everything down after testing here\n",
    "        pass\n",
    "\n",
    "    def test_class_setup_and_serialization(self):\n",
    "        # given\n",
    "        interactive_document_data_object = InteractiveDocument()\n",
    "        interactive_document_repository_impl = InteractiveDocumentRepositoryImpl(interactive_document_data_object)\n",
    "        # when\n",
    "        interactive_document_repository_impl.test()\n",
    "        print(interactive_document_repository_impl.data_object.dict())\n",
    "        # then\n",
    "        result = 6\n",
    "        self.assertEqual(result, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e87a57",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4e23f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENTITY ---------------------------------------------------------------------------\n",
    "class PreprocessingManager(BaseModel):\n",
    "    '''\n",
    "    PreprocessingManager: class that enables all preprocessing \n",
    "    '''\n",
    "    def __init__(self):\n",
    "        print('preprocessing manager constructed.')\n",
    "\n",
    "# REPOSITORY ----------------------------------------------------------------------\n",
    "class PreprocessingManagerRepository(ABC):\n",
    "    @abstractmethod\n",
    "    def parse_pdfs_to_interactive_documents(pdf_folder_path):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def create_summary(interactive_document):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def extract_keywords(interactive_document, keyword_percentage_per_document):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def preprocess_raw_text(interactive_document):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def write_interactive_document_corpus_to_file(interactive_document_corpus, file_path):\n",
    "        pass\n",
    "    \n",
    "# REPOSITORY IMPLEMENTATION -------------------------------------------------------     \n",
    "class PreprocessingManagerRepositoryImpl(PreprocessingManagerRepository):\n",
    "    # Constructor / if needed initialize iunstance attributes here\n",
    "    def __init__(self, preprocessing_manager_data_object: PreprocessingManager):\n",
    "        self.data_object = preprocessing_manager_data_object\n",
    "        self.text_processor = spacy.load('en_core_web_sm')\n",
    "        print('{0} initialized.'.format(self.__class__.__name__))\n",
    "    \n",
    "    # override methods\n",
    "    #@override\n",
    "    def create_summary(self, interactive_document):\n",
    "        print('test')\n",
    "        \n",
    "    #@override\n",
    "    def extract_keywords(self, interactive_document, keyword_percentage_per_document, method=\"textrank\", must_have_keyword=None):\n",
    "        preprocessed_text = interactive_document.content['preprocessed_text']\n",
    "        if(method==\"textrank\"):\n",
    "            tr4w = TextRank4Keyword()\n",
    "            tr4w.analyze(preprocessed_text, candidate_pos = ['NOUN', 'PROPN', 'VERB'], window_size=4, lower=True, stopwords=STOPWORDS_LIST) # window_size=5\n",
    "            # get the number of different words in this document\n",
    "            vocab = tr4w.vocab\n",
    "            vocab_length = len(vocab)\n",
    "            #print(vocab)\n",
    "            #print(vocab_length)\n",
    "            number_of_keywords_for_document = int((vocab_length / 100) * keyword_percentage_per_document)\n",
    "            keywords = tr4w.get_keywords(number_of_keywords_for_document, must_have_keyword)\n",
    "        elif(method==\"rake\"):\n",
    "            #number_of_keywords_per_document = 20\n",
    "            #rake_keyword_extractor = Rake(min_length=1, max_length=3)\n",
    "            #rake_keyword_extractor.extract_keywords_from_text(preprocessed_text)\n",
    "            #keywords = rake_keyword_extractor.get_ranked_phrases()\n",
    "            #keywords = ' '.join(keywords)\n",
    "            #keywords = nltk.word_tokenize(keywords)\n",
    "            #keywords = [token.lower() for token in keywords]\n",
    "            # take the n most important keywords\n",
    "            #if (len(keywords) > number_of_keywords_per_document):\n",
    "            #    keywords = keywords[:number_of_keywords_per_document]\n",
    "            keywords = [] # TODO: find other implementation of RAKE algorithm\n",
    "        else:\n",
    "            print('ERROR: no valid method for keyword extraction provided.')\n",
    "        return keywords\n",
    "    \n",
    "    #@override\n",
    "    def preprocess_raw_text(self, interactive_document):\n",
    "        whole_document_text_string = interactive_document.content['raw_text']\n",
    "        # replace any unicode surrogates like: u\"\\u002d\" with the emtpty string \"\", found here: https://stackoverflow.com/questions/59952915/in-python-3-how-do-you-remove-all-non-utf8-characters-from-a-string\n",
    "        whole_document_text_string = whole_document_text_string.encode('utf-8','ignore').decode('utf8')\n",
    "        text = self.text_processor(whole_document_text_string)\n",
    "        cleaned_text = []\n",
    "        for token in text:\n",
    "            if not token.is_space and not token.is_bracket and (token.is_alpha or (token.text.lower()=='.') or (token.text.lower()==',') or (token.text.lower()=='?') or (token.text.lower()==':') or (token.text.lower()=='-')):\n",
    "                cleaned_text.append(token.text.lower())   #(token.lemma_.lower())\n",
    "        return ' '.join(cleaned_text)\n",
    "        \n",
    "    #@override\n",
    "    def parse_pdfs_to_interactive_documents(self, pdf_files_folder_path): \n",
    "        # 1. parse plain pdfs from folder into json objects\n",
    "        command = 'java -Xmx6g -jar /pip_stuff/science-parse-cli-assembly-2.0.3.jar' + ' ' + pdf_files_folder_path + ' ' + '-f' + ' ' + allen_ai_parsed_output_json_file_path\n",
    "        self.parse_pdfs_from_folder_into_json_objects(command)\n",
    "        # 2. load parsed pdf documents from json\n",
    "        #corpus_objects_list = self.load_parsed_pdf_documents_from_json(allen_ai_parsed_output_json_file_path)\n",
    "        # 3. parse the data from the json objects into interactive document objects\n",
    "        #interactive_document_corpus = self.parse_data_from_json_to_interactive_document_object(corpus_objects_list)\n",
    "        interactive_documents = []\n",
    "        return interactive_document_corpus\n",
    "     \n",
    "    #@override\n",
    "    def write_interactive_document_corpus_to_file(self, interactive_document_corpus, file_path, remove_content=False):\n",
    "        print('...writing interactive document corpus to file.')\n",
    "        print('remove_content: {0}'.format(remove_content))\n",
    "        serialized_interactive_document_corpus = []\n",
    "        for i, interactive_document in enumerate(interactive_document_corpus):\n",
    "            # filter out the content (raw_text, preprocessed_text, ...), e.g. to reduce overhead in frontend \n",
    "            if(remove_content):\n",
    "                interactive_document.content = {}\n",
    "            serialized_interactive_document = interactive_document.dict()\n",
    "            serialized_interactive_document_corpus.append(serialized_interactive_document)\n",
    "            #interactive_document_corpus[i] = serialized_interactive_document\n",
    "        with open(file_path, 'w') as output_file:\n",
    "            json.dump(serialized_interactive_document_corpus, output_file)\n",
    "        print('DONE: writing corpus objects list to file completed.')\n",
    "    \n",
    "    # utility methods\n",
    "    # find AllenAi science parser Repo here: https://github.com/allenai/science-parse/blob/master/cli/README.md\n",
    "    #process = subprocess.run(cmd.split(), check=True, stdout=subprocess.PIPE, universal_newlines=True)\n",
    "    #print(process.stdout)\n",
    "    def get_doi_list_from_bibtex_file(self, bibtex_file_path):\n",
    "        print('...extracting doi list from bibtex file.')\n",
    "        parser = bibtex.Parser()\n",
    "        bib_database = parser.parse_file(bibtex_file_path)\n",
    "        # FIX package specific DICT problems => convert specific dict to normal list\n",
    "        # converting the specific bibliography object DICT into a LIST of data objects\n",
    "        dictlist = []\n",
    "        for key, value in bib_database.entries.items():\n",
    "            temp = [key,value]\n",
    "            dictlist.append(temp)\n",
    "        #print(dictlist[0][1].fields['url'])\n",
    "        #print(dictlist[0])\n",
    "        bib_object_list = []\n",
    "        for dict_obj in dictlist:\n",
    "            bib_object_list.append(dict_obj[1].fields)\n",
    "        #print(bib_object_list[0])\n",
    "        doi_list = []\n",
    "        print(bibtex_file_path)\n",
    "        print('len bib_object_list: ' + str(len(bib_object_list)))\n",
    "        for index, bib_object in enumerate(bib_object_list):\n",
    "            #print(bib_data.entries[entry_key])\n",
    "            if('doi' in bib_object):\n",
    "                doi_list.append(bib_object['doi'])\n",
    "            elif('DOI' in bib_object):\n",
    "                doi_list.append(bib_object['DOI'])\n",
    "            #else:\n",
    "                #print('ERROR at document number: {0}'.format(index))\n",
    "        print('len doi_list: ' + str(len(doi_list)))\n",
    "        print('DONE: doi list successfully extracted.')\n",
    "        return doi_list\n",
    "    \n",
    "    def get_url_list_from_bibtex_file(self, bibtex_file_path):\n",
    "        print('...extracting url list from bibtex file.')\n",
    "        parser = bibtex.Parser()\n",
    "        bib_database = parser.parse_file(bibtex_file_path)\n",
    "        # FIX package specific DICT problems => convert specific dict to normal list\n",
    "        # converting the specific bibliography object DICT into a LIST of data objects\n",
    "        dictlist = []\n",
    "        for key, value in bib_database.entries.items():\n",
    "            temp = [key,value]\n",
    "            dictlist.append(temp)\n",
    "        #print(dictlist[0][1].fields['url'])\n",
    "        #print(dictlist[0])\n",
    "        bib_object_list = []\n",
    "        for dict_obj in dictlist:\n",
    "            bib_object_list.append(dict_obj[1].fields)\n",
    "        #print(bib_object_list[0])\n",
    "        url_list = []\n",
    "        print(bibtex_file_path)\n",
    "        print('len bib_object_list: ' + str(len(bib_object_list)))\n",
    "        for index, bib_object in enumerate(bib_object_list):\n",
    "            #print(bib_data.entries[entry_key])\n",
    "            if('url' in bib_object):\n",
    "                url_list.append(bib_object['url'])\n",
    "            elif('URL' in bib_object):\n",
    "                url_list.append(bib_object['URL'])\n",
    "            #else:\n",
    "                #print('ERROR at document number: {0}'.format(index))\n",
    "        print('len url_list: ' + str(len(url_list)))\n",
    "        print('DONE: url list successfully extracted.')\n",
    "        return url_list\n",
    "    \n",
    "    def download_pdfs_from_doi_list(self, doi_list, output_directory):\n",
    "        print('...downloading pdfs from doi list.')\n",
    "        for index, doi in enumerate(tqdm(doi_list)):\n",
    "            html_url = 'https://sci-hub.se/' + str(doi) # scihub alternatives: ...\n",
    "            response = urllib.request.urlopen(html_url)\n",
    "            response_bytes = response.read()\n",
    "            response_html_string = response_bytes.decode('utf8')\n",
    "            response.close()\n",
    "            #print(response_html_string)\n",
    "            soup = BeautifulSoup(response_html_string, 'html.parser')\n",
    "            try:\n",
    "                pdf_url = soup.find('embed', {'id': 'pdf'}).attrs['src'].split('#')[0]\n",
    "                print(pdf_url)\n",
    "                if(pdf_url.startswith('//')):\n",
    "                    preposition = 'https:'\n",
    "                    pdf_url = preposition + pdf_url\n",
    "            except:\n",
    "                try:\n",
    "                    pdf_url = soup.find('iframe', {'id': 'pdf'}).attrs['src'].split('#')[0]\n",
    "                    print(pdf_url)\n",
    "                    if(pdf_url.startswith('//')):\n",
    "                        preposition = 'https:'\n",
    "                        pdf_url = preposition + pdf_url\n",
    "                except:\n",
    "                    continue\n",
    "            #print(pdf_url)\n",
    "            paper_output_file_path = output_directory + 'doi_' + str(index) + '_' + '.pdf'\n",
    "            try: # wrong url problem: unknown url type: '//sci-hub.se/downloads/2019-01-22//82/xinzhao2013.pdf?rand=6107eb6429921' -> fix by replacing start with https://...\n",
    "                urllib.request.urlretrieve(pdf_url, paper_output_file_path)\n",
    "            except Exception as e:\n",
    "                print('DOI paper link not reachable at index ' + str(index))\n",
    "                print(e)\n",
    "                continue\n",
    "        print('DONE: pdfs from doi list successfully downloaded.')\n",
    "    \n",
    "    def download_pdfs_from_url_list(self, url_list, output_directory):\n",
    "        print('...downloading pdfs from url list.')\n",
    "        for index, url in enumerate(tqdm(url_list)):\n",
    "            pdf_url = url + '.pdf'\n",
    "            paper_output_file_path = output_directory + 'url_paper_' + str(index) + '.pdf'\n",
    "            try:\n",
    "                urllib.request.urlretrieve(pdf_url, paper_output_file_path)\n",
    "            except:\n",
    "                print('ERROR: ' + str(url) + ' could not be loaded.')\n",
    "                continue\n",
    "        print('DONE: pdfs from url list successfully downloaded.')\n",
    "    \n",
    "    def parse_pdfs_from_folder_into_json_objects(self, command):\n",
    "        print('...parsing pdfs into json objects.')\n",
    "        process = subprocess.Popen(command.split(), stdout=subprocess.PIPE, universal_newlines=True)\n",
    "        # wait until the process is finished\n",
    "        exit_code = process.wait()\n",
    "        if exit_code == 0:\n",
    "            print(exit_code)\n",
    "            print('DONE: pdf parsing SUCCESSFUL.')\n",
    "        else:\n",
    "            print(exit_code)\n",
    "            print('ERROR: pdf parsing FAILED.')\n",
    "        \n",
    "    # objects are written linewise into the json file, thats why we have to grab them by line\n",
    "    def load_parsed_pdf_documents_from_json(self, allen_ai_parsed_output_json_file_path):\n",
    "        corpus_objects_list = []\n",
    "        with open(allen_ai_parsed_output_json_file_path) as f:\n",
    "            for line in f:\n",
    "                corpus_objects_list.append(json.loads(line))\n",
    "        # printing name of function that called found here: https://stackoverflow.com/questions/5067604/determine-function-name-from-within-that-function-without-using-traceback\n",
    "        print('DONE: loading parsed documents from json completed.')\n",
    "        return corpus_objects_list\n",
    "    \n",
    "    def parse_data_from_json_to_interactive_document_object(self, corpus_objects_list, corpus_doi_list=None):\n",
    "        print('...parsing data from json to interactive document objects.')\n",
    "        interactive_document_corpus = []\n",
    "        for i, raw_source_object in enumerate(corpus_objects_list):\n",
    "            interactive_document = InteractiveDocument()\n",
    "            interactive_document.content['raw_text'] = \"\"\n",
    "            # get additional information to source object\n",
    "            interactive_document.uri = i # DOI_list[i], or in extra variable as seen below\n",
    "            interactive_document.doi = corpus_doi_list[i] if (corpus_doi_list != None) else 'empty'\n",
    "            interactive_document.title = raw_source_object['metadata']['title'] if (str(raw_source_object['metadata']['title']) != 'null' and raw_source_object['metadata']['title'] != None) else 'empty'\n",
    "            interactive_document.authors = raw_source_object['metadata']['authors'] if (str(raw_source_object['metadata']['authors']) != 'null' and raw_source_object['metadata']['authors'] != None) else []\n",
    "            interactive_document.timestamp = raw_source_object['metadata']['year'] if (str(raw_source_object['metadata']['year']) != 'null' and raw_source_object['metadata']['year'] != None) else 'empty'\n",
    "            interactive_document.rating = 0.0\n",
    "            interactive_document.annotation = \"\"\n",
    "            interactive_document.tags = []\n",
    "            interactive_document.paragraphs = []\n",
    "            interactive_document.document_vector = [0,0]\n",
    "            interactive_document.summary = raw_source_object['metadata']['abstractText'] if (str(raw_source_object['metadata']['abstractText']) != 'null' and raw_source_object['metadata']['abstractText'] != None) else ''\n",
    "            # check if the source object contains text sections at all\n",
    "            if(raw_source_object['metadata']['sections'] != None):\n",
    "                # get text of the object by looping through document sections\n",
    "                for j, section in enumerate(raw_source_object['metadata']['sections']):\n",
    "                    if(section['heading'] != None):\n",
    "                        #print(section['heading'])\n",
    "                        # add the title section without leading space\n",
    "                        if(j == 0):\n",
    "                            interactive_document.content['raw_text'] += section['text']\n",
    "                        else:\n",
    "                            # filter out certain sections, e.g. acknowledgement section\n",
    "                            if(section['heading'] == 'Acknowledgements'):\n",
    "                                continue\n",
    "                            interactive_document.content['raw_text'] = interactive_document.content['raw_text'] + \" \" + section['text']\n",
    "            else:\n",
    "                print('INFO: source {0} of json data has no raw text SECTIONS.'.format(i))\n",
    "            # add created interactive document to corpus\n",
    "            interactive_document_corpus.append(interactive_document)\n",
    "        print('DONE: parse_data_from_json_to_interactive_document_object completed.')\n",
    "        return interactive_document_corpus\n",
    "    \n",
    "    def parse_data_from_csv_to_interactive_document_object(self, corpus_csv_file_path):\n",
    "        '''this is an example implementation for a csv parser. in future we need a generalized parser to bring different data (e.g. pfds, csvs, docs, ...) into the interactive document corpus format.'''\n",
    "        print('...parsing data from csv to interactive document objects.')\n",
    "        # load the dataset from csv\n",
    "        covid_news_df = pd.read_csv(corpus_csv_file_path)\n",
    "        #covid_news_df.head()\n",
    "        # filter out only the reliable data sources\n",
    "        reliable_covid_news_df = covid_news_df.loc[covid_news_df['reliability'] > 0]\n",
    "        reliable_covid_news_df = reliable_covid_news_df.reset_index()\n",
    "        # ALTERNATIVE: SHUFFLE DATA FRAME: e.g. needed when only a subset is needed for visualization => IMPORTANT: be aware: this breaks frontend procedures!\n",
    "        #reliable_covid_news_df = reliable_covid_news_df.sample(frac=1).reset_index(drop=True)\n",
    "        #reliable_covid_news_df.head()\n",
    "        #print(reliable_covid_news_df.shape)\n",
    "        # iterate over the rows to fill the interactive document objects for the interactive document corpus\n",
    "        interactive_document_corpus = []\n",
    "        reliable_covid_news_df_records = reliable_covid_news_df.to_dict('records')\n",
    "        for index, row in enumerate(reliable_covid_news_df_records):\n",
    "            interactive_document = InteractiveDocument()\n",
    "            interactive_document.content['raw_text'] = row['body_text']\n",
    "            interactive_document.uri = index\n",
    "            interactive_document.title = row['title'] if (str(row['title']) != 'null' and row['title'] != None) else 'empty'\n",
    "            interactive_document.authors = str(row['author']).replace('\\'', '').strip('][').split(', ') if (str(row['author']) != 'null' and row['author'] != None) else []\n",
    "            interactive_document.timestamp = row['publish_date'] if (str(row['publish_date']) != 'null' and row['publish_date'] != None) else 'empty'\n",
    "            interactive_document.rating = 0.0\n",
    "            interactive_document.annotation = \"\"\n",
    "            interactive_document.tags = []\n",
    "            interactive_document.paragraphs = []\n",
    "            interactive_document.document_vector = [0,0]\n",
    "            interactive_document.summary = row['body_text'] if (str(row['body_text']) != 'null' and row['body_text'] != None) else ''\n",
    "            #print(len(interactive_document.summary))\n",
    "            # CREATE SUMMARIES\n",
    "            # we take the first 1800 characters as the abstract of each document\n",
    "            interactive_document.summary = (interactive_document.summary[:1800] + ' .') if len(interactive_document.summary) > 1800 else interactive_document.summary\n",
    "            # we have to preprocess the summary to make sure it is clean\n",
    "            interactive_document.summary = interactive_document.summary.encode('utf-8','ignore').decode('utf8')\n",
    "            summary_text = self.text_processor(interactive_document.summary)\n",
    "            cleaned_text = []\n",
    "            for token in summary_text:\n",
    "                if not token.is_space and not token.is_bracket and (token.is_alpha or (token.text.lower()=='.') or (token.text.lower()==',') or (token.text.lower()=='?') or (token.text.lower()==':') or (token.text.lower()=='-')):\n",
    "                    cleaned_text.append(token.text.lower())   #(token.lemma_.lower())\n",
    "            interactive_document.summary = ' '.join(cleaned_text)\n",
    "            interactive_document_corpus.append(interactive_document)\n",
    "        print('DONE: parse_data_from_json_to_interactive_document_object completed.')\n",
    "        return interactive_document_corpus\n",
    "    \n",
    "    def parse_custom_data_to_interactive_document_object(self, custom_data_file_path):\n",
    "        print('test')\n",
    "        # 1. Load custom dataset\n",
    "        # a) meaning shift data set\n",
    "        # b) \n",
    "        # c) 20 newsgroups data set \n",
    "        # d) enron dataset -> emails\n",
    "        # f) google blogger corpus -> blogs\n",
    "        # g) \n",
    "    \n",
    "    def preprocess_raw_text_in_corpus(self, interactive_document_corpus):\n",
    "        print('... preprocessing raw text of whole corpus.')\n",
    "        for interactive_document in tqdm(interactive_document_corpus):\n",
    "            preprocessed_text = self.preprocess_raw_text(interactive_document)\n",
    "            interactive_document.content['preprocessed_text'] = preprocessed_text\n",
    "        print('DONE: preprocessing raw text of whole corpus completed.')\n",
    "        return interactive_document_corpus\n",
    "    \n",
    "    def create_keywords_in_corpus(self, interactive_document_corpus, must_have_keyword=None):\n",
    "        print('...creating_keywords in whole corpus.')\n",
    "        for interactive_document in tqdm(interactive_document_corpus):\n",
    "            keywords = self.extract_keywords(interactive_document, keyword_percentage_per_document, must_have_keyword=must_have_keyword)\n",
    "            interactive_document.keywords = keywords\n",
    "        print('DONE: keywords in corpus successfully created.')\n",
    "        return interactive_document_corpus\n",
    "    \n",
    "    def create_summaries_in_corpus():\n",
    "        pass\n",
    "        \n",
    "# TEST ----------------------------------------------------------------------------\n",
    "class TestPreprocessingManager(unittest.TestCase):\n",
    "\n",
    "    @classmethod\n",
    "    def setUpClass(self):\n",
    "        # set all things up for the test series here\n",
    "        pass\n",
    "\n",
    "    @classmethod\n",
    "    def tearDownClass(self):\n",
    "        # tear everything down after testing here\n",
    "        pass\n",
    "\n",
    "    def test_class_setup_and_serialization(self):\n",
    "        # given\n",
    "        preprocessing_manager_data_object = PreprocessingManager()\n",
    "        preprocessing_manager_repository_impl = PreprocessingManagerRepositoryImpl(preprocessing_manager_data_object)\n",
    "        # when\n",
    "        preprocessing_manager_repository_impl.create_summary()\n",
    "        print(preprocessing_manager_repository_impl.data_object.dict())\n",
    "        # then\n",
    "        #result = 6\n",
    "        #self.assertEqual(result, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4160de",
   "metadata": {},
   "source": [
    "## Keyword Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf67b8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find on github here: https://gist.github.com/BrambleXu/3d47bbdbd1ee4e6fc695b0ddb88cbf99\n",
    "# find tutorial here: https://towardsdatascience.com/textrank-for-keyword-extraction-by-python-c0bae21bcec0\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "class TextRank4Keyword():\n",
    "    \"\"\"Extract keywords from text\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.d = 0.85 # damping coefficient, usually is .85\n",
    "        self.min_diff = 1e-5 # convergence threshold\n",
    "        self.steps = 10 # iteration steps\n",
    "        self.node_weight = None # save keywords and its weight\n",
    "\n",
    "    \n",
    "    def set_stopwords(self, stopwords):  \n",
    "        \"\"\"Set stop words\"\"\"\n",
    "        for word in STOP_WORDS.union(set(stopwords)):\n",
    "            lexeme = nlp.vocab[word]\n",
    "            lexeme.is_stop = True\n",
    "    \n",
    "    def sentence_segment(self, doc, candidate_pos, lower):\n",
    "        \"\"\"Store those words only in cadidate_pos\"\"\"\n",
    "        sentences = []\n",
    "        for sent in doc.sents:\n",
    "            selected_words = []\n",
    "            for token in sent:\n",
    "                # Store words only with cadidate POS tag\n",
    "                if token.pos_ in candidate_pos and token.is_stop is False:\n",
    "                    if lower is True:\n",
    "                        selected_words.append(token.text.lower())\n",
    "                    else:\n",
    "                        selected_words.append(token.text)\n",
    "            sentences.append(selected_words)\n",
    "        return sentences\n",
    "        \n",
    "    def get_vocab(self, sentences):\n",
    "        \"\"\"Get all tokens\"\"\"\n",
    "        vocab = OrderedDict()\n",
    "        i = 0\n",
    "        for sentence in sentences:\n",
    "            for word in sentence:\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = i\n",
    "                    i += 1\n",
    "        return vocab\n",
    "    \n",
    "    def get_token_pairs(self, window_size, sentences):\n",
    "        \"\"\"Build token_pairs from windows in sentences\"\"\"\n",
    "        token_pairs = list()\n",
    "        for sentence in sentences:\n",
    "            for i, word in enumerate(sentence):\n",
    "                for j in range(i+1, i+window_size):\n",
    "                    if j >= len(sentence):\n",
    "                        break\n",
    "                    pair = (word, sentence[j])\n",
    "                    if pair not in token_pairs:\n",
    "                        token_pairs.append(pair)\n",
    "        return token_pairs\n",
    "        \n",
    "    def symmetrize(self, a):\n",
    "        return a + a.T - np.diag(a.diagonal())\n",
    "    \n",
    "    def get_matrix(self, vocab, token_pairs):\n",
    "        \"\"\"Get normalized matrix\"\"\"\n",
    "        # Build matrix\n",
    "        vocab_size = len(vocab)\n",
    "        g = np.zeros((vocab_size, vocab_size), dtype='float')\n",
    "        for word1, word2 in token_pairs:\n",
    "            i, j = vocab[word1], vocab[word2]\n",
    "            g[i][j] = 1\n",
    "            \n",
    "        # Get Symmeric matrix\n",
    "        g = self.symmetrize(g)\n",
    "        \n",
    "        # Normalize matrix by column\n",
    "        norm = np.sum(g, axis=0)\n",
    "        g_norm = np.divide(g, norm, where=norm!=0) # this is ignore the 0 element in norm\n",
    "        \n",
    "        return g_norm\n",
    "\n",
    "    \n",
    "    def get_keywords(self, number=10, must_have_keyword=None):\n",
    "        \"\"\"Print top number keywords\"\"\"\n",
    "        keywords = []\n",
    "        node_weight = OrderedDict(sorted(self.node_weight.items(), key=lambda t: t[1], reverse=True))\n",
    "        for i, (key, value) in enumerate(node_weight.items()):\n",
    "            #print(key + ' - ' + str(value))\n",
    "            keywords.append(key)\n",
    "            if i > number:\n",
    "                break\n",
    "        # check if must_have_keyword in keywords, if not -> add it\n",
    "        if(must_have_keyword is not None):\n",
    "            if(str(must_have_keyword) not in keywords):\n",
    "                for i, (key, value) in enumerate(node_weight.items()):\n",
    "                    if(str(key) == str(must_have_keyword)):\n",
    "                        keywords.append(key)\n",
    "                        break\n",
    "        return keywords\n",
    "        \n",
    "        \n",
    "    def analyze(self, text, \n",
    "                candidate_pos=['NOUN', 'PROPN'], \n",
    "                window_size=4, lower=False, stopwords=list()):\n",
    "        \"\"\"Main function to analyze text\"\"\"\n",
    "        \n",
    "        # Set stop words\n",
    "        self.set_stopwords(stopwords)\n",
    "        \n",
    "        # Pare text by spaCy\n",
    "        doc = nlp(text)\n",
    "        \n",
    "        # Filter sentences\n",
    "        sentences = self.sentence_segment(doc, candidate_pos, lower) # list of list of words\n",
    "        \n",
    "        # Build vocabulary\n",
    "        vocab = self.get_vocab(sentences)\n",
    "        \n",
    "        # safe vocabulary to object variable\n",
    "        self.vocab = vocab\n",
    "        \n",
    "        # Get token_pairs from windows\n",
    "        token_pairs = self.get_token_pairs(window_size, sentences)\n",
    "        \n",
    "        # Get normalized matrix\n",
    "        g = self.get_matrix(vocab, token_pairs)\n",
    "        \n",
    "        # Initionlization for weight(pagerank value)\n",
    "        pr = np.array([1] * len(vocab))\n",
    "        \n",
    "        # Iteration\n",
    "        previous_pr = 0\n",
    "        for epoch in range(self.steps):\n",
    "            pr = (1-self.d) + self.d * np.dot(g, pr)\n",
    "            if abs(previous_pr - sum(pr))  < self.min_diff:\n",
    "                break\n",
    "            else:\n",
    "                previous_pr = sum(pr)\n",
    "\n",
    "        # Get weight for each node\n",
    "        node_weight = dict()\n",
    "        for word, index in vocab.items():\n",
    "            node_weight[word] = pr[index]\n",
    "        \n",
    "        self.node_weight = node_weight\n",
    "        \n",
    "# Usage\n",
    "#text = '''\n",
    "#The Wandering Earth, described as China’s first big-budget science fiction thriller, quietly made it onto screens at AMC theaters in North America this weekend, and it shows a new side of Chinese filmmaking — one focused toward futuristic spectacles rather than China’s traditionally grand, massive historical epics. At the same time, The Wandering Earth feels like a throwback to a few familiar eras of American filmmaking. While the film’s cast, setting, and tone are all Chinese, longtime science fiction fans are going to see a lot on the screen that reminds them of other movies, for better or worse.\n",
    "#'''\n",
    "\n",
    "#tr4w = TextRank4Keyword()\n",
    "#tr4w.analyze(text, candidate_pos = ['NOUN', 'PROPN', 'VERB'], window_size=5, lower=True)\n",
    "#tr4w.get_keywords(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1f0e7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LexRank implementation, from: https://github.com/UKPLab/sentence-transformers/blob/8a87467870a43b5662372366d8433f8a1f017417/examples/applications/text-summarization/LexRank.py\n",
    "import numpy as np\n",
    "from scipy.sparse.csgraph import connected_components\n",
    "\n",
    "def degree_centrality_scores(\n",
    "    similarity_matrix,\n",
    "    threshold=None,\n",
    "    increase_power=True,\n",
    "):\n",
    "    if not (\n",
    "        threshold is None\n",
    "        or isinstance(threshold, float)\n",
    "        and 0 <= threshold < 1\n",
    "    ):\n",
    "        raise ValueError(\n",
    "            '\\'threshold\\' should be a floating-point number '\n",
    "            'from the interval [0, 1) or None',\n",
    "        )\n",
    "\n",
    "    if threshold is None:\n",
    "        markov_matrix = create_markov_matrix(similarity_matrix)\n",
    "\n",
    "    else:\n",
    "        markov_matrix = create_markov_matrix_discrete(\n",
    "            similarity_matrix,\n",
    "            threshold,\n",
    "        )\n",
    "\n",
    "    scores = stationary_distribution(\n",
    "        markov_matrix,\n",
    "        increase_power=increase_power,\n",
    "        normalized=False,\n",
    "    )\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "def _power_method(transition_matrix, increase_power=True):\n",
    "    eigenvector = np.ones(len(transition_matrix))\n",
    "\n",
    "    if len(eigenvector) == 1:\n",
    "        return eigenvector\n",
    "\n",
    "    transition = transition_matrix.transpose()\n",
    "\n",
    "    while True:\n",
    "        eigenvector_next = np.dot(transition, eigenvector)\n",
    "\n",
    "        if np.allclose(eigenvector_next, eigenvector):\n",
    "            return eigenvector_next\n",
    "\n",
    "        eigenvector = eigenvector_next\n",
    "\n",
    "        if increase_power:\n",
    "            transition = np.dot(transition, transition)\n",
    "\n",
    "\n",
    "def connected_nodes(matrix):\n",
    "    _, labels = connected_components(matrix)\n",
    "\n",
    "    groups = []\n",
    "\n",
    "    for tag in np.unique(labels):\n",
    "        group = np.where(labels == tag)[0]\n",
    "        groups.append(group)\n",
    "\n",
    "    return groups\n",
    "\n",
    "\n",
    "def create_markov_matrix(weights_matrix):\n",
    "    n_1, n_2 = weights_matrix.shape\n",
    "    if n_1 != n_2:\n",
    "        raise ValueError('\\'weights_matrix\\' should be square')\n",
    "\n",
    "    row_sum = weights_matrix.sum(axis=1, keepdims=True)\n",
    "\n",
    "    return weights_matrix / row_sum\n",
    "\n",
    "\n",
    "def create_markov_matrix_discrete(weights_matrix, threshold):\n",
    "    discrete_weights_matrix = np.zeros(weights_matrix.shape)\n",
    "    ixs = np.where(weights_matrix >= threshold)\n",
    "    discrete_weights_matrix[ixs] = 1\n",
    "\n",
    "    return create_markov_matrix(discrete_weights_matrix)\n",
    "\n",
    "\n",
    "def graph_nodes_clusters(transition_matrix, increase_power=True):\n",
    "    clusters = connected_nodes(transition_matrix)\n",
    "    clusters.sort(key=len, reverse=True)\n",
    "\n",
    "    centroid_scores = []\n",
    "\n",
    "    for group in clusters:\n",
    "        t_matrix = transition_matrix[np.ix_(group, group)]\n",
    "        eigenvector = _power_method(t_matrix, increase_power=increase_power)\n",
    "        centroid_scores.append(eigenvector / len(group))\n",
    "\n",
    "    return clusters, centroid_scores\n",
    "\n",
    "\n",
    "def stationary_distribution(\n",
    "    transition_matrix,\n",
    "    increase_power=True,\n",
    "    normalized=True,\n",
    "):\n",
    "    n_1, n_2 = transition_matrix.shape\n",
    "    if n_1 != n_2:\n",
    "        raise ValueError('\\'transition_matrix\\' should be square')\n",
    "\n",
    "    distribution = np.zeros(n_1)\n",
    "\n",
    "    grouped_indices = connected_nodes(transition_matrix)\n",
    "\n",
    "    for group in grouped_indices:\n",
    "        t_matrix = transition_matrix[np.ix_(group, group)]\n",
    "        eigenvector = _power_method(t_matrix, increase_power=increase_power)\n",
    "        distribution[group] = eigenvector\n",
    "\n",
    "    if normalized:\n",
    "        distribution /= n_1\n",
    "\n",
    "    return distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50543624",
   "metadata": {},
   "source": [
    "## Map Creator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19337af7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ENTITY\n",
    "class MapCreator(BaseModel):\n",
    "    '''\n",
    "    MapCreator: class that takes interactive documents and creates \n",
    "    '''\n",
    "    # Class Variables\n",
    "    interactive_document_corpus: List[Any] = None\n",
    "    paragraph_corpus: List[str] = None\n",
    "    doc_2_paragraph_index: Any\n",
    "    paragraph_2_doc_index: Any\n",
    "    contextualized_word_vectors: Any\n",
    "    contextualized_word_labels: List[str] = None\n",
    "    contextualized_word_2_doc_index: List[int] = None\n",
    "    contextualized_word_map: Any\n",
    "    document_vectors: Any\n",
    "    document_labels: List[str] = None\n",
    "    document_map: Any\n",
    "    paragraph_vectors: Any\n",
    "    paragraph_labels: List[str] = None\n",
    "    paragraph_map: Any\n",
    "    topic_vectors: Any\n",
    "    topic_labels: List[str] = None\n",
    "    topic_corpus: List[Any] = None\n",
    "    semantic_map: Any \n",
    "    \n",
    "        \n",
    "# REPOSITORY\n",
    "class MapCreatorRepository(ABC):\n",
    "    @abstractmethod\n",
    "    def load_interactive_document_corpus(self, method:str, interactive_document_corpus_uri:str):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def doc_id_2_paragraph_ids(self, interactive_document_id:int):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def paragraph_id_2_doc_id(self, paragraph_id:int):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def create_paragraph_embedding(self, paragraph:str):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def create_word_embeddings(self, paragraph:str, keywords:List[str]):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def get_topic_vectors(self):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def get_document_vectors(self):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def get_paragraph_vectors(self):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def get_contextualized_word_vectors(self):\n",
    "        pass\n",
    "    def get_semantic_map(self):\n",
    "        pass\n",
    "    # ... more to come ...\n",
    "    \n",
    "# REPOSITORY IMPLEMENTATION\n",
    "class MapCreatorRepositoryImpl(MapCreatorRepository):\n",
    "    # Constructor / if needed initialize iunstance attributes here\n",
    "    def __init__(self, map_creator_data_object: MapCreator):\n",
    "        self.data_object = map_creator_data_object\n",
    "        # ininitalize text processor\n",
    "        self.text_processor = spacy.load('en_core_web_sm')\n",
    "        print('Textprocessor: SpaCy initialized.') \n",
    "        # initialize the Sentence Transformer for the Embedding Model\n",
    "        self.sentence_model = SentenceTransformer(allenai_specter_model_dir, device=\"cuda\") #SentenceTransformer(allenai_scibert_model_dir, device=\"cuda\") #SentenceTransformer('all-mpnet-base-v2', device=\"cuda\")\n",
    "        self.bert_tokenizer = AutoTokenizer.from_pretrained(allenai_specter_model_dir, do_lower_case=True) #AutoTokenizer.from_pretrained(allenai_scibert_model_dir, do_lower_case=True) #AutoTokenizer.from_pretrained(allenai_specter_model_dir, do_lower_case=True)\n",
    "        #self.bert_tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-mpnet-base-v2', do_lower_case=True)\n",
    "        #Change the length to 512\n",
    "        self.sentence_model.max_seq_length = 512\n",
    "        print(\"Sentence Transformer Max Sequence Length: \", self.sentence_model.max_seq_length)\n",
    "        # change tokenizer to allen_ai_specter tokenizer\n",
    "        self.sentence_model.tokenizer = self.bert_tokenizer\n",
    "        print(\"Sentence Transformer Tokenizer: \", self.sentence_model.tokenizer)\n",
    "        print('Sentence Transformer: allenai-specter initialized.')\n",
    "        print('{0} initialized.'.format(self.__class__.__name__))\n",
    "    \n",
    "    # override methods\n",
    "    #@override\n",
    "    def load_interactive_document_corpus(self, interactive_document_corpus_uri:str, method:str):\n",
    "        if(method=='load_from_file'):\n",
    "            print('... loading interactive_document_corpus from: {0}'.format(interactive_document_corpus_uri))\n",
    "            # load corpus from txt file\n",
    "            interactive_document_corpus = self.load_interactive_document_corpus_from_file(interactive_document_corpus_uri)\n",
    "            # set a reference of the corpus to the map creator data object\n",
    "            self.data_object.interactive_document_corpus = interactive_document_corpus\n",
    "            return interactive_document_corpus\n",
    "        elif(method=='load_from_web_resource'):\n",
    "            print('method not implemented yet.')\n",
    "            return []\n",
    "        else:\n",
    "            print('ERROR: No valid method provided.')\n",
    "            return []\n",
    "    \n",
    "    #@override\n",
    "    def doc_id_2_paragraph_ids(self, interactive_document_id:int):\n",
    "        '''doc_id_2_paragraph_ids: function takes a document id and gives back a list of paragraph indices.'''\n",
    "        return self.data_object.doc_2_paragraph_index[interactive_document_id]\n",
    "    #@override\n",
    "    def paragraph_id_2_doc_id(self, paragraph_id:int):\n",
    "        '''paragraph_id_2_doc_id: function takes a paragraph index and gives back a document index.'''\n",
    "        return self.data_object.paragraph_2_doc_index[paragraph_id]\n",
    "    #@override\n",
    "    def create_paragraph_embedding(self, paragraph:str):\n",
    "        print('')\n",
    "    #@override \n",
    "    def create_word_embeddings(self, paragraph:str, keywords:List[str]):\n",
    "        print('')\n",
    "    #@override\n",
    "    def get_topic_vectors(self):\n",
    "        return self.data_object.topic_vectors\n",
    "    \n",
    "    #@override\n",
    "    def get_contextualized_word_vectors(self):\n",
    "        return self.data_object.contextualized_word_vectors\n",
    "    \n",
    "    #@override\n",
    "    def get_document_vectors(self):\n",
    "        return self.data_object.document_vectors\n",
    "    \n",
    "    #@override\n",
    "    def get_paragraph_vectors(self):\n",
    "        return self.data_object.paragraph_vectors\n",
    "    \n",
    "    #@override\n",
    "    def get_semantic_map(self):\n",
    "        return self.data_object.semantic_map\n",
    "        \n",
    "    # utility methods\n",
    "    def load_interactive_document_corpus_from_file(self, corpus_txt_file_path):\n",
    "        # load objects from json file\n",
    "        with open(corpus_txt_file_path) as json_file:\n",
    "            interactive_document_corpus = json.load(json_file)\n",
    "        # parse raw strings into objects\n",
    "        for i, raw_document in enumerate(interactive_document_corpus):\n",
    "            interactive_document = InteractiveDocument.parse_obj(raw_document)\n",
    "            interactive_document_corpus[i] = interactive_document\n",
    "        print('loading interactive_document_corpus completed.')\n",
    "        return interactive_document_corpus\n",
    "    \n",
    "    def get_paragraphs_from_doc(self, interactive_document:InteractiveDocument, paragraph_length:int):\n",
    "        preprocessed_text = interactive_document.content['preprocessed_text']\n",
    "        preprocessed_text = self.text_processor(preprocessed_text)\n",
    "        tokenized_text = []\n",
    "        # transform text into token list\n",
    "        for token in preprocessed_text:\n",
    "            tokenized_text.append(token.text)\n",
    "        # divide text into sublists\n",
    "        list_of_tokenized_sublists = list(self.divide_list_into_sublists(tokenized_text, paragraph_length))\n",
    "        paragraphs = []\n",
    "        # turn each paragraph back into a string\n",
    "        for single_paragraph_token_list in list_of_tokenized_sublists:\n",
    "            single_paragraph = ' '.join(single_paragraph_token_list)\n",
    "            paragraphs.append(single_paragraph)\n",
    "        # check for the special case, if no paragraphs are found in the document, then we just deliver an empty paragraph to be consistent for further paragraph counting\n",
    "        if(len(paragraphs)==0):\n",
    "            print('WARNING: no text paragraphs detected in document!')\n",
    "            empty_paragraph = ''\n",
    "            paragraphs.append(empty_paragraph)\n",
    "        return paragraphs\n",
    "    \n",
    "    def divide_list_into_sublists(self, input_list, sublist_length):\n",
    "        # looping till length l \n",
    "        for i in range(0, len(input_list), sublist_length):  \n",
    "            yield input_list[i:i + sublist_length]   \n",
    "    \n",
    "    def create_paragraph_corpus_and_doc_2_par_indices(self, interactive_document_corpus, paragraph_length:int):\n",
    "        print('... creating paragraph_corpus and doc_2_par indices.')\n",
    "        # 1. initialize doc_2_paragraph_index and paragraph_to_doc_index\n",
    "        self.data_object.paragraph_corpus = []\n",
    "        self.data_object.doc_2_paragraph_index = []\n",
    "        self.data_object.paragraph_2_doc_index = []\n",
    "        # 2. for every document build the paragraphs\n",
    "        for i, interactive_document in enumerate(interactive_document_corpus):\n",
    "            paragraphs = self.get_paragraphs_from_doc(interactive_document, paragraph_length)\n",
    "            # add the paragraphs to the data_objects paragraph_corpus\n",
    "            for paragraph in paragraphs:\n",
    "                self.data_object.paragraph_corpus.append(paragraph)\n",
    "            document_paragraph_list = []\n",
    "            number_of_paragraphs = len(paragraphs)\n",
    "            # catch the first document as a special case\n",
    "            if(i==0):\n",
    "                document_paragraph_list = [p for p in range(number_of_paragraphs)]\n",
    "            else:\n",
    "                # check if the last paragraph of the last document exists, if this is empty, that means that the last document has had 0 paragraphs and therefore we take second last document\n",
    "                last_paragraph_index = self.data_object.doc_2_paragraph_index[-1][-1]\n",
    "                new_last_paragraph_index = last_paragraph_index + number_of_paragraphs\n",
    "                document_paragraph_list = [p for p in range(last_paragraph_index +1, new_last_paragraph_index+1)]\n",
    "            # put the paragraphs in the respective index\n",
    "            for j, _ in enumerate(paragraphs):\n",
    "                # for every paragraph put the respective document in the list -> [0,0,0,0,1,1,1,] -> self.data_object.paragraph_2_doc_index[paragraph_id]= document_id\n",
    "                self.data_object.paragraph_2_doc_index.append(i)\n",
    "            # append the document corresponding paragraph list to the doc2paragraph index -> self.data_object.doc_2_paragraph_index[document_id] = [23,25,26,27,28] (=list of paragraph indices)\n",
    "            self.data_object.doc_2_paragraph_index.append(document_paragraph_list) \n",
    "        print('DONE: paragraph corpus and doc_2_par indices successfully created.')\n",
    "        \n",
    "    def create_topic_model(self, top_n_words=10, calculate_probabilities=True, n_gram_range_upper_bound=1, number_of_best_matching_docs=10, \\\n",
    "                           extractive_summarization_method='lex_rank', abstractive_summarization_method=None): # bart_summarizer\n",
    "        print('... creating topic model.')\n",
    "        #self.topic_model = BERTopic(embedding_model=self.sentence_model, top_n_words=top_n_words).fit(self.data_object.paragraph_corpus)\n",
    "        topic_model = BERTopic(embedding_model= self.sentence_model, top_n_words=top_n_words, calculate_probabilities=calculate_probabilities,  n_gram_range=(1, n_gram_range_upper_bound))\n",
    "        # train topic model \n",
    "        paragraph_to_topic_map, probabilities = topic_model.fit_transform(self.data_object.paragraph_corpus)\n",
    "        #print(topic_model.get_topic_info())\n",
    "        # create topic corpus\n",
    "        documents_list = []\n",
    "        paragraphs_list = []\n",
    "        paragraph_topics_list = []\n",
    "        topic_probabilities_list = []\n",
    "        for index, topic in enumerate(tqdm(paragraph_to_topic_map)):\n",
    "            if(topic != -1):\n",
    "                #print('doc: ' + str(map_creator_repository_impl.data_object.paragraph_2_doc_index[index]) +  ', par: ' + str(index) + ', topic: ' + str(topic) + ', probability: ' + str(probs[index][topic]))\n",
    "                current_document = self.data_object.paragraph_2_doc_index[index]\n",
    "                documents_list.append(current_document)\n",
    "                current_paragraph = index\n",
    "                paragraphs_list.append(current_paragraph)\n",
    "                current_topic = topic\n",
    "                paragraph_topics_list.append(current_topic)\n",
    "                current_max_probability = probabilities[index][topic]\n",
    "                topic_probabilities_list.append(current_max_probability)\n",
    "            #else:\n",
    "            #    print('no topic detected.')\n",
    "        # CREATE dataframe from lists, solution found here: https://www.geeksforgeeks.org/create-a-pandas-dataframe-from-lists/\n",
    "        topic_2_source_df = pd.DataFrame(list(zip(documents_list, paragraphs_list, paragraph_topics_list, topic_probabilities_list)),\n",
    "                       columns =['document_index', 'paragraph_index', 'topic_index', 'topic_probability'])\n",
    "        # SORT dataframe for 1st: topic_index, 2nd: topic_probability, solution found here: https://datatofish.com/sort-pandas-dataframe/, ascending=false -> we sort to last topic and highest probability\n",
    "        topic_2_source_df.sort_values(by=['topic_index','topic_probability'], inplace=True, ascending=False)\n",
    "        #print(topic_2_source_df.head())\n",
    "        # EXTRACT/READ dataframe for every topic_index, so that we can find the most relevant sources for each topic, solution found here: https://pandas.pydata.org/pandas-docs/stable/getting_started/intro_tutorials/03_subset_data.html\n",
    "        all_topics_df = topic_model.get_topic_info()\n",
    "        #print(all_topics_df.head())\n",
    "        topic_extend = all_topics_df['Topic'].tolist()\n",
    "        topic_min = min(topic_extend)\n",
    "        if(topic_min != -1):\n",
    "            print('ERROR: no -1 topic detected.')\n",
    "        #print(topic_min)\n",
    "        topic_max = max(topic_extend)\n",
    "        #print(topic_max)\n",
    "        topic_corpus = []\n",
    "        for topic_index in range(0, topic_max+1): # we need to include the topic_max index -> therefore +1\n",
    "            current_topic_dataframe = topic_2_source_df[topic_2_source_df[\"topic_index\"] == topic_index] \n",
    "            # get the documents as a list and, make a set out of it (=remove duplicates) and take the top n out of it\n",
    "            current_topic_documents = current_topic_dataframe['document_index'].tolist() # https://stackoverflow.com/questions/22341271/get-list-from-pandas-dataframe-column-or-row\n",
    "            current_topic_best_matching_documents_indices = list(set(current_topic_documents))[:number_of_best_matching_docs] # this is accurate, because we SORTED ascending before!\n",
    "            #print(current_topic_best_matching_documents_indices)\n",
    "            # store topic_model into a json file which can be used by frontend, format: [{topic_index:0, topic_words:['test', 'koala'], best_matching_documents_indices:[16, 3, 2, 3, 5, 6]}]\n",
    "            single_topic_words = topic_model.get_topic(topic_index)\n",
    "            single_topic_word_list = []\n",
    "            for word in single_topic_words:\n",
    "                single_topic_word_list.append(word[0])\n",
    "            #print(single_topic_word_list)\n",
    "            current_topic_info_df = all_topics_df[all_topics_df[\"Topic\"] == topic_index] \n",
    "            #print(current_topic_info_df)\n",
    "            # get value of single cell in datafrane, solution found here: https://stackoverflow.com/questions/16729574/how-to-get-a-value-from-a-cell-of-a-dataframe\n",
    "            current_topic_name = current_topic_info_df.iloc[0]['Name']\n",
    "            #print(current_topic_name)\n",
    "            current_topic_size = current_topic_info_df.iloc[0]['Count']\n",
    "            # create topic summary from paragraphs of the best matching sources -> first create extractive summary from the paragraphs and then make it nice using the huggingface abstractive summarization pipeline\n",
    "            current_best_matching_paragraph_indices = current_topic_dataframe['paragraph_index'].tolist()\n",
    "            topic_summary = ''\n",
    "            best_matching_paragraphs_concatenated = ''\n",
    "            if(extractive_summarization_method != None):\n",
    "                for paragraph_index in current_best_matching_paragraph_indices[:5]: # 5 works well!\n",
    "                    paragraph_text = self.data_object.paragraph_corpus[paragraph_index]\n",
    "                    best_matching_paragraphs_concatenated = best_matching_paragraphs_concatenated + '. ' + paragraph_text\n",
    "                if(extractive_summarization_method == 'text_rank' and len(best_matching_paragraphs_concatenated) > 30): # make sure we have a minimum length we can summarize\n",
    "                    # create extractive summary using sumy, solution found here: https://jcharistech.wordpress.com/2019/01/05/how-to-summarize-text-or-document-with-sumy/\n",
    "                    parser = PlaintextParser.from_string(best_matching_paragraphs_concatenated,Tokenizer(\"english\"))\n",
    "                    text_rank_summarizer = TextRankSummarizer()\n",
    "                    text_rank_summary = text_rank_summarizer(parser.document,5)\n",
    "                    text_rank_summary_sentences = []\n",
    "                    for sentence in text_rank_summary:\n",
    "                        text_rank_summary_sentences.append(str(sentence))\n",
    "                    rank_summary = ' '.join([str(sentence) for sentence in text_rank_summary_sentences])\n",
    "                    #print(text_rank_summary)\n",
    "                elif(extractive_summarization_method == 'lex_rank' and len(best_matching_paragraphs_concatenated) > 30): \n",
    "                    sentences = nltk.sent_tokenize(best_matching_paragraphs_concatenated)\n",
    "                    # eliminate too short sentences\n",
    "                    sentences = [sentence for sentence in sentences if len(sentence) > 2]\n",
    "                    embeddings = self.sentence_model.encode(sentences, convert_to_tensor=True)\n",
    "                    embeddings = embeddings.cpu()\n",
    "                    cos_scores = util.cos_sim(embeddings, embeddings).numpy()\n",
    "                    centrality_scores = degree_centrality_scores(cos_scores, threshold=None)\n",
    "                    most_central_sentence_indices = np.argsort(-centrality_scores)\n",
    "                    lex_rank_summary_sentences_indices = most_central_sentence_indices[0:5]\n",
    "                    rank_summary = ' '.join([str(sentences[index]) for index in lex_rank_summary_sentences_indices])\n",
    "                #elif(): https://towardsdatascience.com/understand-text-summarization-and-create-your-own-summarizer-in-python-b26a9f09fc70, \n",
    "                else:\n",
    "                    print('ERROR: invalid text summarization method given OR concatenated paragraphs to short.')\n",
    "                    rank_summary = 'empty'\n",
    "                if(abstractive_summarization_method != None):\n",
    "                    # create abstractive summary using huggingface summarizer pipeline, ALTERNATIVE: https://github.com/UKPLab/sentence-transformers/blob/master/examples/applications/text-summarization/text-summarization.py\n",
    "                    summarizer = pipeline(\"summarization\") # https://huggingface.co/transformers/main_classes/pipelines.html#transformers.SummarizationPipeline\n",
    "                    topic_summary = summarizer(rank_summary, min_length=40, max_length=100, do_sample=False)[0]['summary_text']\n",
    "                else:\n",
    "                    topic_summary = rank_summary\n",
    "            #print(topic_summary)                  \n",
    "            #print(current_topic_size)\n",
    "            current_topic_object = {\n",
    "            'topic_index': str(topic_index), \n",
    "            'topic_name':  str(current_topic_name),\n",
    "            'topic_size': str(current_topic_size),\n",
    "            'topic_words': single_topic_word_list, \n",
    "            'topic_summary': topic_summary,\n",
    "            'best_matching_documents_indices': current_topic_best_matching_documents_indices\n",
    "            }\n",
    "            topic_corpus.append(current_topic_object)\n",
    "        self.topic_corpus = topic_corpus\n",
    "        print('DONE: topic model successfully created.')\n",
    "        \n",
    "    def get_topic_info(self, max_topic_number:int):\n",
    "        info_df = self.topic_model.get_topic_info().head(max_topic_number)\n",
    "        return info_df\n",
    "    \n",
    "    def get_topic_corpus(self):\n",
    "        topic_corpus = self.topic_corpus\n",
    "        return topic_corpus\n",
    "        \n",
    "    def create_topic_vectors(self):\n",
    "        print('... creating topic vectors for corpus.')\n",
    "        topic_vectors_np_array = self.topic_model.topic_embeddings\n",
    "        topic_labels = []\n",
    "        for t in range(-1, len(topic_vectors_np_array)-1): # topic label -1 is the label that captures all the words that do not \n",
    "            topic_labels.append(str(t))\n",
    "        #print(len(topic_vectors_np_array))\n",
    "        #print(topic_vectors[0])\n",
    "        # convert topic vectors from numpy to pandas dataframe\n",
    "        topic_vectors_df = pd.DataFrame(topic_vectors_np_array)\n",
    "        #print(topic_vectors_df.shape)\n",
    "        self.data_object.topic_vectors = topic_vectors_df\n",
    "        self.data_object.topic_labels = topic_labels\n",
    "        print('DONE: topic vectors successfully created.')\n",
    "        \n",
    "    def create_document_vectors(self):\n",
    "        print('...creating document vectors for corpus.')\n",
    "        # get interactive document corpus\n",
    "        document_summary_corpus = []\n",
    "        for i, interactive_document in enumerate(tqdm(self.data_object.interactive_document_corpus)):\n",
    "            # check if the abstract of the document exists, if yes get the abstract of the respective document, if no put an empty abstract\n",
    "            if(interactive_document.summary!= None):\n",
    "                document_summary = interactive_document.summary\n",
    "            else:\n",
    "                document_summary = ''\n",
    "            # add the abstract to the corpus\n",
    "            document_summary_corpus.append(document_summary)\n",
    "        # create labels for the document_vectors\n",
    "        document_labels = []\n",
    "        # if we use the summaries only, then every document is represented by its abstract/summary and therefore its label is its ID/index in the interactive document corpus\n",
    "        for index in range(len(self.data_object.interactive_document_corpus)):\n",
    "            document_labels.append(str(index))\n",
    "        # embedd the summaries\n",
    "        document_summary_embeddings = self.get_document_embeddings_for_corpus(document_summary_corpus)\n",
    "        #  turn list of embedding vectors into pandas dataframe\n",
    "        document_summary_embeddings_np_array = [i.numpy() for i in document_summary_embeddings] \n",
    "        document_summary_embeddings_df = pd.DataFrame(document_summary_embeddings_np_array)\n",
    "        # set the embeddings of the abstracts as the document_vectors of the data_object\n",
    "        self.data_object.document_vectors = document_summary_embeddings_df\n",
    "        self.data_object.document_labels = document_labels\n",
    "        print('DONE: document_vectors created successfully.')\n",
    "        \n",
    "    def create_paragraph_vectors(self):\n",
    "        print('...creating paragraph vectors for corpus.')\n",
    "        # with using paragraph vectors the every document is split into paragraphs and every paragraph is embedded into a single vector and a map is created from that. \n",
    "        document_paragraph_corpus = self.data_object.paragraph_corpus\n",
    "        document_paragraph_labels = []\n",
    "        for paragraph_index in range(len(self.data_object.paragraph_corpus)):\n",
    "            document_paragraph_labels.append(self.data_object.paragraph_2_doc_index[paragraph_index])\n",
    "        # embedd the paragraphs\n",
    "        document_paragraph_embeddings = self.get_document_embeddings_for_corpus(document_paragraph_corpus)\n",
    "        #  turn list of embedding vectors into pandas dataframe\n",
    "        document_paragraph_embeddings_np_array = [i.numpy() for i in document_paragraph_embeddings] \n",
    "        document_paragraph_embeddings_df = pd.DataFrame(document_paragraph_embeddings_np_array)\n",
    "        # set the embeddings of the abstracts as the document_vectors of the data_object\n",
    "        self.data_object.paragraph_vectors = document_paragraph_embeddings_df\n",
    "        self.data_object.paragraph_labels = document_paragraph_labels\n",
    "        print('DONE: document_vectors created successfully.')\n",
    "        \n",
    "    def create_contextualized_word_vectors(self, use_summaries_only=False):\n",
    "        print('... creating contextualized word vectors.')\n",
    "        # aggregation data structures to aggregate all the embedding frames from all paragraphs in all documents\n",
    "        all_contextualized_embedding_data_frames = []\n",
    "        all_contextualized_word_labels = []\n",
    "        all_contextualized_word_2_doc_mappings = []\n",
    "        # for every document get the paragraphs and the keywords\n",
    "        for i, interactive_document in enumerate(tqdm(self.data_object.interactive_document_corpus)):\n",
    "            document_keywords = interactive_document.keywords\n",
    "            document_paragraph_indices = self.doc_id_2_paragraph_ids(i)\n",
    "            document_paragraphs = []\n",
    "            if(use_summaries_only):\n",
    "                if(interactive_document.summary!= None):\n",
    "                    summary = interactive_document.summary\n",
    "                    # restrict the summary to a length of 300 words\n",
    "                    summary = interactive_document.summary.split()\n",
    "                    summary = summary[:300]\n",
    "                    summary = ' '.join(summary)\n",
    "                else:\n",
    "                    summary = ''\n",
    "                document_paragraphs.append(summary)\n",
    "            else:\n",
    "                for paragraph_index in document_paragraph_indices:\n",
    "                    document_paragraphs.append(self.data_object.paragraph_corpus[paragraph_index])\n",
    "            # for every paragraph get the contextualized word embeddings of the document keywords\n",
    "            for index, paragraph in enumerate(document_paragraphs):\n",
    "                contextualized_embeddings, labels = self.get_paragraph_contextualized_word_embeddings(paragraph, document_keywords)\n",
    "                contextualized_embeddings_np_array = [i.numpy() for i in contextualized_embeddings] \n",
    "                contextualized_embeddings_df = pd.DataFrame(contextualized_embeddings_np_array)\n",
    "                # lemmatize and clean the labels\n",
    "                # TODO: lemmatize and clean the labels based on SPACY and after Lemmatizing and cleaning the labels ALSO lemmatize and clean the keywords of the document!!!\n",
    "                labels = self.lemmatize_and_clean_labels(labels)\n",
    "                # lemmatize and clean the keywords respectively and update them in the interactive document object, IMPORTANT: check if we do this AFTER getting all labels (= after the last paragraph)\n",
    "                if(index == len(document_paragraphs)-1):\n",
    "                    cleaned_document_keywords = self.lemmatize_and_clean_labels(document_keywords)\n",
    "                    # remove duplicates\n",
    "                    cleaned_document_keywords = list(set(cleaned_document_keywords))\n",
    "                    interactive_document.keywords = cleaned_document_keywords\n",
    "                # for every contextualized word, we create a mapping to the document it comes from\n",
    "                contextualized_word_2_doc_mappings = []\n",
    "                for label in labels:\n",
    "                    contextualized_word_2_doc_mappings.append(i)\n",
    "                # put the new data frame into the list of all dataframes\n",
    "                all_contextualized_embedding_data_frames.append(contextualized_embeddings_df)\n",
    "                # remember the length/number of the respective paragraphs points\n",
    "                # number_of_single_source_embeddings.append(contextualized_embeddings_df.shape[0])\n",
    "                # concatenate the labels \n",
    "                all_contextualized_word_labels = all_contextualized_word_labels + labels\n",
    "                # concatenate the cword 2 doc mappings\n",
    "                all_contextualized_word_2_doc_mappings = all_contextualized_word_2_doc_mappings + contextualized_word_2_doc_mappings \n",
    "        # concatenate all the pandas data frames of the single paragraphs\n",
    "        concatenated_contextualized_embeddings_df = pd.concat(all_contextualized_embedding_data_frames).reset_index(drop=True)\n",
    "        #print(concatenated_context_embeddings_df.shape)\n",
    "        # save the contextualized_word_vectors and the labels from all documents in the data_object variable\n",
    "        self.data_object.contextualized_word_vectors = concatenated_contextualized_embeddings_df\n",
    "        self.data_object.contextualized_word_labels = all_contextualized_word_labels\n",
    "        self.data_object.contextualized_word_2_doc_index = all_contextualized_word_2_doc_mappings\n",
    "        print('DONE: contextualized word vectors successfully created.')\n",
    "        \n",
    "    def get_document_embedding(self, document):\n",
    "        # get the document_embedding vector from the sentence transformer\n",
    "        #Sentences are encoded by calling model.encode()\n",
    "        document_embedding = self.sentence_model.encode(document, convert_to_tensor=True)\n",
    "        document_embedding = document_embedding.cpu()\n",
    "        return document_embedding\n",
    "    \n",
    "    def get_document_embeddings_for_corpus(self, corpus):\n",
    "        # corpus = list of documents, like: corpus = ['This framework generates embeddings for each input sentence','Sentences are passed as a list of string.','The quick brown fox jumps over the lazy dog.']\n",
    "        # embedd the whole corpus at once\n",
    "        document_embeddings = self.sentence_model.encode(corpus, convert_to_tensor=True)\n",
    "        document_embeddings = document_embeddings.cpu()\n",
    "        return document_embeddings\n",
    "        \n",
    "    def get_paragraph_contextualized_word_embeddings(self, paragraph, document_keywords, label_lemmatization=False):\n",
    "        # check if paragraph is empty, if that is the case return empty lists\n",
    "        if(paragraph==''):\n",
    "            contextualized_embeddings = []\n",
    "            labels = [] \n",
    "            return contextualized_embeddings, labels \n",
    "        # 1. turn paragraph into a list of tokens\n",
    "        paragraph_text = self.text_processor(paragraph)\n",
    "        paragraph_tokens = []\n",
    "        # transform text into token list\n",
    "        for token in paragraph_text:\n",
    "            paragraph_tokens.append(token.text)\n",
    "        # get the indices of the keywords in the tokenized paragraph text\n",
    "        keyword_indices = self.get_keyword_indices_in_paragraph(paragraph_tokens, document_keywords)\n",
    "        # tokenize the paragraph for the BERT encoder\n",
    "        ids = self.bert_tokenizer.encode(paragraph, add_special_tokens = True, truncation = True, padding = \"max_length\", max_length=512)\n",
    "        sub_tokens = self.bert_tokenizer.convert_ids_to_tokens(ids)\n",
    "        #print(len(sub_tokens))\n",
    "        #print(sub_tokens)\n",
    "        # get the whole word indices from the subtokenized bert tokens\n",
    "        full_word_indices = self.get_full_word_indices_from_tokens(paragraph_tokens, sub_tokens)\n",
    "        #print(full_word_indices)\n",
    "        # get contextualized word/BERT embedding tokens for whole document\n",
    "        all_contextualized_word_embeddings = self.get_contextualized_word_embeddings_for_paragraph(paragraph)\n",
    "        all_contextualized_word_embeddings = all_contextualized_word_embeddings.cpu()\n",
    "        #print(len(all_contextualized_word_embeddings))      \n",
    "        # filter out the embeddings of the selected document keywords\n",
    "        contextualized_embeddings = []\n",
    "        labels = [] \n",
    "        #print('keyword indices: ' + str(keyword_indices))\n",
    "        for j, keyword_index in enumerate(keyword_indices):\n",
    "            # get the respective list from the full_word_indices and take the last token out of the list as the representative one for the whole word\n",
    "            try:\n",
    "                keyword_subtoken_list = full_word_indices[keyword_index]\n",
    "            except:\n",
    "                print('WARNING: cound not find keyword in abstract. this happens due to the fact that the embedding length of an abstract is restricted to max_length=512 tokens')\n",
    "                continue\n",
    "            # check if the keyword_subtoken_list is empty -> if this is the case, CONTINUE. THIS CAN HAPPEN e.g. when using ABSTRACTS/SUMMARIES only version, because in these cases\n",
    "            # there might be more keywords detected in the document, but we cannot find an embedding for them IN THE ABSTRACT!!! => therefore just continue in the loop!\n",
    "            if(len(keyword_subtoken_list)==0):\n",
    "                continue\n",
    "            #print(keyword_subtoken_list)\n",
    "            # get last element of the keyword_token_list and get the embedding for this\n",
    "            representative_token = keyword_subtoken_list[-1]\n",
    "            try:\n",
    "                contextualized_embedding = all_contextualized_word_embeddings[representative_token]\n",
    "                contextualized_embeddings.append(contextualized_embedding)\n",
    "                label = paragraph_tokens[keyword_index]\n",
    "                labels.append(label)\n",
    "            except:\n",
    "                print('error:')\n",
    "                print(keyword_index)\n",
    "                print(keyword_subtoken_list)\n",
    "                print(full_word_indices)\n",
    "        \n",
    "        return contextualized_embeddings, labels \n",
    "    \n",
    "    def get_keyword_indices_in_paragraph(self, paragraph_tokens, document_keywords):\n",
    "        keyword_indices = []\n",
    "        for i, word in enumerate(paragraph_tokens):\n",
    "            if word in document_keywords:\n",
    "                keyword_indices.append(i)\n",
    "            else:\n",
    "                continue\n",
    "        return keyword_indices\n",
    "     \n",
    "    def get_full_word_indices_from_tokens(self, full_tokens, sub_tokens):\n",
    "        '''\n",
    "        full_tokens: single word tokens produced by self.text_processor\n",
    "        sub_tokens: sub word tokens produced by BERTTokenizer    \n",
    "        '''\n",
    "        bert_encoder_max_length = 512\n",
    "        # subtoken to token mapping found here: https://github.com/tensorflow/text/issues/275\n",
    "        curr_index = -1 # index is incremented before any access to the array, so this is a smart way to initialize\n",
    "        count = 0\n",
    "        full_word_indices = [ [] for _ in range(bert_encoder_max_length) ] #  len(document_tokens)+2 => add CLS and SEP tokens\n",
    "        #print(len(document_tokens))\n",
    "        #print(len(tokens))\n",
    "        #print(len(full_word_indexes))\n",
    "        for i, token in enumerate(sub_tokens):\n",
    "            if token[:2] != '##':\n",
    "                curr_index += 1\n",
    "                count = 0\n",
    "            full_word_indices[curr_index].append(i) \n",
    "            count += 1\n",
    "        return full_word_indices \n",
    "    \n",
    "    def get_contextualized_word_embeddings_for_paragraph(self, paragraph):\n",
    "        paragraph_contextualized_word_embeddings = self.sentence_model.encode(paragraph, convert_to_tensor=True, output_value='token_embeddings')\n",
    "        return paragraph_contextualized_word_embeddings\n",
    "    \n",
    "    def lemmatize_and_clean_labels(self, labels):\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        cleaned_labels = []\n",
    "        for label in labels:\n",
    "            # replace the hashtags\n",
    "            label = label.replace('##', '')\n",
    "            #print(label)\n",
    "            # lemmatize the label\n",
    "            label = lemmatizer.lemmatize(label)\n",
    "            #print(label)\n",
    "            cleaned_labels.append(label)\n",
    "        return cleaned_labels\n",
    "        # Using Spacy lemmatization api\n",
    "        #for token in paragraph_text:\n",
    "        #    paragraph_tokens.append(token.lemma.lower())\n",
    "    \n",
    "    \n",
    "    def create_topic_map(self):\n",
    "        # TODO!\n",
    "        print('... creating topic map.')\n",
    "    \n",
    "    def create_document_map(self):\n",
    "        print('... creating document map')\n",
    "        assert not self.data_object.document_vectors.empty, \"ERROR: no document_vectors created, yet.\" \n",
    "        document_vectors_df = self.data_object.document_vectors\n",
    "        umap_reduced_document_map_df = self.umap_dimensionality_reduction(document_vectors_df)\n",
    "        document_labels = self.data_object.document_labels\n",
    "        doc_to_doc_index = []\n",
    "        for d in range(len(self.data_object.document_labels)):\n",
    "            doc_to_doc_index.append(-999) # we choose -999 as the placeholder to mark documents in this case, because it is highly unlikely to have so many topic levels\n",
    "        umap_reduced_document_map_df['word_2_doc_index'] = doc_to_doc_index\n",
    "        self.data_object.document_map = umap_reduced_document_map_df\n",
    "        print('DONE: document map successfully created.')\n",
    "        \n",
    "    def create_paragraph_map(self):\n",
    "        print('... creating paragraph map.')\n",
    "        assert not self.data_object.paragraph_vectors.empty, \"ERROR: no document_vectors created, yet.\" \n",
    "        paragraph_vectors_df = self.data_object.paragraph_vectors\n",
    "        umap_reduced_paragraph_map_df = self.umap_dimensionality_reduction(paragraph_vectors_df)\n",
    "        paragraph_labels = self.data_object.paragraph_labels\n",
    "        umap_reduced_paragraph_map_df['paragraph_2_doc_index'] = paragraph_labels\n",
    "        # for each paragraph get 2 most important keywords\n",
    "        tr4w = TextRank4Keyword()\n",
    "        all_paragraph_keywords = []\n",
    "        for index, paragraph in enumerate(self.data_object.paragraph_corpus):\n",
    "            tr4w.analyze(paragraph, candidate_pos = ['NOUN', 'PROPN', 'VERB'], window_size=5, lower=True)\n",
    "            number_of_keywords_for_document = 2\n",
    "            paragraph_keywords = tr4w.get_keywords(number_of_keywords_for_document)\n",
    "            all_paragraph_keywords.append(paragraph_keywords)\n",
    "        umap_reduced_paragraph_map_df['paragraph_keywords'] = all_paragraph_keywords\n",
    "        self.data_object.paragraph_map = umap_reduced_paragraph_map_df\n",
    "        print('DONE: paragraph map successfully created.')\n",
    "        \n",
    "    def create_contextualized_word_map(self):\n",
    "        print('...creating contextualized_word_map.')\n",
    "        assert not self.data_object.contextualized_word_vectors.empty, \"ERROR: no contextualized_word_vectors created, yet.\" \n",
    "        contextualized_word_vectors_df = self.data_object.contextualized_word_vectors\n",
    "        umap_reduced_contextualized_word_map_df = self.umap_dimensionality_reduction(contextualized_word_vectors_df)\n",
    "        umap_reduced_contextualized_word_map_df['label'] = self.data_object.contextualized_word_labels\n",
    "        umap_reduced_contextualized_word_map_df['word_2_doc_index'] = self.data_object.contextualized_word_2_doc_index\n",
    "        self.data_object.contextualized_word_map = umap_reduced_contextualized_word_map_df\n",
    "        print('DONE: contextualized word map successfully created.')\n",
    "        \n",
    "    def create_semantic_map(self):\n",
    "        # check if we have all the relevant object variables\n",
    "        assert not self.data_object.topic_vectors.empty, \"ERROR: no topic_vectors created, yet.\" \n",
    "        assert not self.data_object.contextualized_word_vectors.empty, \"ERROR: no contextualized_word_vectors created, yet.\" \n",
    "        assert not self.data_object.document_vectors.empty, \"ERROR: no document_vectors created, yet.\" \n",
    "        print('... creating semantic map.')\n",
    "        # stack the topic vectors and the contextualized word vectors above each other to reduce them later in one step in a single umap map\n",
    "        topic_vectors_df = self.data_object.topic_vectors\n",
    "        document_vectors_df = self.data_object.document_vectors\n",
    "        contextualized_word_vectors_df = self.data_object.contextualized_word_vectors\n",
    "        stack_list = []\n",
    "        stack_list.append(topic_vectors_df)\n",
    "        stack_list.append(document_vectors_df)\n",
    "        stack_list.append(contextualized_word_vectors_df)\n",
    "        complete_semantic_map_df = pd.concat(stack_list).reset_index(drop=True)\n",
    "        #print(complete_semantic_map_df.shape)\n",
    "        # UMAP the whole STACK (topic vectors + contextualized word vectors) onto a sphere\n",
    "        complete_umap_reduced_semantic_map_df = self.umap_dimensionality_reduction(complete_semantic_map_df)\n",
    "        # append labels column\n",
    "        complete_labels = self.data_object.topic_labels + self.data_object.document_labels + self.data_object.contextualized_word_labels \n",
    "        complete_umap_reduced_semantic_map_df['label'] = complete_labels\n",
    "        # append contextualized word 2 doc mapping column for later QUANTIZATION\n",
    "        # for the topic_vectors we append the label -1, because they do not belong to a document\n",
    "        topic_to_doc_index = []\n",
    "        for l in range(len(self.data_object.topic_labels)):\n",
    "            topic_to_doc_index.append(-1)\n",
    "        # for the document_vectors we append the label -1, because they do not belong to a document\n",
    "        doc_to_doc_index = []\n",
    "        for d in range(len(self.data_object.document_labels)):\n",
    "            doc_to_doc_index.append(-999) # we choose -999 as the placeholder to mark documents in this case, because it is highly unlikely to have so many topic levels\n",
    "        complete_index = topic_to_doc_index + doc_to_doc_index + self.data_object.contextualized_word_2_doc_index\n",
    "        complete_umap_reduced_semantic_map_df['word_2_doc_index'] = complete_index\n",
    "        #print(concatenated_umap_embedding_df.shape)\n",
    "        # store the created semantic in the data_object variables\n",
    "        self.data_object.semantic_map = complete_umap_reduced_semantic_map_df\n",
    "        print('DONE: semantic map successfully created.')\n",
    "        \n",
    "        \n",
    "    def umap_dimensionality_reduction(self, dataframe, output_metric='haversine'):\n",
    "        if(output_metric == 'haversine'):\n",
    "            print('UMAP: dim reduction using HAVERSINE output metric.')\n",
    "            umap_model = umap.UMAP(n_neighbors=15, min_dist=0.0, metric='cosine', output_metric='haversine', random_state=42)\n",
    "            umap_model.fit(dataframe)\n",
    "            # print(umap_embeddings.shape)\n",
    "            # OPTIONAL: make a pandas dataframe out of the embedding tensor\n",
    "            # umap_embedding_df = pd.DataFrame(umap_embeddings)\n",
    "            # spherical projection from here: https://umap-learn.readthedocs.io/en/latest/embedding_space.html#spherical-embeddings\n",
    "            # formulas from here: https://vvvv.org/blog/polar-spherical-and-geographic-coordinates#:~:text=In%20order%20to%20match%20the,a%20longitude%20of%200%C2%B0.\n",
    "            x = np.sin(umap_model.embedding_[:, 0]) * np.cos(umap_model.embedding_[:, 1])\n",
    "            y = np.sin(umap_model.embedding_[:, 0]) * np.sin(umap_model.embedding_[:, 1])\n",
    "            z = np.cos(umap_model.embedding_[:, 0])\n",
    "            ## lat / lon conversion -> x -> lat, y -> lon, for checking: lat range = -90 and +90 , lon range = -180 and +180\n",
    "            #lon = np.arctan2(y, x) # lon ranges produced by this are in [-pi,pi]|[-180,180], which is equivalent to earth mapping in degree [-180,180] [-pi/pi] OK\n",
    "            lon = np.arctan2(x, y) # IMPORTANT CHANGE!!!! -> use x,y here, otherwise the points are mirrored!!! and the projection in frntend will not work!\n",
    "            lat = np.arccos(z)# lat ranges produced by this are in [-pi, 0]|[-180,0], which is NOT equivalent to earth mapping in degree [-90, 90] || \n",
    "            lat = lat - math.pi/2 # = -90° \n",
    "            ## convert rad to degree: Degree = Radians * (180 / PI)\n",
    "            lon = lon*(180/(3.14159)) # divide by math.pi ~ 3.14159\n",
    "            lat = lat*(180/(3.14159)) # divide by math.pi ~ 3.14159\n",
    "            ## truncating numpy floats, otherwise pandas will not round correctly when dividing by irrational number py, found here: https://stackoverflow.com/questions/42021972/truncating-decimal-digits-numpy-array-of-floats\n",
    "            lon = self.trunc(lon, decs=6)\n",
    "            lat = self.trunc(lat, decs=6)\n",
    "            # store in dataframe\n",
    "            umap_embedding_df = pd.DataFrame(columns=['lat', 'lon'])\n",
    "            umap_embedding_df['lat'] = lat\n",
    "            umap_embedding_df['lon'] = lon\n",
    "        elif(output_metric == 'euclidean'):\n",
    "            print('UMAP: dim reduction using EUCLIDEAN output metric.')\n",
    "            umap_model = umap.UMAP(n_neighbors=15, min_dist=0.0, n_components=2, metric='cosine', random_state=42)\n",
    "            umap_model.fit(dataframe)\n",
    "            x = umap_model.embedding_.T[0]\n",
    "            y = umap_model.embedding_.T[1]\n",
    "            umap_embedding_df = pd.DataFrame(columns=['x', 'y'])\n",
    "            umap_embedding_df['x'] = x\n",
    "            umap_embedding_df['y'] = y\n",
    "        else:\n",
    "            umap_embedding_df = pd.DataFrame(columns=['0', '1'])\n",
    "            print('ERROR: no valid metric provided for umap dimensionality reduction. CHOOSE: haversine or euclidean')\n",
    "        return umap_embedding_df   \n",
    "    \n",
    "    def plot_dataframe(self, dataframe):\n",
    "        print('... plotting dataframe.')\n",
    "        umap_model = umap.UMAP(n_neighbors=15, min_dist=0.0, metric='cosine', output_metric='haversine', random_state=42)\n",
    "        umap_model.fit(dataframe)\n",
    "        # print(umap_embeddings.shape)\n",
    "        # OPTIONAL: make a pandas dataframe out of the embedding tensor\n",
    "        # umap_embedding_df = pd.DataFrame(umap_embeddings)\n",
    "        # spherical projection from here: https://umap-learn.readthedocs.io/en/latest/embedding_space.html#spherical-embeddings\n",
    "        # v/phi/latitude, u/theta/longitute, phi/theta/rho are just another name for latitude, longitude, and altitude.\n",
    "        ## formulas from here: https://vvvv.org/blog/polar-spherical-and-geographic-coordinates#:~:text=In%20order%20to%20match%20the,a%20longitude%20of%200%C2%B0.\n",
    "        x = np.sin(umap_model.embedding_[:, 0]) * np.cos(umap_model.embedding_[:, 1]) # sin(v) * cos(u)\n",
    "        y = np.sin(umap_model.embedding_[:, 0]) * np.sin(umap_model.embedding_[:, 1]) # sin(v) * cos(v)\n",
    "        z = np.cos(umap_model.embedding_[:, 0]) # = cos(v)\n",
    "        # define plot sizes\n",
    "        plt.rcParams[\"figure.figsize\"] = (15,15)\n",
    "        # plot on 3D sphere\n",
    "        #Visualize lat/lon projection like found here: https://umap-learn.readthedocs.io/en/latest/embedding_space.html\n",
    "        fig = plt.figure(0)\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        ax.scatter(y, x, z, cmap='Spectral') # check coordinate system axis -> swap x and y !! because of coordinate system of earth\n",
    "        ax.set_xlabel('Y axis')\n",
    "        ax.set_ylabel('X axis')\n",
    "        ax.set_zlabel('Z axis')\n",
    "        # plot 2D mapping of 3D sphere  \n",
    "        # lat / lon conversion -> x -> lat, y -> lon, for checking: lat range = -90 and +90 , lon range = -180 and +180\n",
    "        # lon = np.arctan2(y, x) # lon ranges produced by this are in [-pi,pi]|[-180,180], which is equivalent to earth mapping in degree [-180,180] [-pi/pi] OK\n",
    "        lon = np.arctan2(x, y) # IMPORTANT CHANGE!!!! -> use x,y here, otherwise the points are mirrored!!! and the projection in frntend will not work!\n",
    "        lat = np.arccos(z)# lat ranges produced by this are in [-pi, 0]|[-180,0], which is NOT equivalent to earth mapping in degree [-90, 90] || [-pi/2, pi/2] => NOT OK -> add pi/2 to lon?!\n",
    "        lat = lat - math.pi/2 # = -90° \n",
    "        ## convert rad to degree: Degree = Radians * (180 / PI)\n",
    "        lon = lon*(180/(3.14159)) # divide by math.pi ~ 3.14159\n",
    "        lat = lat*(180/(3.14159)) # divide by math.pi ~ 3.14159\n",
    "        ## truncating numpy floats, otherwise pandas will not round correctly when dividing by irrational number py, found here: https://stackoverflow.com/questions/42021972/truncating-decimal-digits-numpy-array-of-floats\n",
    "        lon = self.trunc(lon, decs=6)\n",
    "        lat = self.trunc(lat, decs=6)\n",
    "        plt.figure(1)\n",
    "        plt.scatter(lon, lat, cmap='Spectral')\n",
    "        plt.xlabel('lon-axis', fontsize=18)\n",
    "        plt.ylabel('lat-axis', fontsize=16)\n",
    "        # plot 2d plane\n",
    "        umap_plane_model = umap.UMAP(n_neighbors=15, min_dist=0.0, n_components=2, metric='cosine', random_state=42).fit(dataframe)\n",
    "        plt.figure(2)\n",
    "        plt.scatter(umap_plane_model.embedding_.T[0], umap_plane_model.embedding_.T[1] , cmap='Spectral')\n",
    "     \n",
    "    def get_quantized_corpus(self, number_of_decimals:int):\n",
    "        '''\n",
    "        number_of_decimals: quantizes every point of the pandas data frame to the given number of decimals and then aggregates the complete_semantic_map \n",
    "                            into an aggregated version\n",
    "        '''\n",
    "        print('.. quantizing corpus map points to {0} number of decimals.'.format(number_of_decimals))\n",
    "        reduced_contextualized_word_vectors_df = self.get_contextualized_word_map()\n",
    "        #print(reduced_contextualized_word_vectors.shape)\n",
    "        # QUANTIZE WORD VECTORS: quantize the lat and lon column to the given number of decimals\n",
    "        reduced_contextualized_word_vectors_df = reduced_contextualized_word_vectors_df.round({'lat': number_of_decimals, 'lon': number_of_decimals}) # df['c']=df['c'].apply(lambda x:np.round(x,number_of_decimals))\n",
    "        #print(reduced_contextualized_word_vectors.head(5))\n",
    "        # sort the rows ascending to their document number AND \\\n",
    "        # aggregate the points over the lat lon values\n",
    "        reduced_contextualized_word_vectors_df = reduced_contextualized_word_vectors_df.groupby(['word_2_doc_index','lat','lon'])['label'].apply(list) #.apply(lambda x: x.sum())\n",
    "        # reset index to get grouped columns back\n",
    "        reduced_contextualized_word_vectors_df = reduced_contextualized_word_vectors_df.reset_index() \n",
    "        reduced_contextualized_word_vectors_df.columns = ['word_2_doc_index', 'lat', 'lon', 'labels']\n",
    "        #print(reduced_contextualized_word_vectors_df.head(10))\n",
    "        #print(quantized_semantic_map_df.shape)\n",
    "        #print(quantized_semantic_map_df.head(10))\n",
    "        print('DONE: corpus map points successfuyll quantized.')\n",
    "        return reduced_contextualized_word_vectors_df\n",
    "    \n",
    "    def get_quantized_semantic_map(self, number_of_decimals:int):\n",
    "        '''\n",
    "        number_of_decimals: quantizes every point of the pandas data frame to the given number of decimals and then aggregates the complete_semantic_map \n",
    "                            into an aggregated version\n",
    "        '''\n",
    "        print('.. aggregating the semantic map to a base map'.format(number_of_decimals))\n",
    "        # TODO: QUANTIZE ADAPTIVELY!!! compute the point density in every bin/degree bin and decide then to quantize lower or higher to get optimal map for visualization!!!!\n",
    "        semantic_map_df = self.get_semantic_map()\n",
    "        # take the word vectors out of the semantic map\n",
    "        reduced_contextualized_word_vectors_df = semantic_map_df.loc[semantic_map_df['word_2_doc_index'] >= 0] # semantic_map_df[\"word_2_doc_index\"] >= 0\n",
    "        #print(reduced_contextualized_word_vectors.shape)\n",
    "        # QUANTIZE WORD VECTORS: quantize the lat and lon column to the given number of decimals\n",
    "        reduced_contextualized_word_vectors_df = reduced_contextualized_word_vectors_df.round({'lat': number_of_decimals, 'lon': number_of_decimals})\n",
    "        # get out the lat, lon and labels columns because they are the only we need\n",
    "        reduced_contextualized_word_vectors_df = reduced_contextualized_word_vectors_df[['lat', 'lon', 'label']]\n",
    "        # aggregate the points over the lat lon values\n",
    "        quantized_semantic_map_df = reduced_contextualized_word_vectors_df.groupby(['lat','lon'])['label'].apply(list) #.apply(lambda x: x.sum())\n",
    "        # reset index to get grouped columns back\n",
    "        quantized_semantic_map_df = quantized_semantic_map_df.reset_index() \n",
    "        quantized_semantic_map_df.columns = ['lat', 'lon', 'labels']\n",
    "        # assign every point in the quantized base map to a cluster\n",
    "        topic_cluster_assigned_quantized_semantic_map = self.assign_clusters_to_points_of_df(quantized_semantic_map_df)\n",
    "        print('DONE: semantic map successfuyll aggregated to a base map.')\n",
    "        return topic_cluster_assigned_quantized_semantic_map\n",
    "    \n",
    "    def get_quantized_base_map(self, number_of_decimals:int):\n",
    "        '''\n",
    "        number_of_decimals: quantizes every point of the pandas data frame to the given number of decimals and then aggregates the complete_semantic_map \n",
    "                            into an aggregated version\n",
    "        '''\n",
    "        print('.. aggregating the contextualized word map to a base map'.format(number_of_decimals))\n",
    "        # TODO: QUANTIZE ADAPTIVELY!!! compute the point density in every bin/degree bin and decide then to quantize lower or higher to get optimal map for visualization!!!!\n",
    "        contextualized_word_map_df = self.get_contextualized_word_map()\n",
    "        #print(contextualized_word_map_df.shape)\n",
    "        # QUANTIZE WORD VECTORS: quantize the lat and lon column to the given number of decimals\n",
    "        reduced_contextualized_word_vectors_df = contextualized_word_map_df.round({'lat': number_of_decimals, 'lon': number_of_decimals})\n",
    "        # get out the lat, lon and labels columns because they are the only we need\n",
    "        reduced_contextualized_word_vectors_df = reduced_contextualized_word_vectors_df[['lat', 'lon', 'label']]\n",
    "        # aggregate the points over the lat lon values\n",
    "        quantized_base_map_df = reduced_contextualized_word_vectors_df.groupby(['lat','lon'])['label'].apply(list) #.apply(lambda x: x.sum())\n",
    "        # reset index to get grouped columns back\n",
    "        quantized_base_map_df = quantized_base_map_df.reset_index() \n",
    "        quantized_base_map_df.columns = ['lat', 'lon', 'labels']\n",
    "        # assign every point in the quantized base map to a cluster\n",
    "        topic_cluster_assigned_quantized_base_map = self.assign_clusters_to_points_of_df(quantized_base_map_df)\n",
    "        print('DONE: contextualized word map successfuyll aggregated to a base map.')\n",
    "        return topic_cluster_assigned_quantized_base_map\n",
    "    \n",
    "    def assign_clusters_to_points_of_df(self, dataframe):\n",
    "        '''dataframe: a pandas datframe that comes with the first two columns as lat lon meaning ['lat', 'lon', '..', '...']'''\n",
    "        print('--- assigning clusters to points of dataframe.')\n",
    "        # get a subframe that contains the coordinates in lat lon\n",
    "        coordinates_df = dataframe[['lat', 'lon']]\n",
    "        # convert the degrees into radians \n",
    "        coordinates_df['lat'] = coordinates_df['lat'] / (180/(3.14159))\n",
    "        coordinates_df['lon'] = coordinates_df['lon'] / (180/(3.14159))\n",
    "        # apply hdbscan\n",
    "        hdbscan_model = hdbscan.HDBSCAN(min_cluster_size=10, metric='haversine', cluster_selection_method='eom', prediction_data=True)\n",
    "        hdbscan_model.fit(coordinates_df)\n",
    "        #print(hdbscan_model.labels_.max())\n",
    "        #print(hdbscan_model.labels_)\n",
    "        #print(hdbscan_model.probabilities_)\n",
    "        # append the created labels and label probabilities to the original dataframe\n",
    "        dataframe['topic_label'] = hdbscan_model.labels_\n",
    "        dataframe['topic_label_probability'] = hdbscan_model.probabilities_\n",
    "        print('DONE: clusters successfuyll assigned to points of dataframe.')\n",
    "        return dataframe\n",
    "        \n",
    "    def get_topic_map(self, topic_level:int):\n",
    "        print('... extracting topic map out of whole semantic map.')\n",
    "        semantic_map_df = self.get_semantic_map()\n",
    "        # take the topic vectors out of the semantic map\n",
    "        reduced_topic_vectors_df = semantic_map_df.loc[semantic_map_df['word_2_doc_index'] == -1] #semantic_map_df[\"word_2_doc_index\"] < 0 \n",
    "        #print(reduced_topic_vectors_df.shape)\n",
    "        # TODO: select the topic vectors with the correct topic level (=ranges from -1 to -n, with different number of topics per level)\n",
    "        # \n",
    "        # get the topic size for every topic self.topic_model.get_topic_info().head(max_topic_number)\n",
    "        topic_list = list(self.topic_model.topics.keys())\n",
    "        #print(topic_list)\n",
    "        topic_list.sort()\n",
    "        topic_frequencies = []\n",
    "        for topic in topic_list:\n",
    "            topic_frequency = self.topic_model.get_topic_freq(topic)\n",
    "            topic_frequencies.append(topic_frequency)\n",
    "        #print(topic_frequencies)\n",
    "        # get the top 20 words for each topic\n",
    "        all_topic_word_lists = [self.topic_model.get_topic(topic) for topic in topic_list]\n",
    "        topic_words = []\n",
    "        for i, single_topic_words in enumerate(all_topic_word_lists):\n",
    "            single_topic_word_list = []\n",
    "            for word in single_topic_words:\n",
    "                single_topic_word_list.append(word[0])\n",
    "            topic_words.append(single_topic_word_list)\n",
    "        #print(topic_words)\n",
    "        # get the top 10 most similar documents for each topic\n",
    "        topic_vectors = self.data_object.topic_vectors.values\n",
    "        #print(topic_vectors)\n",
    "        #print(len(topic_vectors))\n",
    "        document_vectors = self.data_object.document_vectors.values\n",
    "        #print(document_vectors)\n",
    "        #print(len(document_vectors))\n",
    "        #Compute cosine-similarities for each topic vector with each of the document vectors\n",
    "        cosine_scores = util.pytorch_cos_sim(topic_vectors, document_vectors)\n",
    "        #print(len(cosine_scores))\n",
    "        #print(cosine_scores[0])\n",
    "        # for every topic vector find the n indices of the documents that are the most similar, found here: https://stackoverflow.com/questions/16878715/how-to-find-the-index-of-n-largest-elements-in-a-list-or-np-array-python\n",
    "        all_topic_n_most_similar_doc_uris = []\n",
    "        for single_top2doc_similarity_scores in cosine_scores:\n",
    "            single_top2doc_similarity_scores = np.array(single_top2doc_similarity_scores)\n",
    "            top_n_most_similar_doc_uris = single_top2doc_similarity_scores.argsort()[-5:] # here we use top 5\n",
    "            top_n_most_similar_doc_uris = top_n_most_similar_doc_uris.tolist() # make normal python list out of it to make sure we can easily JSON serialize it\n",
    "            all_topic_n_most_similar_doc_uris.append(top_n_most_similar_doc_uris)\n",
    "            #print(topic_scores_np_array)\n",
    "            #print(top_n_most_similar_doc_uris)\n",
    "        #print(all_topic_n_most_similar_doc_uris)\n",
    "        # get topic names based on most frequent words in the topic\n",
    "        #topic_names = []\n",
    "        #for topic in topic_list:\n",
    "        #    topic_name = self.topic_model.get_topic_info(topic)\n",
    "        #    topic_names.append(topic_name)\n",
    "        # get the topic id = LABEL => DONE\n",
    "        # QUANTIZE TOPIC VECTORS: same procedure as for contextualized word vectors ...\n",
    "        #reduced_topic_vectors_df = reduced_topic_vectors_df.round({'lat': 4, 'lon': 4})\n",
    "        #reduced_topic_vectors_df = reduced_topic_vectors_df.groupby(['word_2_doc_index','lat','lon'])['label'].apply(list)\n",
    "        #reduced_topic_vectors_df = reduced_topic_vectors_df.reset_index() \n",
    "        #reduced_topic_vectors_df.columns = ['word_2_doc_index', 'lat', 'lon', 'labels']\n",
    "        reduced_topic_vectors_df['topic_frequency'] = topic_frequencies\n",
    "        reduced_topic_vectors_df['topic_words'] = topic_words\n",
    "        reduced_topic_vectors_df['similar_document_uris'] = all_topic_n_most_similar_doc_uris\n",
    "        #print(reduced_topic_vectors_df.shape)\n",
    "        #print(reduced_topic_vectors_df.head(10)) \n",
    "        # get the topic size/freq, top 50 words, top 10 most similar documents(use \n",
    "        print('DONE: topic map successfully extracted.')\n",
    "        return reduced_topic_vectors_df\n",
    "    \n",
    "    def get_document_map(self):\n",
    "        #print('... extracting document map out of the whole semantic map.')\n",
    "        #semantic_map_df = self.get_semantic_map()\n",
    "        # take the document_vectors out of the semantic map\n",
    "        #reduced_document_vectors_df = semantic_map_df.loc[semantic_map_df['word_2_doc_index'] == -999]\n",
    "        # QUANTIZE: NO quantization for document vectors, columns look like: ['word_2_doc_index', 'lat', 'lon', 'labels']\n",
    "        #print('DONE: document_map successfully extracted.')\n",
    "        document_map_df = self.data_object.document_map\n",
    "        return document_map_df\n",
    "    \n",
    "    def get_paragraph_map(self):\n",
    "        print('...getting paragraph map from map creator object.')\n",
    "        paragraph_map_df = self.data_object.paragraph_map\n",
    "        # QUANTIZATION: TODO: discuss if we need a quantization of the paragraph map\n",
    "        print('DONE: paragraph map successfully extracted.')\n",
    "        return paragraph_map_df\n",
    "    \n",
    "    def get_contextualized_word_map(self):\n",
    "        contextualized_word_map_df = self.data_object.contextualized_word_map\n",
    "        return contextualized_word_map_df\n",
    "    \n",
    "    def assign_document_vectors_to_interactive_document_corpus(self):\n",
    "        print('...assigning document_vectors to documents in interactive_document_corpus')\n",
    "        # get the document map\n",
    "        reduced_document_vectors_df = self.get_document_map()\n",
    "        reduced_document_vectors_records = reduced_document_vectors_df.to_dict('records')\n",
    "        # for all the document_vectors\n",
    "        for index, record in enumerate(reduced_document_vectors_records):\n",
    "            document_vector_coordinates = [] # initialize the coordinates of the doc, format: [lon, lat]\n",
    "            lon = record['lon']\n",
    "            lat = record['lat']\n",
    "            document_vector_coordinates.append(lon)\n",
    "            document_vector_coordinates.append(lat)\n",
    "            self.data_object.interactive_document_corpus[index].document_vector = document_vector_coordinates\n",
    "        print('DONE: document_vectors successfully assigned to interactive document corpus.')\n",
    "        \n",
    "    def assign_paragraph_vectors_to_interactive_document_corpus(self):\n",
    "        print('...assigning paragraph_vectors to documents in interactive_document_corpus')\n",
    "         # get the paragraph vectors\n",
    "        reduced_paragraph_vectors_df = self.get_paragraph_map()\n",
    "        reduced_paragraph_vectors_records = reduced_paragraph_vectors_df.to_dict('records')\n",
    "        for index, record in enumerate(reduced_paragraph_vectors_records):\n",
    "            paragraph = {}\n",
    "            paragraph_vector_coordinates = []\n",
    "            lon = record['lon']\n",
    "            lat = record['lat']\n",
    "            paragraph_vector_coordinates.append(lon)\n",
    "            paragraph_vector_coordinates.append(lat)\n",
    "            paragraph['paragraph_vector'] = paragraph_vector_coordinates\n",
    "            paragraph['paragraph_keywords'] = record['paragraph_keywords']\n",
    "            document_index = self.data_object.paragraph_2_doc_index[index]\n",
    "            self.data_object.interactive_document_corpus[document_index].paragraphs.append(paragraph)\n",
    "        \n",
    "    def write_dataframe_to_file(self, dataframe, file_path, js_compatible=False):\n",
    "        print('... writing dataframe to filepath: {0}'.format(file_path))\n",
    "        all_df_records = dataframe.to_dict('records') # concatenated_umap_embedding_df.astype(str).to_dict('records')\n",
    "        with open(file_path, 'w') as output_file:\n",
    "            if(js_compatible):\n",
    "                file_name = os.path.basename(file_path)\n",
    "                # remove .txt\n",
    "                variable_name = file_name[:-4]\n",
    "                output_file.write('export const ' + str(variable_name) + ' = ')\n",
    "            json.dump(all_df_records, output_file)\n",
    "        print('DONE: writing dataframe to file completed.')\n",
    "        \n",
    "    def write_list_to_file(self, list_to_write, file_path, js_compatible=False):\n",
    "        print('... writing list to filepath: {0}'.format(file_path)) \n",
    "        with open(file_path, 'w') as output_file:\n",
    "            if(js_compatible):\n",
    "                file_name = os.path.basename(file_path)\n",
    "                # remove .txt\n",
    "                variable_name = file_name[:-4]\n",
    "                output_file.write('export const ' + str(variable_name) + ' = ')\n",
    "            json.dump(list_to_write, output_file)\n",
    "        print('DONE: writing list to file completed.')\n",
    "        \n",
    "    def write_interactive_document_corpus_to_file(self, interactive_document_corpus, file_path, remove_content=True, js_compatible=False):\n",
    "        print('...writing interactive document corpus to file.')\n",
    "        print('remove_content: {0}'.format(remove_content))\n",
    "        serialized_interactive_document_corpus = []\n",
    "        for i, interactive_document in enumerate(interactive_document_corpus):\n",
    "            # filter out the content (raw_text, preprocessed_text, ...), e.g. to reduce overhead in frontend \n",
    "            if(remove_content):\n",
    "                interactive_document.content = {}\n",
    "            serialized_interactive_document = interactive_document.dict()\n",
    "            serialized_interactive_document_corpus.append(serialized_interactive_document)\n",
    "            #interactive_document_corpus[i] = serialized_interactive_document\n",
    "        with open(file_path, 'w') as output_file:\n",
    "            if(js_compatible):\n",
    "                file_name = os.path.basename(file_path)\n",
    "                # remove .txt\n",
    "                variable_name = file_name[:-4]\n",
    "                output_file.write('export const ' + str(variable_name) + ' = ')\n",
    "            json.dump(serialized_interactive_document_corpus, output_file)\n",
    "        print('DONE: writing corpus objects list to file completed.')\n",
    "    \n",
    "    def trunc(self, values, decs=0):\n",
    "        return np.trunc(values*10**decs)/(10**decs)\n",
    "    \n",
    "    \n",
    "# TEST\n",
    "class TestMapCreator(unittest.TestCase):\n",
    "\n",
    "    @classmethod\n",
    "    def setUpClass(self):\n",
    "        # set all things up for the test series here\n",
    "        pass\n",
    "\n",
    "    @classmethod\n",
    "    def tearDownClass(self):\n",
    "        # tear everything down after testing here\n",
    "        pass\n",
    "\n",
    "    def test_class_setup_and_serialization(self):\n",
    "        # given\n",
    "        interactive_document_data_object = InteractiveDocument()\n",
    "        interactive_document_repository_impl = InteractiveDocumentRepositoryImpl(interactive_document_data_object)\n",
    "        # when\n",
    "        interactive_document_repository_impl.test()\n",
    "        print(interactive_document_repository_impl.data_object.dict())\n",
    "        # then\n",
    "        result = 6\n",
    "        self.assertEqual(result, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94685dd",
   "metadata": {},
   "source": [
    "# Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eea1d5ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.1\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Check GPU reachability \n",
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "# unicode error, tutorial found here: https://support.prodi.gy/t/unicodeencodeerror-during-training/955/3 and https://github.com/explosion/spaCy/issues/2570\n",
    "# set langugage to en us and encoding to utf-8\n",
    "#import locale\n",
    "#print(locale.getlocale())\n",
    "#locale.setlocale(locale.LC_ALL, 'en_US.UTF-8')\n",
    "#print(locale.getlocale())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe1ec4c",
   "metadata": {},
   "source": [
    "### Interactive Document Corpus Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "100682e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing manager constructed.\n",
      "PreprocessingManagerRepositoryImpl initialized.\n",
      "DONE: loading parsed documents from json completed.\n",
      "...parsing data from json to interactive document objects.\n",
      "INFO: source 570 of json data has no raw text SECTIONS.\n",
      "DONE: parse_data_from_json_to_interactive_document_object completed.\n",
      "... preprocessing raw text of whole corpus.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 603/603 [04:20<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: preprocessing raw text of whole corpus completed.\n",
      "...creating_keywords in whole corpus.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 603/603 [05:18<00:00,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: keywords in corpus successfully created.\n",
      "...writing interactive document corpus to file.\n",
      "remove_content: False\n",
      "DONE: writing corpus objects list to file completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# get basic instances\n",
    "preprocessing_manager_data_object = PreprocessingManager()\n",
    "preprocessing_manager_repository_impl = PreprocessingManagerRepositoryImpl(preprocessing_manager_data_object)\n",
    "\n",
    "# INTERACTIVE DOCUMENT CORPUS CREATION -------------------------------------------------\n",
    "# => First Parse PDFs from the folder into .json file using allenai science parse library, --> this may take a while -> if its done assert(False) hits -> then go on\n",
    "# create interactive document corpus from raw pdfs\n",
    "#interactive_document_corpus = preprocessing_manager_repository_impl.parse_pdfs_to_interactive_documents(pdf_files_folder)\n",
    "\n",
    "#assert(False)\n",
    "\n",
    "# ALTERNATIVE: alternatively load already parsed pdfs from json\n",
    "corpus_objects_list = preprocessing_manager_repository_impl.load_parsed_pdf_documents_from_json(allen_ai_parsed_output_json_file_path)\n",
    "#print(len(corpus_objects_list))\n",
    "#print(corpus_objects_list[50])\n",
    "#assert(False)\n",
    "\n",
    "# ALTERNATIVE\n",
    "# parse the data from the json objects into interactive document objects\n",
    "interactive_document_corpus = preprocessing_manager_repository_impl.parse_data_from_json_to_interactive_document_object(corpus_objects_list) #, corpus_doi_list)\n",
    "#print(interactive_document_corpus[10].dict())\n",
    "#assert(False)\n",
    "\n",
    "# ALTERNATIVE: parse data from csv into interactive document objects\n",
    "#interactive_document_corpus = preprocessing_manager_repository_impl.parse_data_from_csv_to_interactive_document_object(recovery_news_data_csv_file_path)\n",
    "#print(interactive_document_corpus[10].dict())\n",
    "\n",
    "#-------------------------------------------------------\n",
    "\n",
    "# PREPROCESSING -------------------------------------------------------\n",
    "# OPTIONAL: restrict the corpus size if needed\n",
    "#interactive_document_corpus = interactive_document_corpus [:1000]\n",
    "\n",
    "# preprocess the raw_text of every document\n",
    "interactive_document_corpus = preprocessing_manager_repository_impl.preprocess_raw_text_in_corpus(interactive_document_corpus)\n",
    "#print(interactive_document_corpus[10].content['preprocessed_text'])\n",
    "\n",
    "# extract the keywords of every document\n",
    "must_have_keyword = None #'ball' # None # select a specific keyword that MUST be in the list of keywords, e.g. if you have a specific interest in a certain word and its contexts\n",
    "interactive_document_corpus = preprocessing_manager_repository_impl.create_keywords_in_corpus(interactive_document_corpus, must_have_keyword=must_have_keyword)\n",
    "#print(interactive_document_corpus[10].keywords)\n",
    "\n",
    "# write interactive document corpus to file \n",
    "# for further MAP CREATION\n",
    "preprocessing_manager_repository_impl.write_interactive_document_corpus_to_file(interactive_document_corpus, interactive_document_corpus_full_file_path, remove_content=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8348572",
   "metadata": {},
   "source": [
    "### Map Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92108c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /mnt/local/2022_EMNLP_KeywordScape_Visual_Document_Exploration_using_Contextualized_Word_Embeddings/data/allenai_specter/. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Textprocessor: SpaCy initialized.\n",
      "Sentence Transformer Max Sequence Length:  512\n",
      "Sentence Transformer Tokenizer:  PreTrainedTokenizerFast(name_or_path='/mnt/local/2022_EMNLP_KeywordScape_Visual_Document_Exploration_using_Contextualized_Word_Embeddings/data/allenai_specter/', vocab_size=31116, model_max_len=1000000000000000019884624838656, is_fast=True, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n",
      "Sentence Transformer: allenai-specter initialized.\n",
      "MapCreatorRepositoryImpl initialized.\n",
      "... loading interactive_document_corpus from: /mnt/local/2022_EMNLP_KeywordScape_Visual_Document_Exploration_using_Contextualized_Word_Embeddings/datasets/emnlp_2021_full_papers_dataset/emnlp_corpus_full.txt\n",
      "loading interactive_document_corpus completed.\n",
      "... creating paragraph_corpus and doc_2_par indices.\n",
      "WARNING: no text paragraphs detected in document!\n",
      "WARNING: no text paragraphs detected in document!\n",
      "WARNING: no text paragraphs detected in document!\n",
      "WARNING: no text paragraphs detected in document!\n",
      "WARNING: no text paragraphs detected in document!\n",
      "DONE: paragraph corpus and doc_2_par indices successfully created.\n",
      "... creating topic model.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 9392/9392 [00:00<00:00, 2275336.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: topic model successfully created.\n",
      "...creating document vectors for corpus.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 603/603 [00:00<00:00, 1016137.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: document_vectors created successfully.\n",
      "...creating paragraph vectors for corpus.\n",
      "DONE: document_vectors created successfully.\n",
      "... creating contextualized word vectors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 603/603 [00:27<00:00, 22.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE: contextualized word vectors successfully created.\n",
      "... creating document map\n",
      "UMAP: dim reduction using HAVERSINE output metric.\n",
      "DONE: document map successfully created.\n",
      "... creating paragraph map.\n",
      "UMAP: dim reduction using HAVERSINE output metric.\n",
      "DONE: paragraph map successfully created.\n",
      "...creating contextualized_word_map.\n",
      "UMAP: dim reduction using HAVERSINE output metric.\n",
      "DONE: contextualized word map successfully created.\n",
      "...assigning document_vectors to documents in interactive_document_corpus\n",
      "DONE: document_vectors successfully assigned to interactive document corpus.\n",
      "...assigning paragraph_vectors to documents in interactive_document_corpus\n",
      "...getting paragraph map from map creator object.\n",
      "DONE: paragraph map successfully extracted.\n",
      "... writing list to filepath: /mnt/local/2022_EMNLP_KeywordScape_Visual_Document_Exploration_using_Contextualized_Word_Embeddings/datasets/emnlp_2021_full_papers_dataset/emnlp_corpus_topic_corpus.txt\n",
      "DONE: writing list to file completed.\n",
      ".. quantizing corpus map points to 0 number of decimals.\n",
      "DONE: corpus map points successfuyll quantized.\n",
      "... writing dataframe to filepath: /mnt/local/2022_EMNLP_KeywordScape_Visual_Document_Exploration_using_Contextualized_Word_Embeddings/datasets/emnlp_2021_full_papers_dataset/emnlp_corpus_corpus_points.txt\n",
      "DONE: writing dataframe to file completed.\n",
      ".. aggregating the contextualized word map to a base map\n",
      "--- assigning clusters to points of dataframe.\n",
      "DONE: clusters successfuyll assigned to points of dataframe.\n",
      "DONE: contextualized word map successfuyll aggregated to a base map.\n",
      "... writing dataframe to filepath: /mnt/local/2022_EMNLP_KeywordScape_Visual_Document_Exploration_using_Contextualized_Word_Embeddings/datasets/emnlp_2021_full_papers_dataset/emnlp_corpus_base_map_points.txt\n",
      "DONE: writing dataframe to file completed.\n",
      "...writing interactive document corpus to file.\n",
      "remove_content: True\n",
      "DONE: writing corpus objects list to file completed.\n"
     ]
    }
   ],
   "source": [
    "# get map creator instance and data object\n",
    "# get basic instances\n",
    "map_creator_data_object = MapCreator()\n",
    "map_creator_repository_impl = MapCreatorRepositoryImpl(map_creator_data_object)\n",
    "\n",
    "# get interactive_document_corpus\n",
    "interactive_document_corpus = map_creator_repository_impl.load_interactive_document_corpus(interactive_document_corpus_full_file_path, \"load_from_file\")\n",
    "\n",
    "# map interactive_document_corpus to paragraph_corpus || build doc2par and par2doc indices\n",
    "map_creator_repository_impl.create_paragraph_corpus_and_doc_2_par_indices(interactive_document_corpus, paragraph_max_length)\n",
    "#print(map_creator_repository_impl.data_object.paragraph_corpus[0])\n",
    "#print(map_creator_repository_impl.data_object.doc_2_paragraph_index[0])\n",
    "#print(map_creator_repository_impl.data_object.paragraph_2_doc_index[0])\n",
    "\n",
    "#assert(False)\n",
    "\n",
    "# create topic model from paragraph corpus\n",
    "# set the number of top n words for each topic to give back \n",
    "top_n_words = 20 \n",
    "map_creator_repository_impl.create_topic_model(top_n_words)\n",
    "#max_topic_number = 5\n",
    "#info_df = map_creator_repository_impl.get_topic_info(max_topic_number)\n",
    "#print(info_df)\n",
    "\n",
    "# create and get topic embedding vectors\n",
    "#map_creator_repository_impl.create_topic_vectors()\n",
    "#topic_vectors_df = map_creator_repository_impl.get_topic_vectors()\n",
    "#print(topic_vectors[0])\n",
    "#print(len(map_creator_repository_impl.data_object.topic_labels)) \n",
    "\n",
    "# create document_vectors\n",
    "map_creator_repository_impl.create_document_vectors()\n",
    "#document_vectors = map_creator_repository_impl.get_document_vectors()\n",
    "#print(document_vectors.shape)\n",
    "#print(document_vectors[0])\n",
    "#print(len(map_creator_repository_impl.data_object.document_vectors))\n",
    "\n",
    "# create paragraph_vectors\n",
    "map_creator_repository_impl.create_paragraph_vectors()\n",
    "#paragraph_vectors = map_creator_repository_impl.get_paragraph_vectors()\n",
    "#print(paragraph_vectors.shape)\n",
    "\n",
    "# create and get contextualized word embedding vectors\n",
    "map_creator_repository_impl.create_contextualized_word_vectors(use_summaries_only=True)\n",
    "#contextualized_word_vectors = map_creator_repository_impl.get_contextualized_word_vectors()\n",
    "#print(contextualized_word_vectors.shape)\n",
    "#print(contextualized_word_vectors[0])\n",
    "#print(len(map_creator_repository_impl.data_object.contextualized_word_labels))\n",
    "#print(len(map_creator_repository_impl.data_object.contextualized_word_2_doc_index))\n",
    "\n",
    "# SANITY CHECK: plot points on 3D sphere or 2D projection of sphere for checking if everything worked out\n",
    "# plot the semantic map on 3d sphere, 2d sphere projection and 2d plane for VERIFICATION\n",
    "#topic_vectors_df = map_creator_repository_impl.get_topic_vectors()\n",
    "#print(topic_vectors_df.shape)\n",
    "#contextualized_word_vectors_df = map_creator_repository_impl.get_contextualized_word_vectors()\n",
    "#print(contextualized_word_vectors_df.shape)\n",
    "#document_vectors_df = map_creator_repository_impl.get_document_vectors()\n",
    "#print(document_vectors_df.shape)\n",
    "#paragraph_vectors_df = map_creator_repository_impl.get_paragraph_vectors()\n",
    "#print(paragraph_vectors_df.shape)\n",
    "#semantic_map_df = map_creator_repository_impl.get_semantic_map()\n",
    "#print(map_creator_repository_impl.data_object.semantic_map.shape)\n",
    "#map_creator_repository_impl.plot_dataframe(paragraph_vectors_df)\n",
    "\n",
    "# create document map\n",
    "map_creator_repository_impl.create_document_map()\n",
    "#document_map = map_creator_repository_impl.get_document_map()\n",
    "#print(document_map.head())\n",
    "\n",
    "# create paragraph map\n",
    "map_creator_repository_impl.create_paragraph_map()\n",
    "#paragraph_map = map_creator_repository_impl.get_paragraph_map()\n",
    "#print(paragraph_map.head())\n",
    "#map_creator_repository_impl.write_dataframe_to_file(paragraph_map, interactive_document_corpus_paragraph_map_points_file_path)\n",
    "\n",
    "# create contextualized word map\n",
    "map_creator_repository_impl.create_contextualized_word_map()\n",
    "#contextualized_word_map = map_creator_repository_impl.get_contextualized_word_map()\n",
    "#print(contextualized_word_map.head())\n",
    "\n",
    "#assert(False) # break point\n",
    "\n",
    "# get topic map and write topic map to file\n",
    "#topic_level = -1\n",
    "#topic_map = map_creator_repository_impl.get_topic_map(topic_level)\n",
    "#map_creator_repository_impl.write_dataframe_to_file(topic_map, interactive_document_corpus_topic_map_points_file_path)\n",
    "\n",
    "# assign the document vectors to the interactive document corpus\n",
    "#document_map = map_creator_repository_impl.get_document_map()\n",
    "#print(document_map.head())\n",
    "map_creator_repository_impl.assign_document_vectors_to_interactive_document_corpus()\n",
    "map_creator_repository_impl.assign_paragraph_vectors_to_interactive_document_corpus()\n",
    "#interactive_document_corpus = map_creator_repository_impl.data_object.interactive_document_corpus\n",
    "#print(interactive_document_corpus[0])\n",
    "\n",
    "# get topic corpus and write topic corpus to file\n",
    "topic_corpus = map_creator_repository_impl.get_topic_corpus()\n",
    "map_creator_repository_impl.write_list_to_file(topic_corpus, interactive_document_corpus_topic_corpus_file_path, js_compatible=True)\n",
    "\n",
    "# get quantized corpus and write corpus to file \n",
    "number_of_decimals = 0 # 0, 1, 2 works well -> depends on how many documents you want to map. \n",
    "corpus_points = map_creator_repository_impl.get_quantized_corpus(number_of_decimals)\n",
    "map_creator_repository_impl.write_dataframe_to_file(corpus_points, interactive_document_corpus_corpus_points_file_path, js_compatible=True)\n",
    "\n",
    "# get quantized base map (topic clusters already assigned in quantization process) and write to file\n",
    "number_of_decimals = 0# 0, 1, 2 works well -> depends on how many documents you want to map. \n",
    "base_map = map_creator_repository_impl.get_quantized_base_map(number_of_decimals)\n",
    "#print(base_map.head(25))\n",
    "#print(base_map.tail(25))\n",
    "map_creator_repository_impl.write_dataframe_to_file(base_map, interactive_document_corpus_base_map_points_file_path, js_compatible=True)\n",
    "\n",
    "# write interactive_document_corpus to file for final usage in frontend\n",
    "# for FRONTEND USAGE\n",
    "interactive_document_corpus = map_creator_repository_impl.data_object.interactive_document_corpus\n",
    "map_creator_repository_impl.write_interactive_document_corpus_to_file(interactive_document_corpus, interactive_document_corpus_file_path, remove_content=True, js_compatible=True) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interactive_documents_env_kernel",
   "language": "python",
   "name": "interactive_documents_env_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
