export const vis_2020_proceedings_corpus = [{"uri": "0", "title": "Visual cohort comparison for spatial single-cell omics-data", "timestamp": "2020", "rating": "0.0", "annotation": "", "tags": [], "authors": ["Antonios Somarakis", "Marieke E. Ijsselsteijn", "Sietse J. Luk", "Boyd Kenkhuis", "Noel F.C.C. de Miranda", "Boudewijn P.F. Lelieveldt", "Thomas H\u00f6llt"], "summary": "Spatially-resolved omics-data enable researchers to precisely distinguish cell types in tissue and explore their spatial interactions, enabling deep understanding of tissue functionality. To understand what causes or deteriorates a disease and identify related biomarkers, clinical researchers regularly perform large-scale cohort studies, requiring the comparison of such data at cellular level. In such studies, with little a-priori knowledge of what to expect in the data, explorative data analysis is a necessity. Here, we present an interactive visual analysis workflow for the comparison of cohorts of spatially-resolved omics-data. Our workflow allows the comparative analysis of two cohorts based on multiple levels-of-detail, from simple abundance of contained cell types over complex co-localization patterns to individual comparison of complete tissue images. As a result, the workflow enables the identification of cohort-differentiating features, as well as outlier samples at any stage of the workflow. During the development of the workflow, we continuously consulted with domain experts. To show the effectiveness of the workflow, we conducted multiple case studies with domain experts from different application areas and with different data modalities.", "keywords": ["exploration", "use", "image", "sample", "cell", "user", "comparison", "tissue", "type", "microenvironments", "tumor", "compare", "heatmap", "-", "raincloud", "information", "workflow", "fig", "expert", "combination", "work", "abundance", "plot", "cohort", "difference", "provide", "based", "case", "data", "microenvironment", "t", "identified", "set", "analysis", "example", "study", "sect"], "document_vector": [-121.973487, -27.076164], "paragraphs": [{"paragraph_vector": [-123.072715, 11.005924], "paragraph_keywords": ["cell", "cohorts", "data", "omics"]}, {"paragraph_vector": [-123.029777, 8.71302], "paragraph_keywords": ["cohorts", "data", "approaches", "comparison"]}, {"paragraph_vector": [-122.195114, 9.818842], "paragraph_keywords": ["cell", "exploration", "data", "tools"]}, {"paragraph_vector": [-122.477935, 10.141596], "paragraph_keywords": ["comparison", "data", "information", "cohorts"]}, {"paragraph_vector": [-121.324623, 10.613542], "paragraph_keywords": ["data", "cohorts", "cell", "samples"]}, {"paragraph_vector": [-119.508827, 9.322895], "paragraph_keywords": ["cell", "cells", "microenvironment", "types"]}, {"paragraph_vector": [-120.436309, 13.089563], "paragraph_keywords": ["cohort", "cell", "cells", "cohorts"]}, {"paragraph_vector": [-119.689987, 11.2809], "paragraph_keywords": ["cell", "cohort", "types", "microenvironments"]}, {"paragraph_vector": [-121.611457, 11.271105], "paragraph_keywords": ["samples", "support", "cohorts", "cell"]}, {"paragraph_vector": [-120.820526, 11.523864], "paragraph_keywords": ["cohorts", "plots", "cell", "types"]}, {"paragraph_vector": [-120.312004, 11.5585], "paragraph_keywords": ["cell", "types", "plots", "-"]}, {"paragraph_vector": [-120.241363, 12.078032], "paragraph_keywords": ["cell", "cohort", "microenvironments", "based"]}, {"paragraph_vector": [-120.116249, 11.540366], "paragraph_keywords": ["combination", "heatmap", "cohorts", "cell"]}, {"paragraph_vector": [102.977836, -51.028331], "paragraph_keywords": ["cell", "microenvironment", "type", "types"]}, {"paragraph_vector": [-119.761383, 12.96746], "paragraph_keywords": ["cell", "plot", "type", "plots"]}, {"paragraph_vector": [-121.729774, 12.854386], "paragraph_keywords": ["cell", "tissue", "application", "types"]}, {"paragraph_vector": [-120.718917, 12.471219], "paragraph_keywords": ["cell", "studies", "participants", "case"]}, {"paragraph_vector": [-121.145408, 12.310347], "paragraph_keywords": ["cell", "cells", "types", "t"]}, {"paragraph_vector": [-120.167984, 11.507554], "paragraph_keywords": ["cells", "cohort", "expert", "plot"]}, {"paragraph_vector": [-119.725891, 13.98882], "paragraph_keywords": ["heatmap", "macrophages", "cell", "cohort"]}, {"paragraph_vector": [-122.265304, 11.124816], "paragraph_keywords": ["cell", "types", "cohort", "t"]}, {"paragraph_vector": [-121.989196, 13.883136], "paragraph_keywords": ["cells", "t", "tumor", "plot"]}, {"paragraph_vector": [-122.21585, 13.207418], "paragraph_keywords": ["cells", "microglia", "images", "study"]}, {"paragraph_vector": [-117.000137, 16.71154], "paragraph_keywords": ["questionnaire", "subtype", "block", "feedback"]}, {"paragraph_vector": [-120.091537, 10.233161], "paragraph_keywords": ["cell", "cohorts", "feedback", "abundance"]}, {"paragraph_vector": [-120.484626, 12.373895], "paragraph_keywords": ["images", "data", "cell", "microenvironments"]}, {"paragraph_vector": [-118.399955, 14.958612], "paragraph_keywords": ["research", "cell", "cohorts", "funding"]}], "content": {}, "doi": "10.1109/TVCG.2020.3029412"}, {"uri": "1", "title": "II-20: Intelligent and pragmatic analytic categorization of image collections", "timestamp": "2020", "rating": "0.0", "annotation": "", "tags": [], "authors": ["Jan Zah\u00e1lka", "Marcel Worring", "Jarke J. van Wijk"], "summary": "In this paper, we introduce II-20 (Image Insight 2020), a multimedia analytics approach for analytic categorization of image collections. Advanced visualizations for image collections exist, but they need tight integration with a machine model to support the task of analytic categorization. Directly employing computer vision and interactive learning techniques gravitates towards search. Analytic categorization, however, is not machine classification (the difference between the two is called the pragmatic gap): a human adds/redefines/deletes categories of relevance on the fly to build insight, whereas the machine classifier is rigid and non-adaptive. Analytic categorization that truly brings the user to insight requires a flexible machine model that allows dynamic sliding on the exploration-search axis, as well as semantic interactions: a human thinks about image data mostly in semantic terms. II-20 brings three major contributions to multimedia analytics on image collections and towards closing the pragmatic gap. Firstly, a new machine model that closely follows the user\u2019s interactions and dynamically models her categories of relevance. II-20\u2019s machine model, in addition to matching and exceeding the state of the art\u2019s ability to produce relevant suggestions, allows the user to dynamically slide on the exploration-search axis without any additional input from her side. Secondly, the dynamic, 1-image-at-a-time Tetris metaphor that synergizes with the model. It allows a well-trained model to analyze the collection by itself with minimal interaction from the user and complements the classic grid metaphor. Thirdly, the fast-forward interaction, allowing the user to harness the model to quickly expand (\u201cfast-forward\u201d) the categories of relevance, expands the multimedia analytics semantic interaction dictionary. Automated experiments show that II-20\u2019s machine model outperforms the existing state of the art and also demonstrate the Tetris metaphor\u2019s analytic quality. User studies further confirm that II-20 is an intuitive, efficient, and effective multimedia analytics tool.", "keywords": ["feedback", "exploration", "use", "b", "image", "metaphor", "categorization", "feature", "vope", "user", "insight", "machine", "tetri", "f", "mode", "search", "model", "suggestion", "learning", "ui", "provides", "actor", "number", "content", "collection", "multimedia", "control", "category", "precision", "session", "forward", "k", "analytics", "interaction", "approach", "oracle", "task", "relevance", "dataset", "celeba", "based", "case", "bucket", "datasets", "video", "axis", "data", "recall", "time", "performance", "quality", "classifier", "set", "grid", "study", "interface", "experiment"], "document_vector": [118.246223, -49.220207], "paragraphs": [{"paragraph_vector": [24.572994, 50.840961], "paragraph_keywords": ["university", "data", "ieee", "multimedia"]}, {"paragraph_vector": [39.031147, 49.550666], "paragraph_keywords": ["content", "analytics", "user", "collection"]}, {"paragraph_vector": [35.417888, 46.110408], "paragraph_keywords": ["model", "interactions", "multimedia", "search"]}, {"paragraph_vector": [31.907144, 46.937572], "paragraph_keywords": ["model", "user", "analytics", "image"]}, {"paragraph_vector": [32.020137, 46.72163], "paragraph_keywords": ["based", "user", "index", "analytics"]}, {"paragraph_vector": [31.917932, 47.152725], "paragraph_keywords": ["learning", "machine", "analytics", "model"]}, {"paragraph_vector": [33.771633, 45.184875], "paragraph_keywords": ["images", "buckets", "model", "user"]}, {"paragraph_vector": [34.30495, 46.960971], "paragraph_keywords": ["user", "model", "image", "bucket"]}, {"paragraph_vector": [31.869861, 47.658786], "paragraph_keywords": ["bucket", "user", "model", "images"]}, {"paragraph_vector": [37.319293, 49.89513], "paragraph_keywords": ["image", "images", "user", "model"]}, {"paragraph_vector": [32.748386, 47.8055], "paragraph_keywords": ["bucket", "images", "model", "user"]}, {"paragraph_vector": [34.277206, 45.40567], "paragraph_keywords": ["representation", "features", "image", "centroid"]}, {"paragraph_vector": [29.027202, 45.425762], "paragraph_keywords": ["bucket", "search", "images", "confidence"]}, {"paragraph_vector": [29.855304, 47.272769], "paragraph_keywords": ["images", "distance", "b", "bucket"]}, {"paragraph_vector": [27.238945, 47.596481], "paragraph_keywords": ["images", "bucket", "set", "b"]}, {"paragraph_vector": [29.702745, 47.255176], "paragraph_keywords": ["images", "oracle", "classifier", "\u03c3b"]}, {"paragraph_vector": [29.38397, 46.96503], "paragraph_keywords": ["search", "pclass", "suggestions", "model"]}, {"paragraph_vector": [26.718736, 49.005741], "paragraph_keywords": ["images", "f", "bucket", "model"]}, {"paragraph_vector": [22.463041, 60.615119], "paragraph_keywords": ["dataset", "model", "experiments", "vope"]}, {"paragraph_vector": [24.899969, 56.125156], "paragraph_keywords": ["images", "learning", "use", "dataset"]}, {"paragraph_vector": [23.990921, 53.804691], "paragraph_keywords": ["images", "relevance", "actors", "celeba"]}, {"paragraph_vector": [89.189483, 64.578559], "paragraph_keywords": ["user", "actor", "image", "model"]}, {"paragraph_vector": [166.829132, 75.576049], "paragraph_keywords": ["user", "tetris", "insight", "users"]}, {"paragraph_vector": [138.544769, 57.404506], "paragraph_keywords": ["user", "baselines", "time", "celeba"]}, {"paragraph_vector": [38.750949, 52.202205], "paragraph_keywords": ["tetris", "grid", "positives", "precision"]}, {"paragraph_vector": [10.133411, 53.625919], "paragraph_keywords": ["user", "content", "vope", "insights"]}, {"paragraph_vector": [130.147872, 27.872503], "paragraph_keywords": ["users", "user", "tetris", "metaphor"]}, {"paragraph_vector": [32.656311, 46.899772], "paragraph_keywords": ["user", "datasets", "images", "interaction"]}, {"paragraph_vector": [34.746681, 49.24596], "paragraph_keywords": ["model", "user", "image", "suggestions"]}, {"paragraph_vector": [-114.757812, 72.305313], "paragraph_keywords": ["ieee", "use", "republication", "redistribution"]}], "content": {}, "doi": "10.1109/TVCG.2020.3029413"}, {"uri": "2", "title": "A Structured Review of Data Management Technology for Interactive Visualization and Analysis", "timestamp": "2020", "rating": "0.0", "annotation": "", "tags": [], "authors": ["Leilani Battle", "Carlos Scheidegger"], "summary": "In the last two decades, interactive visualization and analysis have become a central tool in data-driven decision making. Concurrently to the contributions in data visualization, research in data management has produced technology that directly benefits interactive analysis. Here, we contribute a systematic review of 30 years of work in this adjacent field, and highlight techniques and principles we believe to be underappreciated in visualization work. We structure our review along two axes. First, we use task taxonomies from the visualization literature to structure the space of interactions in usual systems. Second, we created a categorization of data management work that strikes a balance between specificity and generality. Concretely, we contribute a characterization of 131 research papers along these two axes. We find that five notions in data management venues fit interactive visualization systems well: materialized views, approximate query processing, user modeling and query prediction, multi-query optimization, lineage techniques, and indexing techniques. In addition, we find a preponderance of work in materialized views and approximate query processing, most targeting a limited subset of the interaction tasks in the taxonomy we used. This suggests natural avenues of future research both in visualization and data management. Our categorization both changes how we visualization researchers design and build our systems, and highlights where future work is necessary.", "keywords": ["way", "agrawal", "processing", "type", "structure", "community", "strategy", "olap", "analytics", "interaction", "indexing", "work", "xu", "access", "materialized", "time", "sampling", "chang", "use", "exploration", "literature", "user", "review", "liu", "improve", "al", "parameswaran", "index", "-", "query", "process", "approach", "battle", "modeling", "algorithm", "cube", "case", "datasets", "data", "scheidegger", "et", "ieee", "applied", "design", "database", "wang", "aggregation", "benefit", "scale", "search", "system", "research", "level", "information", "wu", ".", "existing", "based", "performance", "result", "given", "survey", "example", "li", "optimization", "management", "provenance", "problem", "support", "chen", "technique", "paper", "provide", "visualization", "computation", "view", "method", "heer", "develop", "dbms", "analysis"], "document_vector": [-61.416137, 21.426275], "paragraphs": [{"paragraph_vector": [-70.287216, -86.673706], "paragraph_keywords": ["visualization", "user", "systems", "constraints"]}, {"paragraph_vector": [-13.854067, -84.885566], "paragraph_keywords": ["visualization", "data", "dbms", "found"]}, {"paragraph_vector": [125.367851, -89.140731], "paragraph_keywords": ["visualization", "data", "programmers", "dbms"]}, {"paragraph_vector": [-24.418409, -87.028175], "paragraph_keywords": ["data", "visualization", "provide", "surveys"]}, {"paragraph_vector": [-144.813201, -88.100471], "paragraph_keywords": ["papers", "visualization", "data", "database"]}, {"paragraph_vector": [25.792518, -73.506065], "paragraph_keywords": ["papers", "database", "topics", "data"]}, {"paragraph_vector": [42.815235, -88.119834], "paragraph_keywords": ["use", "visualization", "interaction", "data"]}, {"paragraph_vector": [-171.558334, -71.594459], "paragraph_keywords": ["data", "tasks", "taxonomies", "process"]}, {"paragraph_vector": [3.492283, -79.041435], "paragraph_keywords": ["optimization", "database", "interaction", "papers"]}, {"paragraph_vector": [6.587559, -65.699234], "paragraph_keywords": ["results", "data", "view", "queries"]}, {"paragraph_vector": [7.671696, -59.725227], "paragraph_keywords": ["results", "data", "database", "users"]}, {"paragraph_vector": [-3.799844, -42.846157], "paragraph_keywords": ["data", "visualization", "user", "users"]}, {"paragraph_vector": [-4.813601, -73.907913], "paragraph_keywords": ["data", "system", "requests", "user"]}, {"paragraph_vector": [13.530457, -69.587081], "paragraph_keywords": ["query", "plans", "data", "queries"]}, {"paragraph_vector": [-0.225221, -74.830406], "paragraph_keywords": ["provenance", "data", "seedb", "system"]}, {"paragraph_vector": [-30.520044, -80.17144], "paragraph_keywords": ["data", "database", "structures", "provenance"]}, {"paragraph_vector": [40.403861, -67.371421], "paragraph_keywords": ["interaction", "kyrix", "level", "optimizations"]}, {"paragraph_vector": [1.242255, -67.638412], "paragraph_keywords": ["queries", "interaction", "visualization", "views"]}, {"paragraph_vector": [-165.098403, -74.60231], "paragraph_keywords": ["interactions", "user", "annotate", "performance"]}, {"paragraph_vector": [-13.033213, -82.533569], "paragraph_keywords": ["visualization", "data", "systems", "results"]}, {"paragraph_vector": [-13.821302, -82.403579], "paragraph_keywords": ["provenance", "visualization", "data", "user"]}, {"paragraph_vector": [-7.293961, -77.648529], "paragraph_keywords": ["user", "methods", "research", "optimization"]}, {"paragraph_vector": [-21.676584, -80.062438], "paragraph_keywords": ["optimization", "user", "data", "methods"]}, {"paragraph_vector": [12.698801, -72.136528], "paragraph_keywords": ["optimization", "performance", "distributed", "visualization"]}, {"paragraph_vector": [105.57743, -85.625778], "paragraph_keywords": ["interaction", "optimizations", "systems", "dbmss"]}, {"paragraph_vector": [-23.79491, -84.562896], "paragraph_keywords": ["visualization", "research", "problems", "benefits"]}, {"paragraph_vector": [0.283731, -86.671295], "paragraph_keywords": ["work", "optimization", "mqo", "provenance"]}, {"paragraph_vector": [18.962841, -74.132392], "paragraph_keywords": ["visualization", "analysis", "work", "database"]}, {"paragraph_vector": [13.730088, -60.802356], "paragraph_keywords": ["data", "visualization", "analysis", "battle"]}, {"paragraph_vector": [8.65919, -55.812961], "paragraph_keywords": ["data", "sigmod", "ieee", "use"]}, {"paragraph_vector": [11.318235, -56.85102], "paragraph_keywords": ["exploration", "knowledge", "recommending", "madhavan"]}, {"paragraph_vector": [16.142107, -59.838752], "paragraph_keywords": ["data", "based", "querying", "visualization"]}, {"paragraph_vector": [10.590452, -56.72261], "paragraph_keywords": ["data", "visualization", "analysis", "queries"]}, {"paragraph_vector": [13.043272, -61.862312], "paragraph_keywords": ["ieee", "data", "database", "learning"]}, {"paragraph_vector": [11.645473, -55.54261], "paragraph_keywords": ["data", "visualization", "li", "sampling"]}, {"paragraph_vector": [17.030492, -53.455329], "paragraph_keywords": ["data", "time", "liu", "datasets"]}, {"paragraph_vector": [9.981073, -57.466312], "paragraph_keywords": ["data", "visualizations", "exploration", "design"]}, {"paragraph_vector": [10.718483, -58.946266], "paragraph_keywords": ["data", "ieee", ".", "visualizations"]}, {"paragraph_vector": [8.955749, -56.753662], "paragraph_keywords": ["data", "editors", "agrawal", "cubes"]}, {"paragraph_vector": [13.048733, -60.799179], "paragraph_keywords": ["data", "visualization", "query", "wang"]}, {"paragraph_vector": [16.579965, -53.782497], "paragraph_keywords": ["data", "exploration", "time", "visualization"]}, {"paragraph_vector": [7.686869, -52.504173], "paragraph_keywords": ["time", "data", "su", "olap"]}], "content": {}, "doi": "10.1109/TVCG.2020.3028948"}, {"uri": "3", "title": "Supporting the Problem-Solving Loop: Designing Highly Interactive Optimisation Systems", "timestamp": "2020", "rating": "0.0", "annotation": "", "tags": [], "authors": ["Jie Liu", "Tim Dwyer", "Guido Tack", "Samuel Gratzl", "Kim Marriott"], "summary": "Efficient optimisation algorithms have become important tools for finding high-quality solutions to hard, real-world problems such as production scheduling, timetabling, or vehicle routing. These algorithms are typically \u201cblack boxes\u201d that work on mathematical models of the problem to solve. However, many problems are difficult to fully specify, and require a \u201chuman in the loop\u201d who collaborates with the algorithm by refining the model and guiding the search to produce acceptable solutions. Recently, the Problem-Solving Loop was introduced as a high-level model of such interactive optimisation. Here, we present and evaluate nine recommendations for the design of interactive visualisation tools supporting the Problem-Solving Loop. They range from the choice of visual representation for solutions and constraints to the use of a solution gallery to support exploration of alternate solutions. We first examined the applicability of the recommendations by investigating how well they had been supported in previous interactive optimisation tools. We then evaluated the recommendations in the context of the vehicle routing problem with time windows (VRPTW). To do so we built a sophisticated interactive visual system for solving VRPTW that was informed by the recommendations. Ten participants then used this system to solve a variety of routing problems. We report on participant comments and interaction patterns with the tool. These showed the tool was regarded as highly usable and the results generally supported the usefulness of the underlying recommendations.", "keywords": ["use", "feedback", "modify", "found", "represented", "truck", "solution", "user", "problem", "order", "evaluate", "model", "system", "optimisation", "support", "participant", "understand", "al", "supported", "-", "value", "constraint", "interaction", "process", "service", "map", "need", "provide", "customer", "visualisation", "tool", "window", "making", "solver", "time", "et", "asked", "scenario", "design", "find", "gallery", "interface", "solving", "recommendation"], "document_vector": [81.303138, 78.87886], "paragraphs": [{"paragraph_vector": [-95.122726, 14.504329], "paragraph_keywords": ["optimisation", "user", "solutions", "recommendations"]}, {"paragraph_vector": [-92.660591, 14.236872], "paragraph_keywords": ["tool", "problem", "solution", "optimisation"]}, {"paragraph_vector": [-89.187744, 16.045036], "paragraph_keywords": ["user", "optimisation", "model", "survey"]}, {"paragraph_vector": [-95.325897, 14.440043], "paragraph_keywords": ["optimisation", "system", "design", "al"]}, {"paragraph_vector": [-147.188735, 26.083389], "paragraph_keywords": ["analytics", "user", "sense", "making"]}, {"paragraph_vector": [-96.324943, 14.728626], "paragraph_keywords": ["optimisation", "problem", "loop", "model"]}, {"paragraph_vector": [-98.05233, 13.228688], "paragraph_keywords": ["solution", "optimisation", "solutions", "problem"]}, {"paragraph_vector": [-97.315299, 14.81918], "paragraph_keywords": ["constraints", "optimisation", "solutions", "model"]}, {"paragraph_vector": [-97.686798, 12.555728], "paragraph_keywords": ["solutions", "users", "user", "solution"]}, {"paragraph_vector": [-95.843734, 12.290895], "paragraph_keywords": ["solutions", "solution", "user", "parts"]}, {"paragraph_vector": [-95.650733, 13.04406], "paragraph_keywords": ["solution", "users", "algorithms", "feedback"]}, {"paragraph_vector": [-99.321197, 10.824971], "paragraph_keywords": ["systems", "problem", "et", "al"]}, {"paragraph_vector": [-90.252449, 7.854656], "paragraph_keywords": ["users", "systems", "optimisation", "design"]}, {"paragraph_vector": [-92.728515, 13.280788], "paragraph_keywords": ["problem", "optimisation", "recommendations", "solution"]}, {"paragraph_vector": [-91.049011, 13.262736], "paragraph_keywords": ["solution", "truck", "solutions", "system"]}, {"paragraph_vector": [-84.674453, 15.248198], "paragraph_keywords": ["time", "map", "truck", "customer"]}, {"paragraph_vector": [-83.576484, 14.445227], "paragraph_keywords": ["window", "period", "time", "service"]}, {"paragraph_vector": [-93.702056, 12.057029], "paragraph_keywords": ["solution", "solutions", "customer", "recommendation"]}, {"paragraph_vector": [-92.291015, 11.391645], "paragraph_keywords": ["solution", "recommendation", "users", "-"]}, {"paragraph_vector": [-95.843948, 12.024736], "paragraph_keywords": ["solution", "participants", "tool", "optimisation"]}, {"paragraph_vector": [-91.433731, 15.856011], "paragraph_keywords": ["scenario", "customer", "use", "participants"]}, {"paragraph_vector": [-93.308067, 11.116115], "paragraph_keywords": ["participants", "asked", "truck", "system"]}, {"paragraph_vector": [-90.125595, 14.774388], "paragraph_keywords": ["usability", "solution", "scenarios", "score"]}, {"paragraph_vector": [-89.957176, 13.951007], "paragraph_keywords": ["participants", "customers", "solutions", "-"]}, {"paragraph_vector": [-88.926338, 14.970441], "paragraph_keywords": ["-", "participants", "optimisation", "customer"]}, {"paragraph_vector": [-97.309082, 13.885472], "paragraph_keywords": ["participants", "recommendation", "map", "customer"]}, {"paragraph_vector": [-92.221817, 12.545169], "paragraph_keywords": ["solution", "-", "participants", "truck"]}, {"paragraph_vector": [-96.952629, 11.697794], "paragraph_keywords": ["feedback", "progress", "participant", "wheel"]}, {"paragraph_vector": [-96.726264, 12.282281], "paragraph_keywords": ["solutions", "participants", "solution", "participant"]}, {"paragraph_vector": [-91.58673, 16.346071], "paragraph_keywords": ["solution", "solutions", "time", "gallery"]}, {"paragraph_vector": [-93.310104, 13.233052], "paragraph_keywords": ["recommendations", "solutions", "solution", "gallery"]}, {"paragraph_vector": [-96.880989, 12.671255], "paragraph_keywords": ["required", "optimisation", "ieee", "support"]}], "content": {}, "doi": "10.1109/TVCG.2020.3028953"}, {"uri": "4", "title": "PipelineProfiler : A Visual Analytics Tool for the Exploration of AutoML Pipelines", "timestamp": "2020", "rating": "0.0", "annotation": "", "tags": [], "authors": ["Jorge Piazentin Ono", "Sonia Castelo", "Roque Lopez", "Enrico Bertini", "Juliana Freire", "Claudio Silva"], "summary": "In recent years, a wide variety of automated machine learning (AutoML) methods have been proposed to generate end-to-end ML pipelines. While these techniques facilitate the creation of models, given their black-box nature, the complexity of the underlying algorithms, and the large number of pipelines they derive, they are difficult for developers to debug. It is also challenging for machine learning experts to select an AutoML system that is well suited for a given problem. In this paper, we present the PipelineProfiler , an interactive visualization tool that allows the exploration and comparison of the solution space of machine learning (ML) pipelines produced by AutoML systems. PipelineProfiler is integrated with Jupyter Notebook and can be combined with common data science tools to enable a rich set of analyses of the ML pipelines, providing users a better understanding of the algorithms that generated them as well as insights into how they can be improved. We demonstrate the utility of our tool through use cases where PipelineProfiler is used to better understand and improve a real-world AutoML system. Furthermore, we validate our approach by presenting a detailed analysis of a think-aloud experiment with six data scientists who develop and evaluate AutoML tools.", "keywords": ["use", "score", "exploration", "feature", "pipelineprofiler", "ml", "primitive", "user", "graph", "problem", "classification", "derived", "comparison", "insight", "type", "search", "structure", "system", "model", "end", "number", "hyperparameters", "participant", "pipeline", "value", "information", "produced", "space", "expert", "m", "matrix", "developer", "show", "hyperparameter", "algorithm", "explore", "tool", "contribution", "group", "data", "automl", "evaluation", "view", "time", "performance", "set", "analysis"], "document_vector": [-19.805641, -0.553874], "paragraphs": [{"paragraph_vector": [-57.160606, 51.421634], "paragraph_keywords": ["pipelines", "ml", "use", "automl"]}, {"paragraph_vector": [-56.864646, 49.314781], "paragraph_keywords": ["systems", "pipelines", "automl", "uses"]}, {"paragraph_vector": [-59.248767, 46.52153], "paragraph_keywords": ["pipelines", "primitives", "pipeline", "automl"]}, {"paragraph_vector": [-58.385185, 43.055603], "paragraph_keywords": ["tool", "automl", "systems", "pipeline"]}, {"paragraph_vector": [-59.607196, 50.4001], "paragraph_keywords": ["pipelines", "automl", "model", "systems"]}, {"paragraph_vector": [-54.704463, 52.246067], "paragraph_keywords": ["pipelines", "pipeline", "hyperparameters", "users"]}, {"paragraph_vector": [-53.634124, 46.23196], "paragraph_keywords": ["models", "model", "pipelines", "systems"]}, {"paragraph_vector": [-57.837291, 46.663543], "paragraph_keywords": ["users", "pipelines", "automl", "models"]}, {"paragraph_vector": [-59.456985, 50.699642], "paragraph_keywords": ["pipelines", "time", "scores", "performance"]}, {"paragraph_vector": [-59.073818, 50.744476], "paragraph_keywords": ["pipelines", "systems", "automl", "primitives"]}, {"paragraph_vector": [-61.879669, 47.683963], "paragraph_keywords": ["pipelines", "primitives", "pipeline", "visualizing"]}, {"paragraph_vector": [-62.400547, 48.811328], "paragraph_keywords": ["pipeline", "pipelines", "type", "matrix"]}, {"paragraph_vector": [-63.004806, 48.421936], "paragraph_keywords": ["pipeline", "encoding", "hyperparameter", "scores"]}, {"paragraph_vector": [-10.114952, 29.921398], "paragraph_keywords": ["pipelines", "pipeline", "correlation", "ieee"]}, {"paragraph_vector": [-57.503208, 45.347187], "paragraph_keywords": ["graph", "pipelines", "pipeline", "graphs"]}, {"paragraph_vector": [-60.790306, 49.880348], "paragraph_keywords": ["interactions", "graph", "node", "entry"]}, {"paragraph_vector": [-55.094169, 50.359527], "paragraph_keywords": ["k", "primitives", "algorithm", "groups"]}, {"paragraph_vector": [-60.366569, 48.232578], "paragraph_keywords": ["pipelines", "pipelineprofiler", "jupyter", "run"]}, {"paragraph_vector": [-59.208427, 47.901546], "paragraph_keywords": ["pipelines", "primitives", "automl", "developer"]}, {"paragraph_vector": [-58.591182, 44.820182], "paragraph_keywords": ["pipelines", "primitives", "search", "use"]}, {"paragraph_vector": [-62.014682, 48.913902], "paragraph_keywords": ["systems", "pipeline", "pipelines", "automl"]}, {"paragraph_vector": [-63.607177, 49.733676], "paragraph_keywords": ["system", "pipelines", "use", "estimator"]}, {"paragraph_vector": [-61.398139, 45.141109], "paragraph_keywords": ["pipelines", "system", "search", "automl"]}, {"paragraph_vector": [-71.107032, 60.531162], "paragraph_keywords": ["participants", "pipelines", "system", "problem"]}, {"paragraph_vector": [-62.151687, 47.306594], "paragraph_keywords": ["pipelines", "systems", "use", "automl"]}, {"paragraph_vector": [-60.852161, 47.743671], "paragraph_keywords": ["features", "pipelines", "hyperparameters", "primitives"]}, {"paragraph_vector": [-62.650451, 48.24591], "paragraph_keywords": ["tool", "sus", "pipelineprofiler", "automl"]}, {"paragraph_vector": [-57.154228, 48.470829], "paragraph_keywords": ["automl", "users", "knowledge", "analyses"]}, {"paragraph_vector": [-60.164806, 46.241943], "paragraph_keywords": ["use", "ieee", "darpa", "nsf"]}], "content": {}, "doi": "10.1109/TVCG.2020.3028891"}, {"uri": "5", "title": "Table Scraps: An Actionable Framework for Multi-Table Data Wrangling From An Artifact Study of Computational Journalism", "timestamp": "2020", "rating": "0.0", "annotation": "", "tags": [], "authors": ["Stephen Kasica", "Charles Berret"], "summary": "For the many journalists who use data and computation to report the news, data wrangling is an integral part of their work. Despite an abundance of literature on data wrangling in the context of enterprise data analysis, little is known about the specific operations, processes, and pain points journalists encounter while performing this tedious, time-consuming task. To better understand the needs of this user group, we conduct a technical observation study of 50 public repositories of data and analysis code authored by 33 professional journalists at 26 news organizations. We develop two detailed and cross-cutting taxonomies of data wrangling in computational journalism, for actions and for processes. We observe the extensive use of multiple tables, a notable gap in previous wrangling analyses. We develop a concise, actionable framework for general multi-table data wrangling that includes wrangling operations documented in our taxonomy that are without clear parallels in other work. This framework, the first to incorporate tables as first-class objects, will support future interactive wrangling tools for both computational journalism and general-purpose use. We assess the generative and descriptive power of our framework through discussion of its relationship to our set of taxonomies.", "keywords": ["use", "operation", "literature", "repos", "wrangler", "order", "user", "combine", "row", "transformation", "observation", "provides", "create", "observed", "support", "wrangling", "state", "framework", "supported", "environment", "level", "-", "value", "code", "process", "approach", "work", "task", "action", "column", "dataset", "table", "need", "visualization", "taxonomy", "existing", "based", "tool", "issue", "article", "published", "data", "water", "journalism", "variable", "corpus", "address", "time", "programming", "journalist", "match", "application", "design", "notebook", "class", "set", "analysis", "study"], "document_vector": [-94.838157, 46.208652], "paragraphs": [{"paragraph_vector": [-112.187004, -2.044229], "paragraph_keywords": ["data", "wrangling", "process", "journalists"]}, {"paragraph_vector": [-111.421569, -1.166856], "paragraph_keywords": ["wrangling", "data", "journalists", "tables"]}, {"paragraph_vector": [-109.984573, -1.797803], "paragraph_keywords": ["data", "wrangling", "journalists", "process"]}, {"paragraph_vector": [-110.505744, -2.576144], "paragraph_keywords": ["data", "wrangling", "tools", "design"]}, {"paragraph_vector": [-104.027214, -6.981442], "paragraph_keywords": ["data", "wrangling", "operations", "support"]}, {"paragraph_vector": [-110.567817, -0.262668], "paragraph_keywords": ["data", "wrangling", "practices", "programming"]}, {"paragraph_vector": [-113.158264, 1.806053], "paragraph_keywords": ["data", "journalists", "wrangling", "process"]}, {"paragraph_vector": [-112.213027, -1.457754], "paragraph_keywords": ["data", "observation", "user", "approach"]}, {"paragraph_vector": [-111.717926, 1.203091], "paragraph_keywords": ["repos", "wrangling", "github", "data"]}, {"paragraph_vector": [-110.108909, -1.388817], "paragraph_keywords": ["repos", "codes", "wrangling", "ieee"]}, {"paragraph_vector": [-111.391777, -1.713887], "paragraph_keywords": ["data", "taxonomies", "based", "wrangling"]}, {"paragraph_vector": [-103.678771, -7.319387], "paragraph_keywords": ["data", "wrangling", "literature", "papers"]}, {"paragraph_vector": [-107.024818, -2.744078], "paragraph_keywords": ["wrangling", "data", "framework", "taxonomies"]}, {"paragraph_vector": [-107.494735, -1.538146], "paragraph_keywords": ["data", "wrangling", "record", "actions"]}, {"paragraph_vector": [-110.465751, -0.453852], "paragraph_keywords": ["wrangling", "codes", "table", "level"]}, {"paragraph_vector": [-108.657119, -2.418012], "paragraph_keywords": ["operations", "table", "input", "data"]}, {"paragraph_vector": [-106.146789, -6.994953], "paragraph_keywords": ["tables", "operations", "data", "wrangling"]}, {"paragraph_vector": [-104.307785, -6.002873], "paragraph_keywords": ["table", "operations", "dataset", "columns"]}, {"paragraph_vector": [-103.883712, -7.112924], "paragraph_keywords": ["data", "table", "column", "operations"]}, {"paragraph_vector": [-105.797599, -5.886159], "paragraph_keywords": ["data", "tables", "operations", "column"]}, {"paragraph_vector": [-105.144531, -7.435814], "paragraph_keywords": ["tables", "table", "data", "column"]}, {"paragraph_vector": [-104.650398, -9.657328], "paragraph_keywords": ["operation", "operations", "combine", "data"]}, {"paragraph_vector": [-109.329421, -3.87787], "paragraph_keywords": ["water", "data", "wrangling", "state"]}, {"paragraph_vector": [-106.25254, -5.841517], "paragraph_keywords": ["table", "data", "columns", "values"]}, {"paragraph_vector": [-62.84753, 0.670165], "paragraph_keywords": ["table", "arrivals", "tables", "arrival"]}, {"paragraph_vector": [-107.381813, -9.978785], "paragraph_keywords": ["data", "state", "wrangling", "article"]}, {"paragraph_vector": [-103.372001, -10.098012], "paragraph_keywords": ["wrangling", "data", "processes", "formalism"]}, {"paragraph_vector": [-106.48645, -2.564478], "paragraph_keywords": ["data", "wrangling", "journalists", "tools"]}, {"paragraph_vector": [-110.980537, -2.648257], "paragraph_keywords": ["data", "wrangling", "journalists", "time"]}, {"paragraph_vector": [-107.381858, -3.173247], "paragraph_keywords": ["wrangling", "data", "groups", "match"]}, {"paragraph_vector": [-96.407852, 69.31224], "paragraph_keywords": ["republication", "redistribution", "requires", "ieee"]}], "content": {}, "doi": "10.1109/TVCG.2020.3028975"}, {"uri": "6", "title": "Towards Better Bus Networks: A Visual Analytics Approach", "timestamp": "2020", "rating": "0.0", "annotation": "", "tags": [], "authors": ["Di Weng", "Chengbo Zheng", "Zikun Deng", "Mingze Ma", "Jie Bao", "Yu Zheng", "Mingliang Xu", "Yingcai Wu"], "summary": "Bus routes are typically updated every 3\u20135 years to meet constantly changing travel demands. However, identifying deficient bus routes and finding their optimal replacements remain challenging due to the difficulties in analyzing a complex bus network and the large solution space comprising alternative routes. Most of the automated approaches cannot produce satisfactory results in real-world settings without laborious inspection and evaluation of the candidates. The limitations observed in these approaches motivate us to collaborate with domain experts and propose a visual analytics solution for the performance analysis and incremental planning of bus routes based on an existing bus network. Developing such a solution involves three major challenges, namely, a) the in-depth analysis of complex bus route networks, b) the interactive generation of improved route candidates, and c) the effective evaluation of alternative bus routes. For challenge a, we employ an overview-to-detail approach by dividing the analysis of a complex bus network into three levels to facilitate the efficient identification of deficient routes. For challenge b, we improve a route generation model and interpret the performance of the generation with tailored visualizations. For challenge c, we incorporate a conflict resolution strategy in the progressive decision-making process to assist users in evaluating the alternative routes and finding the most optimal one. The proposed system is evaluated with two usage scenarios based on real-world data and received positive feedback from the experts.", "keywords": ["flow", "route", "generation", "use", "user", "graph", "zone", "transit", "cluster", "search", "generated", "model", "system", "stop", "number", "station", "decision", "network", "strategy", "transportation", "analytics", "fig", "expert", "process", "service", "planning", "matrix", "passenger", "map", "visualization", "based", "criterion", "data", "stage", "making", "view", "time", "performance", "given", "line", "proposed", "design", "jim", "bus", "overview", "set", "analysis", "cost", "interface", "conflict"], "document_vector": [-41.03424, 82.595687], "paragraphs": [{"paragraph_vector": [-73.304878, 24.961814], "paragraph_keywords": ["bus", "university", "china", "routes"]}, {"paragraph_vector": [-73.672851, 24.440481], "paragraph_keywords": ["routes", "route", "bus", "experts"]}, {"paragraph_vector": [-72.58422, 25.620601], "paragraph_keywords": ["route", "bus", "experts", "performance"]}, {"paragraph_vector": [-75.869033, 22.520751], "paragraph_keywords": ["transit", "based", "design", "network"]}, {"paragraph_vector": [-74.222251, 27.381507], "paragraph_keywords": ["data", "based", "analytics", "techniques"]}, {"paragraph_vector": [-75.948745, 24.472814], "paragraph_keywords": ["bus", "network", "data", "route"]}, {"paragraph_vector": [-74.983627, 21.164445], "paragraph_keywords": ["routes", "route", "bus", "experts"]}, {"paragraph_vector": [-76.781761, 26.735645], "paragraph_keywords": ["bus", "routes", "visualization", "experts"]}, {"paragraph_vector": [-75.87342, 25.175737], "paragraph_keywords": ["routes", "route", "experts", "generated"]}, {"paragraph_vector": [-73.922256, 22.257448], "paragraph_keywords": ["route", "routes", "experts", "search"]}, {"paragraph_vector": [-71.997428, 25.635013], "paragraph_keywords": ["node", "station", "r", "nodes"]}, {"paragraph_vector": [-74.738327, 22.610944], "paragraph_keywords": ["route", "station", "stage", "search"]}, {"paragraph_vector": [-72.091636, 23.330778], "paragraph_keywords": ["route", "stop", "model", "time"]}, {"paragraph_vector": [-73.132804, 23.220075], "paragraph_keywords": ["cost", "route", "stop", "graph"]}, {"paragraph_vector": [-74.860084, 20.472253], "paragraph_keywords": ["route", "stations", "routes", "model"]}, {"paragraph_vector": [-73.28096, 24.145273], "paragraph_keywords": ["routes", "interface", "route", "users"]}, {"paragraph_vector": [-74.57077, 25.522451], "paragraph_keywords": ["routes", "route", "network", "zones"]}, {"paragraph_vector": [-72.755195, 26.778415], "paragraph_keywords": ["route", "users", "passenger", "criteria"]}, {"paragraph_vector": [-71.951011, 26.477231], "paragraph_keywords": ["routes", "matrix", "fig", "passengers"]}, {"paragraph_vector": [-73.379425, 23.195308], "paragraph_keywords": ["routes", "route", "users", "fig"]}, {"paragraph_vector": [-71.52613, 22.231199], "paragraph_keywords": ["routes", "users", "route", "conflicts"]}, {"paragraph_vector": [-70.22158, 22.210428], "paragraph_keywords": ["clusters", "route", "conflict", "cf"]}, {"paragraph_vector": [-71.999839, 23.303361], "paragraph_keywords": ["routes", "stop", "cluster", "system"]}, {"paragraph_vector": [-73.264129, 23.414749], "paragraph_keywords": ["routes", "passenger", "route", "stops"]}, {"paragraph_vector": [-71.819442, 23.594881], "paragraph_keywords": ["route", "jim", "passengers", "transfer"]}, {"paragraph_vector": [-70.958511, 21.262191], "paragraph_keywords": ["route", "clusters", "cluster", "fig"]}, {"paragraph_vector": [-72.873847, 22.756353], "paragraph_keywords": ["system", "fig", "conflict", "bus"]}, {"paragraph_vector": [-72.835388, 25.502384], "paragraph_keywords": ["system", "bus", "route", "routes"]}, {"paragraph_vector": [-74.087402, 23.907787], "paragraph_keywords": ["route", "users", "routes", "networks"]}, {"paragraph_vector": [-75.014526, 26.498281], "paragraph_keywords": ["bus", "route", "routes", "networks"]}], "content": {}, "doi": "10.1109/TVCG.2020.3028984"}, {"uri": "7", "title": "An Examination of Grouping and Spatial Organization Tasks for High-Dimensional Data Exploration", "timestamp": "2020", "rating": "0.0", "annotation": "", "tags": [], "authors": ["John Wenskovitch", "Chris North"], "summary": "How do analysts think about grouping and spatial operations? This overarching research question incorporates a number of points for investigation, including understanding how analysts begin to explore a dataset, the types of grouping/spatial structures created and the operations performed on them, the relationship between grouping and spatial structures, the decisions analysts make when exploring individual observations, and the role of external information. This work contributes the design and results of such a study, in which a group of participants are asked to organize the data contained within an unfamiliar quantitative dataset. We identify several overarching approaches taken by participants to design their organizational space, discuss the interactions performed by the participants, and propose design recommendations to improve the usability of future high-dimensional data exploration tools that make use of grouping (clustering) and spatial (dimension reduction) operations.", "keywords": ["use", "operation", "order", "user", "grouping", "cluster", "structure", "system", "performed", "observation", "research", "organization", "knowledge", "support", "participant", "technique", "strategy", "information", "dimension", "labeled", "clustering", "interaction", "space", "animal", "process", "work", "task", "card", "dataset", "spatialization", "created", "understanding", "sensemaking", "tool", "group", "data", "time", "seen", "design", "analysis", "example", "analyst", "study"], "document_vector": [-80.621101, 8.813048], "paragraphs": [{"paragraph_vector": [70.896636, -28.778215], "paragraph_keywords": ["sensemaking", "observations", "order", "information"]}, {"paragraph_vector": [61.935321, -34.176803], "paragraph_keywords": ["study", "sensemaking", "observations", "techniques"]}, {"paragraph_vector": [69.918304, -30.174989], "paragraph_keywords": ["study", "participants", "sensemaking", "analysis"]}, {"paragraph_vector": [72.72966, -28.143634], "paragraph_keywords": ["cognition", "sensemaking", "user", "analysts"]}, {"paragraph_vector": [-146.258453, 21.046159], "paragraph_keywords": ["data", "interactions", "distance", "text"]}, {"paragraph_vector": [70.622131, -30.53716], "paragraph_keywords": ["clustering", "grouping", "clusters", "analysts"]}, {"paragraph_vector": [60.823974, -23.908668], "paragraph_keywords": ["analysts", "data", "clustering", "study"]}, {"paragraph_vector": [76.005592, -31.437742], "paragraph_keywords": ["participants", "data", "dataset", "cards"]}, {"paragraph_vector": [71.964729, -29.472694], "paragraph_keywords": ["study", "participants", "described", "group"]}, {"paragraph_vector": [73.09822, -33.159275], "paragraph_keywords": ["analysis", "process", "created", "section"]}, {"paragraph_vector": [74.993316, -29.72841], "paragraph_keywords": ["participants", "stack", "method", "ieee"]}, {"paragraph_vector": [75.090934, -30.773239], "paragraph_keywords": ["participants", "grouping", "strategy", "strategies"]}, {"paragraph_vector": [67.184822, -27.36782], "paragraph_keywords": ["animals", "groups", "dimension", "group"]}, {"paragraph_vector": [71.735473, -30.928325], "paragraph_keywords": ["participants", "observations", "space", "groups"]}, {"paragraph_vector": [72.50138, -31.817146], "paragraph_keywords": ["groups", "participants", "axes", "space"]}, {"paragraph_vector": [72.962333, -29.469539], "paragraph_keywords": ["groups", "group", "structure", "participant"]}, {"paragraph_vector": [67.445777, -31.071916], "paragraph_keywords": ["groups", "participants", "performed", "operations"]}, {"paragraph_vector": [73.184761, -29.575838], "paragraph_keywords": ["observations", "group", "groups", "participants"]}, {"paragraph_vector": [72.356025, -29.614379], "paragraph_keywords": ["data", "knowledge", "groups", "features"]}, {"paragraph_vector": [66.457046, -27.638746], "paragraph_keywords": ["animals", "groups", "knowledge", "participant"]}, {"paragraph_vector": [73.836029, -29.529064], "paragraph_keywords": ["participants", "groups", "space", "data"]}, {"paragraph_vector": [71.924835, -30.551776], "paragraph_keywords": ["participants", "information", "groups", "interactions"]}, {"paragraph_vector": [71.804496, -29.880006], "paragraph_keywords": ["participants", "organization", "groups", "actions"]}, {"paragraph_vector": [66.968017, -32.493637], "paragraph_keywords": ["study", "participants", "support", "systems"]}, {"paragraph_vector": [-1.741467, -30.574699], "paragraph_keywords": ["study", "dimensions", "space", "participants"]}, {"paragraph_vector": [73.097129, -30.685144], "paragraph_keywords": ["groups", "participants", "structures", "system"]}], "content": {}, "doi": "10.1109/TVCG.2020.3030403"}, {"uri": "8", "title": "A Suggestive Interface for Untangling Mathematical Knots", "timestamp": "2020", "rating": "0.0", "annotation": "", "tags": [], "authors": ["Huan Liu", "Hui Zhang"], "summary": "In this paper we present a user-friendly sketching-based suggestive interface for untangling mathematical knots with complicated structures. Rather than treating mathematical knots as if they were 3D ropes, our interface is designed to assist the user to interact with knots with the right sequence of mathematically legal moves. Our knot interface allows one to sketch and untangle knots by proposing the Reidemeister moves, and can guide the user to untangle mathematical knots to the fewest possible number of crossings by suggesting the moves needed. The system highlights parts of the knot where the Reidemeister moves are applicable, suggests the possible moves, and constrains the user\u2019s drawing to legal moves only. This ongoing suggestion is based on a Reidemeister move analyzer, that reads the evolving knot in its Gauss code and predicts the needed Reidemeister moves towards the fewest possible number of crossings. For our principal test case of mathematical knot diagrams, this for the first time permits us to visualize, analyze, and deform them in a mathematical visual interface. In addition, understanding of a fairly long mathematical deformation sequence in our interface can be aided by visual analysis and comparison over the identified \u201ckey moments\u201d where only critical changes occur in the sequence. Our knot interface allows users to track and trace mathematical knot deformation with a significantly reduced number of visual frames containing only the Reidemeister moves being applied. All these combine to allow a much cleaner exploratory interface for us to analyze and study mathematical knots and their dynamics in topological space.", "keywords": ["use", "notation", "image", "untangle", "user", "system", "suggestion", "performed", "curve", "diagram", "knot", "code", "crossing", "work", "task", "perform", "suggest", "based", "following", "tangle", "t", "sketching", "point", "move", "graphic", "interface", "software"], "document_vector": [77.047775, -78.646286], "paragraphs": [{"paragraph_vector": [9.271318, -5.677224], "paragraph_keywords": ["knots", "topology", "ieee", "strings"]}, {"paragraph_vector": [7.689674, -3.587068], "paragraph_keywords": ["knot", "knots", "proofs", "interface"]}, {"paragraph_vector": [8.843217, -2.471799], "paragraph_keywords": ["interface", "knot", "sketching", "moves"]}, {"paragraph_vector": [8.609733, -3.574876], "paragraph_keywords": ["knot", "interface", "moves", "user"]}, {"paragraph_vector": [8.213978, -4.088022], "paragraph_keywords": ["knot", "diagram", "images", "diagrams"]}, {"paragraph_vector": [8.760672, -2.317205], "paragraph_keywords": ["knot", "pixels", "curve", "foreground"]}, {"paragraph_vector": [11.032831, -5.3716], "paragraph_keywords": ["knot", "knots", "label", "crossings"]}, {"paragraph_vector": [9.725784, -3.40645], "paragraph_keywords": ["tangle", "knot", "moves", "interface"]}, {"paragraph_vector": [9.504277, -2.149284], "paragraph_keywords": ["tangle", "tangles", "code", "performed"]}, {"paragraph_vector": [10.157105, -2.402507], "paragraph_keywords": ["tangle", "interface", "t", "code"]}, {"paragraph_vector": [10.099259, -3.691365], "paragraph_keywords": ["user", "moves", "knot", "system"]}, {"paragraph_vector": [8.399904, -2.982192], "paragraph_keywords": ["code", "use", "gap", "integer"]}, {"paragraph_vector": [11.013525, -3.044586], "paragraph_keywords": ["knot", "nodes", "system", "force"]}, {"paragraph_vector": [11.031885, -4.93892], "paragraph_keywords": ["knots", "interface", "ieee", "core"]}, {"paragraph_vector": [9.697233, -4.299669], "paragraph_keywords": ["tasks", "interface", "participants", "task"]}, {"paragraph_vector": [9.654345, -2.703042], "paragraph_keywords": ["ieee", "suggestion", "use", "gauss"]}, {"paragraph_vector": [8.018032, -4.340753], "paragraph_keywords": ["interface", "knots", "knot", "suggestion"]}, {"paragraph_vector": [-107.446357, 68.632225], "paragraph_keywords": ["ieee", "republication", "redistribution", "requires"]}], "content": {}, "doi": "10.1109/TVCG.2020.3030456"}, {"uri": "9", "title": "Visilant: Visual Support for the Exploration and Analytical Process Tracking in Criminal Investigations", "timestamp": "2020", "rating": "0.0", "annotation": "", "tags": [], "authors": ["Krist\u0131\u0301na Z\u00e1kop\u010danov\u00e1", "Marko \u0158eh\u00e1\u010dek", "Jozef B\u00e1trna", "Daniel Plakinger", "Sergej Stoppel", "Barbora Kozl\u0131\u0301kov\u00e1"], "summary": "The daily routine of criminal investigators consists of a thorough analysis of highly complex and heterogeneous data of crime cases. Such data can consist of case descriptions, testimonies, criminal networks, spatial and temporal information, and virtually any other data that is relevant for the case. Criminal investigators work under heavy time pressure to analyze the data for relationships, propose and verify several hypotheses, and derive conclusions, while the data can be incomplete or inconsistent and is changed and updated throughout the investigation, as new findings are added to the case. Based on a four-year intense collaboration with criminalists, we present a conceptual design for a visual tool supporting the investigation workflow and Visilant, a web-based tool for the exploration and analysis of criminal data guided by the proposed design. Visilant aims to support namely the exploratory part of the investigation pipeline, from case overview, through exploration and hypothesis generation, to the case presentation. Visilant tracks the reasoning process and as the data is changing, it informs investigators which hypotheses are affected by the data change and should be revised. The tool was evaluated by senior criminology experts within two sessions and their feedback is summarized in the paper. Additional supplementary material contains the technical details and exemplary case study.", "keywords": ["exploration", "use", "concept", "branch", "user", "mode", "node", "end", "presented", "support", "network", "state", "framework", "report", "diagram", "al", "evidence", "level", "object", "information", "progress", "interaction", "process", "work", "requirement", "need", "visualization", "load", "based", "tool", "figure", "case", "document", "investigator", "data", "tracking", "finding", "view", "criminalists", "time", "et", "representation", "hypothesis", "scenario", "investigation", "proposed", "design", "database", "analysis", "knowledge", "option"], "document_vector": [-67.905006, 48.961139], "paragraphs": [{"paragraph_vector": [-109.224433, -40.106365], "paragraph_keywords": ["data", "necessity", "criminology", "communication"]}, {"paragraph_vector": [-108.655158, -41.694168], "paragraph_keywords": ["investigation", "data", "model", "investigators"]}, {"paragraph_vector": [-107.921157, -42.04243], "paragraph_keywords": ["investigation", "criminalists", "analysis", "information"]}, {"paragraph_vector": [-109.349815, -40.347518], "paragraph_keywords": ["investigation", "data", "process", "criminalists"]}, {"paragraph_vector": [-109.397979, -42.319396], "paragraph_keywords": ["data", "criminalists", "framework", "exploration"]}, {"paragraph_vector": [-108.428695, -41.353221], "paragraph_keywords": ["analysis", "tools", "networks", "investigations"]}, {"paragraph_vector": [-107.49076, -45.493396], "paragraph_keywords": ["network", "process", "data", "storytelling"]}, {"paragraph_vector": [-108.412506, -19.144292], "paragraph_keywords": ["visualization", "provenance", "exploration", "information"]}, {"paragraph_vector": [-103.741561, -44.371105], "paragraph_keywords": ["network", "load", "visualization", "al"]}, {"paragraph_vector": [-108.68077, -41.963771], "paragraph_keywords": ["process", "investigators", "use", "interviews"]}, {"paragraph_vector": [-105.309616, -44.228946], "paragraph_keywords": ["proposed", "requirements", "results", "investigators"]}, {"paragraph_vector": [-105.44284, -42.771896], "paragraph_keywords": ["investigation", "evidence", "data", "need"]}, {"paragraph_vector": [-105.341247, -43.12664], "paragraph_keywords": ["investigation", "network", "criminalists", "objects"]}, {"paragraph_vector": [-112.547264, -30.919071], "paragraph_keywords": ["visualization", "investigation", "document", "data"]}, {"paragraph_vector": [-110.636161, -32.777221], "paragraph_keywords": ["users", "visualization", "process", "interactions"]}, {"paragraph_vector": [-107.079421, -39.724018], "paragraph_keywords": ["analysis", "progress", "criminalists", "states"]}, {"paragraph_vector": [-105.708366, -42.271255], "paragraph_keywords": ["data", "information", "criminalists", "database"]}, {"paragraph_vector": [-105.611175, -41.1842], "paragraph_keywords": ["data", "knowledge", "level", "evidence"]}, {"paragraph_vector": [-106.985748, -42.658916], "paragraph_keywords": ["framework", "modules", "data", "need"]}, {"paragraph_vector": [-105.951805, -43.793342], "paragraph_keywords": ["data", "progress", "based", "tool"]}, {"paragraph_vector": [-105.709159, -44.146457], "paragraph_keywords": ["figure", "view", "network", "dashboard"]}, {"paragraph_vector": [-106.317031, -44.655761], "paragraph_keywords": ["network", "nodes", "users", "node"]}, {"paragraph_vector": [-102.667358, -43.569171], "paragraph_keywords": ["network", "mode", "use", "comparison"]}, {"paragraph_vector": [-96.452903, -45.762351], "paragraph_keywords": ["state", "analysis", "states", "network"]}, {"paragraph_vector": [-102.534744, -42.487701], "paragraph_keywords": ["diagram", "analysis", "branch", "state"]}, {"paragraph_vector": [-106.202133, -43.426856], "paragraph_keywords": ["experts", "tool", "analysis", "investigation"]}, {"paragraph_vector": [-101.715522, -45.59568], "paragraph_keywords": ["object", "objects", "data", "view"]}, {"paragraph_vector": [-102.462005, -45.050334], "paragraph_keywords": ["investigation", "networks", "network", "session"]}, {"paragraph_vector": [-107.520034, -41.760173], "paragraph_keywords": ["investigations", "concept", "design", "context"]}, {"paragraph_vector": [-97.375129, 74.660499], "paragraph_keywords": ["ieee", "use", "interior", "ministry"]}], "content": {}, "doi": "10.1109/TVCG.2020.3030351"}, {"uri": "10", "title": "CNN EXPLAINER: Learning Convolutional Neural Networks with Interactive Visualization", "timestamp": "2020", "rating": "0.0", "annotation": "", "tags": [], "authors": ["Zijie J. Wang", "Robert Turko", "Omar Shaikh", "Haekyu Park", "Nilaksh Das", "Fred Hohman", "Minsuk Kahng"], "summary": "Deep learning\u2019s great success motivates many practitioners and students to learn about this exciting technology. However, it is often challenging for beginners to take their first step due to the complexity of understanding and applying deep learning. We present CNN EXPLAINER, an interactive visualization tool designed for non-experts to learn and examine convolutional neural networks (CNNs), a foundational deep learning model architecture. Our tool addresses key challenges that novices face while learning about CNNs, which we identify from interviews with instructors and a survey with past students. CNN EXPLAINER tightly integrates a model overview that summarizes a CNN\u2019s structure, and on-demand, dynamic visual explanation views that help users understand the underlying components of CNNs. Through smooth transitions across levels of abstraction, our tool enables users to inspect the interplay between low-level mathematical operations and high-level model structures. A qualitative user study shows that CNN EXPLAINER helps users more easily understand the inner workings of CNNs, and is engaging and enjoyable to use. We also derive design lessons from our study. Developed using modern web technologies, CNN EXPLAINER runs locally in users\u2019 web browsers without the need for installation or specialized hardware, broadening the public\u2019s education access to modern deep learning techniques.", "keywords": ["use", "operation", "scale", "computer", "image", "usa", "feature", "found", "concept", "user", "machine", "classification", "explanation", "animation", "liu", "cnn", "model", "learning", "structure", "web", "explaining", "research", "knowledge", "hyperparameters", "participant", "network", "understand", "beginner", "level", "formula", "fig", "process", "education", "work", "output", "explainer", "help", "input", "visualization", "activation", "learner", "based", "layer", "tool", "understanding", "article", "convolution", "data", "engagement", "view", "ieee", "explain", "result", "cnns", "learn", "design", "overview", "wang", "class", "neuron", "example", "student", "study", "av"], "document_vector": [-11.825283, -8.267363], "paragraphs": [{"paragraph_vector": [-10.769195, 72.716957], "paragraph_keywords": ["learning", "model", "students", "ieee"]}, {"paragraph_vector": [-131.285858, 87.612625], "paragraph_keywords": ["cnn", "visualization", "learning", "model"]}, {"paragraph_vector": [98.376197, 88.863647], "paragraph_keywords": ["learning", "visualization", "cnn", "explainer"]}, {"paragraph_vector": [-13.289148, 69.635459], "paragraph_keywords": ["image", "layers", "input", "cnns"]}, {"paragraph_vector": [-7.724673, 75.621383], "paragraph_keywords": ["learning", "visualization", "model", "classification"]}, {"paragraph_vector": [85.128494, 89.576255], "paragraph_keywords": ["av", "learning", "tools", "data"]}, {"paragraph_vector": [-57.510829, 82.683441], "paragraph_keywords": ["learning", "model", "experts", "instructors"]}, {"paragraph_vector": [-16.022823, 85.860351], "paragraph_keywords": ["cnns", "layers", "structure", "students"]}, {"paragraph_vector": [1.243122, 81.137359], "paragraph_keywords": ["learning", "models", "understand", "model"]}, {"paragraph_vector": [-134.960311, 87.76435], "paragraph_keywords": ["users", "data", "tool", "backpropagation"]}, {"paragraph_vector": [-57.974773, 83.098266], "paragraph_keywords": ["cnn", "users", "model", "sect"]}, {"paragraph_vector": [-22.631591, 70.772811], "paragraph_keywords": ["layer", "color", "cnn", "scale"]}, {"paragraph_vector": [-7.138011, 73.018821], "paragraph_keywords": ["edges", "neuron", "layer", "output"]}, {"paragraph_vector": [-8.691896, 71.713111], "paragraph_keywords": ["view", "input", "user", "kernel"]}, {"paragraph_vector": [-24.547508, 77.047805], "paragraph_keywords": ["view", "user", "logit", "equation"]}, {"paragraph_vector": [-22.078371, 78.589813], "paragraph_keywords": ["users", "cnn", "image", "user"]}, {"paragraph_vector": [-2.913239, 74.913162], "paragraph_keywords": ["visualization", "image", "hyperparameters", "user"]}, {"paragraph_vector": [-26.982547, 76.612556], "paragraph_keywords": ["output", "layer", "janis", "model"]}, {"paragraph_vector": [-6.058341, 72.008789], "paragraph_keywords": ["layer", "class", "damian", "cnn"]}, {"paragraph_vector": [-31.573751, 81.632217], "paragraph_keywords": ["learning", "cnn", "participants", "knowledge"]}, {"paragraph_vector": [151.325988, 7.048235], "paragraph_keywords": ["participants", "tool", "cnn", "features"]}, {"paragraph_vector": [-68.936241, 87.312629], "paragraph_keywords": ["layer", "cnn", "view", "level"]}, {"paragraph_vector": [-94.808082, 88.442863], "paragraph_keywords": ["animation", "helped", "convolution", "participants"]}, {"paragraph_vector": [-51.868507, 86.355476], "paragraph_keywords": ["image", "input", "animations", "participants"]}, {"paragraph_vector": [-7.566804, 75.436477], "paragraph_keywords": ["cnn", "cnns", "article", "learning"]}, {"paragraph_vector": [-14.873424, 70.67044], "paragraph_keywords": ["cnn", "users", "help", "model"]}, {"paragraph_vector": [-76.549507, 86.638221], "paragraph_keywords": ["help", "users", "layer", "learning"]}, {"paragraph_vector": [-69.379318, 86.644515], "paragraph_keywords": ["visualization", "design", "ieee", "use"]}, {"paragraph_vector": [-23.986669, 82.613731], "paragraph_keywords": ["learning", "wang", "visualization", "computer"]}, {"paragraph_vector": [18.588472, 84.606605], "paragraph_keywords": ["learning", "visualization", "chau", "ieee"]}, {"paragraph_vector": [32.054313, 85.870391], "paragraph_keywords": ["liu", "computer", "ieee", "learning"]}, {"paragraph_vector": [-16.896699, 80.630081], "paragraph_keywords": ["ieee", "visualization", "learning", "networks"]}, {"paragraph_vector": [-38.650928, 82.840957], "paragraph_keywords": ["conference", "learning", "chi", "ieee"]}], "content": {}, "doi": "10.1109/TVCG.2020.3030472"}, {"uri": "11", "title": "Calliope: Automatic Visual Data Story Generation from a Spreadsheet", "timestamp": "2020", "rating": "0.0", "annotation": "", "tags": [], "authors": ["Danqing Shi", "Xinyue Xu", "Fuling Sun", "Yang Shi", "Nan Cao"], "summary": "Visual data stories shown in the form of narrative visualizations such as a poster or a data video, are frequently used in data-oriented storytelling to facilitate the understanding and memorization of the story content. Although useful, technique barriers, such as data analysis, visualization, and scripting, make the generation of a visual data story difficult. Existing authoring tools rely on users\u2019 skills and experiences, which are usually inefficient and still difficult. In this paper, we introduce a novel visual data story generating system, Calliope, which creates visual data stories from an input spreadsheet through an automatic process and facilities the easy revision of the generated story based on an online story editor. Particularly, Calliope incorporates a new logic-oriented Monte Carlo tree search algorithm that explores the data space given by the input spreadsheet to progressively generate story pieces (i.e., data facts) and organize them in a logical order. The importance of data facts is measured based on information theory, and each data fact is visualized in a chart and captioned by an automatically generated description. We evaluate the proposed technique through three example stories, two controlled experiments, and a series of interviews with 10 domain experts. Our evaluation shows that Calliope is beneficial to efficient visual data story generation.", "keywords": ["use", "generation", "generating", "shi", "storytelling", "order", "user", "insight", "china", "subspace", "designed", "type", "generated", "text", "system", "calliope", "reward", "lee", "model", "number", "tree", "generate", "technique", "field", "value", "information", "story", "introduced", "fig", "process", "space", "including", "algorithm", "help", "input", "visualization", "indicates", "based", "denoted", "data", "fact", "fi", "time", "quality", "chart", "ieee", "result", "given", "narrative", "design", "wang", "set", "analysis", "logic", "li", "searching", "step"], "document_vector": [169.304, 9.465024], "paragraphs": [{"paragraph_vector": [-163.427917, 5.639562], "paragraph_keywords": ["data", "story", "visualization", "shi"]}, {"paragraph_vector": [-160.608901, 6.977317], "paragraph_keywords": ["data", "story", "based", "facts"]}, {"paragraph_vector": [-163.927047, 4.470574], "paragraph_keywords": ["data", "story", "logic", "information"]}, {"paragraph_vector": [-168.05487, 10.355935], "paragraph_keywords": ["data", "story", "visualization", "designed"]}, {"paragraph_vector": [-164.521636, 3.149928], "paragraph_keywords": ["data", "chart", "based", "mapping"]}, {"paragraph_vector": [-161.07994, 1.285702], "paragraph_keywords": ["data", "generate", "based", "use"]}, {"paragraph_vector": [-157.143768, 6.753881], "paragraph_keywords": ["data", "story", "fact", "pieces"]}, {"paragraph_vector": [-159.040496, 7.272518], "paragraph_keywords": ["story", "data", "system", "fact"]}, {"paragraph_vector": [-160.083175, 7.266255], "paragraph_keywords": ["story", "data", "facts", "generated"]}, {"paragraph_vector": [-60.486869, -11.462829], "paragraph_keywords": ["data", "fact", "subspace", "value"]}, {"paragraph_vector": [-66.659523, -15.120358], "paragraph_keywords": ["fact", "data", "fi", "distribution"]}, {"paragraph_vector": [-66.24987, -4.913444], "paragraph_keywords": ["data", "si", "fact", "probability"]}, {"paragraph_vector": [-156.784545, 7.460567], "paragraph_keywords": ["data", "algorithm", "fact", "formula"]}, {"paragraph_vector": [-159.625976, 2.611231], "paragraph_keywords": ["story", "data", "tree", "searching"]}, {"paragraph_vector": [-149.64682, 2.233623], "paragraph_keywords": ["fi", "f", "tree", "reward"]}, {"paragraph_vector": [-159.753189, 2.273341], "paragraph_keywords": ["fi", "facts", "generated", "data"]}, {"paragraph_vector": [-157.288299, 6.904111], "paragraph_keywords": ["fi", "story", "indicates", "fact"]}, {"paragraph_vector": [-159.976104, 6.240011], "paragraph_keywords": ["story", "fact", "data", "use"]}, {"paragraph_vector": [-159.546981, 6.484853], "paragraph_keywords": ["fact", "story", "data", "chart"]}, {"paragraph_vector": [-160.487167, 5.789328], "paragraph_keywords": ["story", "data", "generated", "fact"]}, {"paragraph_vector": [-147.758193, -13.430779], "paragraph_keywords": ["fact", "generated", "closed", "companies"]}, {"paragraph_vector": [-159.556335, -0.358059], "paragraph_keywords": ["calliope", "factsheets", "generated", "data"]}, {"paragraph_vector": [-158.855728, 5.648079], "paragraph_keywords": ["data", "generated", "orders", "participants"]}, {"paragraph_vector": [-171.107757, -2.188469], "paragraph_keywords": ["data", "use", "system", "ieee"]}, {"paragraph_vector": [-163.081054, 5.247659], "paragraph_keywords": ["story", "data", "results", "mode"]}, {"paragraph_vector": [-163.179397, 5.491961], "paragraph_keywords": ["data", "story", "system", "tool"]}, {"paragraph_vector": [-160.576416, 6.128542], "paragraph_keywords": ["data", "system", "chart", "story"]}, {"paragraph_vector": [-161.474594, 11.660433], "paragraph_keywords": ["system", "story", "ieee", "fact"]}, {"paragraph_vector": [-163.139038, 4.566197], "paragraph_keywords": ["conference", "data", "li", "lee"]}, {"paragraph_vector": [-164.512603, 7.645242], "paragraph_keywords": ["visualization", "heer", "data", "wang"]}, {"paragraph_vector": [-161.652297, 2.912887], "paragraph_keywords": ["data", "visualization", "ieee", "zhang"]}, {"paragraph_vector": [-162.851501, 14.793772], "paragraph_keywords": ["conference", "coherence", "zhang", "discourse"]}], "content": {}, "doi": "10.1109/TVCG.2020.3030375"}, {"uri": "12", "title": "Once Upon A Time In Visualization: Understanding the Use of Textual Narratives for Causality", "timestamp": "2020", "rating": "0.0", "annotation": "", "tags": [], "authors": ["Arjun Choudhry", "Mandar Sharma", "Pramod Chundury", "Thomas Kapler", "Derek W. S. Gray", "Naren Ramakrishnan"], "summary": "Causality visualization can help people understand temporal chains of events, such as messages sent in a distributed system, cause and effect in a historical conflict, or the interplay between political actors over time. However, as the scale and complexity of these event sequences grows, even these visualizations can become overwhelming to use. In this paper, we propose the use of textual narratives as a data-driven storytelling method to augment causality visualization. We first propose a design space for how textual narratives can be used to describe causal data. We then present results from a crowdsourced user study where participants were asked to recover causality information from two causality visualizations\u2014causal graphs and Hasse diagrams\u2014with and without an associated textual narrative. Finally, we describe CAUSEWORKS, a causality visualization system for understanding how specific interventions influence a causal model. The system incorporates an automatic textual narrative mechanism based on our design space. We validate CAUSEWORKS through interviews with experts who used the system for understanding complex events.", "keywords": ["usa", "machine", "acm", "text", "learning", "state", "network", "interpretation", "information", ".", "proceeding", "visualization", "data", "hullman", "ieee", "doi", "university", "analysis", "story", "riche", "conference"], "document_vector": [-157.958068, 58.404567], "paragraphs": [{"paragraph_vector": [-166.700378, 36.746402], "paragraph_keywords": ["doi", "art", "state", "graphics"]}, {"paragraph_vector": [-167.728393, 38.8255], "paragraph_keywords": ["doi", "visualization", "acm", "."]}, {"paragraph_vector": [-164.471862, 33.6762], "paragraph_keywords": ["visualization", "doi", "conference", "drucker"]}, {"paragraph_vector": [-164.027008, 39.574745], "paragraph_keywords": ["language", "applied", "systems", "generation"]}, {"paragraph_vector": [-166.365783, 41.946613], "paragraph_keywords": ["doi", "visualization", "usa", "ieee"]}, {"paragraph_vector": [179.616394, 42.735225], "paragraph_keywords": ["computing", "proceedings", "analysis", "ieee"]}], "content": {}, "doi": "10.1109/TVCG.2020.3030419"}, {"uri": "13", "title": "In Search of Patient Zero: Visual Analytics of Pathogen Transmission Pathways in Hospitals", "timestamp": "2020", "rating": "0.0", "annotation": "", "tags": [], "authors": ["T. Baumgartl", "M. Petzold", "M. Wunderlich", "M. H\u00f6hn", "D. Archambault", "M. Lieser", "A. Dalpke", "S. Scheithauer", "M. Marschollek", "V. M. Eichel", "N. T. Mutters"], "summary": "Pathogen outbreaks (i.e., outbreaks of bacteria and viruses) in hospitals can cause high mortality rates and increase costs for hospitals significantly. An outbreak is generally noticed when the number of infected patients rises above an endemic level or the usual prevalence of a pathogen in a defined population. Reconstructing transmission pathways back to the source of an outbreak \u2013 the patient zero or index patient \u2013 requires the analysis of microbiological data and patient contacts. This is often manually completed by infection control experts. We present a novel visual analytics approach to support the analysis of transmission pathways, patient contacts, the progression of the outbreak, and patient timelines during hospitalization. Infection control experts applied our solution to a real outbreak of Klebsiella pneumoniae in a large German hospital. Using our system, our experts were able to scale the analysis of transmission pathways to longer time intervals (i.e., several years of data instead of days) and across a larger number of wards. Also, the system is able to reduce the analysis time from days to hours. In our final study, feedback from twenty-five experts from seven German hospitals provides evidence that our solution brings significant benefits for analyzing outbreaks.", "keywords": ["use", "hospital", "outbreak", "order", "status", "infection", "contact", "transmission", "number", "support", "participant", "control", "level", "-", "patient", "information", "analytics", "fig", "expert", "approach", "location", "ward", "layout", "task", "help", "pathogen", "visualization", "disease", "pneumoniae", "position", "pathway", "data", "view", "method", "time", "line", "period", "tracing", "event", "transfer", "analysis", "infected"], "document_vector": [-120.418632, 54.635337], "paragraphs": [{"paragraph_vector": [-32.26892, -6.791167], "paragraph_keywords": ["patients", "germany", "pathogen", "university"]}, {"paragraph_vector": [-33.921344, -7.826092], "paragraph_keywords": ["patients", "ieee", "pathogen", "outbreak"]}, {"paragraph_vector": [-35.321804, -5.830582], "paragraph_keywords": ["time", "need", "disease", "infection"]}, {"paragraph_vector": [-33.808898, -6.068157], "paragraph_keywords": ["transmission", "disease", "tracing", "contacts"]}, {"paragraph_vector": [-36.568435, -4.868809], "paragraph_keywords": ["disease", "graphs", "transmission", "time"]}, {"paragraph_vector": [-37.665126, -5.844946], "paragraph_keywords": ["visualization", "time", "contacts", "visualizations"]}, {"paragraph_vector": [-37.414222, -2.32508], "paragraph_keywords": ["tasks", "ieee", "data", "patients"]}, {"paragraph_vector": [-32.628482, -6.498903], "paragraph_keywords": ["patients", "infection", "transmissions", "data"]}, {"paragraph_vector": [-41.5195, -5.460051], "paragraph_keywords": ["events", "patient", "data", "time"]}, {"paragraph_vector": [-38.093555, -1.139194], "paragraph_keywords": ["infection", "data", "patient", "infected"]}, {"paragraph_vector": [-40.938892, -2.183504], "paragraph_keywords": ["infection", "ieee", "patient", "views"]}, {"paragraph_vector": [-36.188343, -5.734775], "paragraph_keywords": ["time", "view", "transmission", "pathogen"]}, {"paragraph_vector": [-36.785526, -8.105822], "paragraph_keywords": ["wards", "layout", "y", "position"]}, {"paragraph_vector": [-36.705734, -7.209591], "paragraph_keywords": ["ieee", "transfers", "publication", "home"]}, {"paragraph_vector": [-38.573402, -6.240544], "paragraph_keywords": ["location", "layout", "use", "constraint"]}, {"paragraph_vector": [-39.141471, -3.689575], "paragraph_keywords": ["patients", "ward", "task", "time"]}, {"paragraph_vector": [-37.332099, -3.114719], "paragraph_keywords": ["ieee", "contacts", "patients", "publication"]}, {"paragraph_vector": [-32.414321, -6.105894], "paragraph_keywords": ["contact", "transmission", "infection", "events"]}, {"paragraph_vector": [-34.81068, -6.641749], "paragraph_keywords": ["patients", "experts", "outbreak", "wards"]}, {"paragraph_vector": [-33.511856, -7.969437], "paragraph_keywords": ["patients", "ieee", "publication", "expert"]}, {"paragraph_vector": [-34.45602, -6.937907], "paragraph_keywords": ["patients", "analysis", "transmission", "hospital"]}, {"paragraph_vector": [-33.546703, -6.23829], "paragraph_keywords": ["patients", "ieee", "patient", "view"]}, {"paragraph_vector": [-37.211082, -0.445779], "paragraph_keywords": ["participants", "analysis", "view", "data"]}, {"paragraph_vector": [-35.98019, -1.384869], "paragraph_keywords": ["participants", "ieee", "views", "transmission"]}, {"paragraph_vector": [-36.935829, -1.874034], "paragraph_keywords": ["visualization", "layout", "found", "colors"]}, {"paragraph_vector": [-35.899829, -5.44303], "paragraph_keywords": ["data", "number", "patients", "help"]}, {"paragraph_vector": [-34.657215, -5.132563], "paragraph_keywords": ["transmission", "analysis", "help", "information"]}, {"paragraph_vector": [-35.919589, -4.954445], "paragraph_keywords": ["ieee", "publication", "approach", "control"]}], "content": {}, "doi": "10.1109/TVCG.2020.3030338"}, {"uri": "14", "title": "Visual Analysis of Discrimination in Machine Learning", "timestamp": "2020", "rating": "0.0", "annotation": "", "tags": [], "authors": ["Qianwen Wang", "Zhenhua Xu", "Zhutian Chen", "Yong Wang", "Shixia Liu", "Huamin Qu"], "summary": "The growing use of automated decision-making in critical applications, such as crime prediction and college admission, has raised questions about fairness in machine learning. How can we decide whether different treatments are reasonable or discriminatory? In this paper, we investigate discrimination in machine learning from a visual analytics perspective and propose an interactive visualization tool, DiscriLens, to support a more comprehensive analysis. To reveal detailed information on algorithmic discrimination, DiscriLens identifies a collection of potentially discriminatory itemsets based on causal modeling and classification rules mining. By combining an extended Euler diagram with a matrix-based visualization, we develop a novel set visualization to facilitate the exploration and interpretation of discriminatory itemsets. A user study shows that users can interpret the visually encoded information in DiscriLens quickly and accurately. Use cases demonstrate that DiscriLens provides informative guidance in understanding and reducing algorithmic discrimination.", "keywords": ["use", "ml", "user", "row", "model", "learning", "attribute", "support", "participant", "understand", "rippleset", "fairness", "value", "accuracy", "identify", "fig", "expert", "risk", "discrilens", "item", "work", "matrix", "task", "dataset", "algorithm", "difference", "visualization", "based", "tool", "group", "data", "resolving", "method", "prediction", "domain", "training", "euler", "protected", "itemset", "itemsets", "design", "set", "analysis", "discrimination", "student", "study"], "document_vector": [-42.009128, 37.618015], "paragraphs": [{"paragraph_vector": [-40.324668, 26.555654], "paragraph_keywords": ["model", "protected", "data", "based"]}, {"paragraph_vector": [-39.789615, 25.053163], "paragraph_keywords": ["discrimination", "methods", "analysis", "ieee"]}, {"paragraph_vector": [-44.300048, 21.965019], "paragraph_keywords": ["discrimination", "itemsets", "based", "visualization"]}, {"paragraph_vector": [-44.551792, 22.290983], "paragraph_keywords": ["discrimination", "model", "analysis", "data"]}, {"paragraph_vector": [-44.965587, 23.286846], "paragraph_keywords": ["fairness", "itemsets", "discrimination", "analysis"]}, {"paragraph_vector": [36.667991, -70.952537], "paragraph_keywords": ["set", "euler", "data", "work"]}, {"paragraph_vector": [-44.035026, 21.040477], "paragraph_keywords": ["attributes", "protected", "admission", "discrimination"]}, {"paragraph_vector": [-45.469547, 20.887474], "paragraph_keywords": ["discrimination", "attributes", "ml", "difference"]}, {"paragraph_vector": [-44.584671, 23.779666], "paragraph_keywords": ["discrimination", "attribute", "degree", "ieee"]}, {"paragraph_vector": [-42.582435, 19.359388], "paragraph_keywords": ["discrimination", "model", "itemsets", "module"]}, {"paragraph_vector": [-46.397666, 14.723714], "paragraph_keywords": ["attributes", "resolving", "classification", "rules"]}, {"paragraph_vector": [-49.591289, 21.823375], "paragraph_keywords": ["itemsets", "discrimination", "attributes", "visualization"]}, {"paragraph_vector": [22.832273, -61.787544], "paragraph_keywords": ["color", "shapes", "shape", "subset"]}, {"paragraph_vector": [37.620174, -65.393302], "paragraph_keywords": ["items", "attribute", "design", "represents"]}, {"paragraph_vector": [-43.022174, 26.701698], "paragraph_keywords": ["resolving", "itemsets", "values", "itemset"]}, {"paragraph_vector": [-44.990669, 20.227697], "paragraph_keywords": ["itemsets", "users", "attributes", "rippleset"]}, {"paragraph_vector": [-45.523242, 24.465984], "paragraph_keywords": ["mode", "participants", "models", "users"]}, {"paragraph_vector": [-45.578891, 23.514566], "paragraph_keywords": ["discrilens", "ieee", "based", "set"]}, {"paragraph_vector": [123.568161, 28.014913], "paragraph_keywords": ["discrilens", "participants", "tasks", "understand"]}, {"paragraph_vector": [-46.393569, 23.476295], "paragraph_keywords": ["attributes", "absence", "days", "set"]}, {"paragraph_vector": [-44.06509, 21.247652], "paragraph_keywords": ["discrimination", "dataset", "itemsets", "absence"]}, {"paragraph_vector": [-42.549861, 21.570713], "paragraph_keywords": ["xgboost", "itemsets", "rippleset", "discrimination"]}, {"paragraph_vector": [-43.308628, 23.199161], "paragraph_keywords": ["discrimination", "students", "itemsets", "values"]}, {"paragraph_vector": [-42.194545, 25.245529], "paragraph_keywords": ["discrimination", "model", "discrilens", "analysis"]}, {"paragraph_vector": [-21.56641, -2.649712], "paragraph_keywords": ["rippleset", "analysis", "visualization", "algorithm"]}, {"paragraph_vector": [-47.636043, 21.600786], "paragraph_keywords": ["analysis", "discrimination", "attributes", "customization"]}, {"paragraph_vector": [-46.445518, 19.998687], "paragraph_keywords": ["discrimination", "discrilens", "protected", "analysis"]}], "content": {}, "doi": "10.1109/TVCG.2020.3030440"}, {"uri": "15", "title": "Mode Surfaces of Symmetric Tensor Fields: Topological Analysis and Seamless Extraction", "timestamp": "2020", "rating": "0.0", "annotation": "", "tags": [], "authors": ["Botong Qu", "Lawrence Roy", "Yue Zhang", "Member"], "summary": "Mode surfaces are the generalization of degenerate curves and neutral surfaces, which constitute 3D symmetric tensor field topology. Efficient analysis and visualization of mode surfaces can provide additional insight into not only degenerate curves and neutral surfaces, but also how these features transition into each other. Moreover, the geometry and topology of mode surfaces can help domain scientists better understand the tensor fields in their applications. Existing mode surface extraction methods can miss features in the surfaces. Moreover, the mode surfaces extracted from neighboring cells have gaps, which make their subsequent analysis difficult. In this paper, we provide novel analysis on the topological structures of mode surfaces, including a common parameterization of all mode surfaces of a tensor field using 2D asymmetric tensors. This allows us to not only better understand the structures in mode surfaces and their interactions with degenerate curves and neutral surfaces, but also develop an efficient algorithm to seamlessly extract mode surfaces, including neutral surfaces. The seamless mode surfaces enable efficient analysis of their geometric structures, such as the principal curvature directions. We apply our analysis and visualization to a number of solid mechanics data sets.", "keywords": ["use", "b", "mode", "manifold", "block", "research", "extract", "curve", "field", "function", "eigenvector", "boundary", "extraction", "tet", "surface", "generalized", "triangle", "eigenvectors", "compression", "tensor", "figure", "asymmetric", "form", "intersection", "mesh", "degenerate", "domain", "t", "point", "parameterization", "find", "set", "analysis"], "document_vector": [-109.87606, -76.668052], "paragraphs": [{"paragraph_vector": [53.820297, 17.916051], "paragraph_keywords": ["tensor", "surfaces", "tensors", "curves"]}, {"paragraph_vector": [51.886699, 14.281159], "paragraph_keywords": ["mode", "surfaces", "figure", "values"]}, {"paragraph_vector": [51.99438, 10.578093], "paragraph_keywords": ["mode", "surfaces", "surface", "subdivision"]}, {"paragraph_vector": [52.459106, 10.857435], "paragraph_keywords": ["mode", "surfaces", "degenerate", "tensor"]}, {"paragraph_vector": [51.727634, 9.883196], "paragraph_keywords": ["surfaces", "degree", "degenerate", "tensor"]}, {"paragraph_vector": [55.202644, 10.282763], "paragraph_keywords": ["tensor", "t", "surfaces", "fields"]}, {"paragraph_vector": [56.547176, 17.702144], "paragraph_keywords": ["tensors", "eigenvectors", "t", "asymmetric"]}, {"paragraph_vector": [51.388446, 16.405035], "paragraph_keywords": ["tensor", "mode", "t", "degenerate"]}, {"paragraph_vector": [53.521221, 12.346028], "paragraph_keywords": ["mode", "surface", "tensor", "surfaces"]}, {"paragraph_vector": [55.319072, 16.813453], "paragraph_keywords": ["field", "tensor", "ieee", "points"]}, {"paragraph_vector": [55.523303, 13.831705], "paragraph_keywords": ["points", "mode", "surface", "eigenvector"]}, {"paragraph_vector": [60.261413, 13.36387], "paragraph_keywords": ["domain", "mode", "surface", "surfaces"]}, {"paragraph_vector": [53.478618, 14.465581], "paragraph_keywords": ["tensor", "domain", "characterized", "mode"]}, {"paragraph_vector": [54.179027, 12.022864], "paragraph_keywords": ["triangle", "mode", "use", "curves"]}, {"paragraph_vector": [48.369094, 7.643158], "paragraph_keywords": ["solutions", "points", "u", "tensor"]}, {"paragraph_vector": [51.91101, 11.506959], "paragraph_keywords": ["curves", "mode", "tet", "point"]}, {"paragraph_vector": [50.676448, 11.278694], "paragraph_keywords": ["point", "domain", "points", "boundary"]}, {"paragraph_vector": [54.408901, 12.857172], "paragraph_keywords": ["eigenvector", "mode", "region", "manifold"]}, {"paragraph_vector": [52.883823, 12.324485], "paragraph_keywords": ["mode", "surfaces", "intersection", "computed"]}, {"paragraph_vector": [53.185211, 12.937619], "paragraph_keywords": ["compression", "block", "mode", "figure"]}, {"paragraph_vector": [50.638843, 13.399634], "paragraph_keywords": ["compression", "block", "surface", "mode"]}, {"paragraph_vector": [52.641796, 12.240003], "paragraph_keywords": ["mode", "loads", "figure", "surfaces"]}, {"paragraph_vector": [51.755657, 13.512441], "paragraph_keywords": ["surfaces", "research", "tensor", "thank"]}], "content": {}, "doi": "10.1109/TVCG.2020.3030433"}, {"uri": "16", "title": "A Visual Analytics Framework for Reviewing Multivariate Time-Series Data with Dimensionality Reduction", "timestamp": "2020", "rating": "0.0", "annotation": "", "tags": [], "authors": ["Takanori Fujiwara", "Naohisa Sakamoto", "Jorji Nonaka", "Keiji Yamamoto", "Kwan-Liu Ma"], "summary": "Data-driven problem solving in many real-world applications involves analysis of time-dependent multivariate data, for which dimensionality reduction (DR) methods are often used to uncover the intrinsic structure and features of the data. However, DR is usually applied to a subset of data that is either single-time-point multivariate or univariate time-series, resulting in the need to manually examine and correlate the DR results out of different data subsets. When the number of dimensions is large either in terms of the number of time points or attributes, this manual task becomes too tedious and infeasible. In this paper, we present MulTiDR, a new DR framework that enables processing of time-dependent multivariate data as a whole to provide a comprehensive overview of the data. With the framework, we employ DR in two steps. When treating the instances, time points, and attributes of the data as a 3D array, the first DR step reduces the three axes of the array to two, and the second DR step visualizes the data in a lower-dimensional space. In addition, by coupling with a contrastive learning method and interactive visualizations, our framework enhances analysts\u2019 ability to interpret DR results. We demonstrate the effectiveness of our framework with four case studies using real-world datasets.", "keywords": ["use", "y", "feature", "pattern", "order", "cluster", "mode", "select", "pm", "multidr", "network", "measure", "value", "information", "fig", "similarity", "matrix", "dataset", "based", "tensor", "shown", "dr", "r", "data", "variable", "view", "method", "time", "quality", "fcs", "result", "point", "umap", "selected", "pca", "timestamps", "analysis", "example", "analyst", "instance", "step"], "document_vector": [-107.099746, 22.320955], "paragraphs": [{"paragraph_vector": [-11.186752, -13.933795], "paragraph_keywords": ["data", "dr", "time", "methods"]}, {"paragraph_vector": [-16.766199, -15.52771], "paragraph_keywords": ["dr", "step", "time", "results"]}, {"paragraph_vector": [-17.833368, -15.133643], "paragraph_keywords": ["order", "tensors", "time", "tensor"]}, {"paragraph_vector": [-19.924842, -21.031291], "paragraph_keywords": ["time", "dr", "space", "points"]}, {"paragraph_vector": [-22.109685, -21.35832], "paragraph_keywords": ["time", "result", "mds", "dr"]}, {"paragraph_vector": [-16.574405, -17.492149], "paragraph_keywords": ["tensor", "order", "decomposition", "dr"]}, {"paragraph_vector": [-15.677285, -13.258487], "paragraph_keywords": ["dr", "matrix", "pca", "y"]}, {"paragraph_vector": [-11.726632, -15.111117], "paragraph_keywords": ["dr", "time", "based", "result"]}, {"paragraph_vector": [-10.824727, -7.041254], "paragraph_keywords": ["cluster", "dr", "y", "time"]}, {"paragraph_vector": [-11.289356, -15.912018], "paragraph_keywords": ["dr", "analysis", "data", "use"]}, {"paragraph_vector": [-10.592691, -11.412764], "paragraph_keywords": ["dr", "information", "features", "analyst"]}, {"paragraph_vector": [-8.531409, -13.816549], "paragraph_keywords": ["clusters", "points", "cluster", "dr"]}, {"paragraph_vector": [-17.255407, -38.494068], "paragraph_keywords": ["view", "contributions", "feature", "fig"]}, {"paragraph_vector": [-4.899192, 1.326642], "paragraph_keywords": ["cluster", "feature", "points", "fcs"]}, {"paragraph_vector": [8.011983, 26.387454], "paragraph_keywords": ["r", "sign", "cluster", "value"]}, {"paragraph_vector": [-0.252936, 15.809135], "paragraph_keywords": ["sign", "cluster", "dr", "signs"]}, {"paragraph_vector": [-4.601505, -15.779219], "paragraph_keywords": ["dr", "multidr", "values", "view"]}, {"paragraph_vector": [-7.602138, -14.71267], "paragraph_keywords": ["weeks", "fig", "cluster", "orange"]}, {"paragraph_vector": [-7.934906, -15.516946], "paragraph_keywords": ["measurements", "fig", "cluster", "shown"]}, {"paragraph_vector": [-5.015477, -14.975359], "paragraph_keywords": ["activity", "cluster", "fig", "measures"]}, {"paragraph_vector": [-53.065181, -51.311199], "paragraph_keywords": ["network", "students", "contacts", "nodes"]}, {"paragraph_vector": [-43.506443, -55.326023], "paragraph_keywords": ["clusters", "students", "fig", "cluster"]}, {"paragraph_vector": [-8.985957, -11.873659], "paragraph_keywords": ["hardware", "logs", "time", "clusters"]}, {"paragraph_vector": [-11.00828, -7.324448], "paragraph_keywords": ["clusters", "cluster", "timestamp", "fig"]}, {"paragraph_vector": [-9.11122, -12.440524], "paragraph_keywords": ["timestamps", "cluster", "clusters", "instance"]}, {"paragraph_vector": [-8.94755, -12.093523], "paragraph_keywords": ["dr", "umap", "step", "matrix"]}, {"paragraph_vector": [-10.366575, -12.80305], "paragraph_keywords": ["umap", "pca", "clusters", "time"]}, {"paragraph_vector": [-9.740957, -13.185596], "paragraph_keywords": ["comparison", "view", "clusters", "mode"]}, {"paragraph_vector": [-13.937562, -14.863207], "paragraph_keywords": ["dr", "analyst", "quality", "time"]}, {"paragraph_vector": [-11.674469, -16.155891], "paragraph_keywords": ["time", "use", "multidr", "multivariate"]}], "content": {}, "doi": "10.1109/TVCG.2020.3030435"}, {"uri": "17", "title": "Visual Causality Analysis of Event Sequence Data", "timestamp": "2020", "rating": "0.0", "annotation": "", "tags": [], "authors": ["Zhuochen Jin", "Shunan Guo", "Nan Chen", "Daniel Weiskopf", "David Gotz", "Nan Cao"], "summary": "Causality is crucial to understanding the mechanisms behind complex systems and making decisions that lead to intended outcomes. Event sequence data is widely collected from many real-world processes, such as electronic health records, web clickstreams, and financial transactions, which transmit a great deal of information reflecting the causal relations among event types. Unfortunately, recovering causalities from observational event sequences is challenging, as the heterogeneous and high-dimensional event variables are often connected to rather complex underlying event excitation mechanisms that are hard to infer from limited observations. Many existing automated causal analysis techniques suffer from poor explainability and fail to include an adequate amount of human knowledge. In this paper, we introduce a visual analytics method for recovering causalities in event sequence data. We extend the Granger causality analysis algorithm on Hawkes processes to incorporate user feedback into causal model refinement. The visualization system includes an interactive causal analysis framework that supports bottom-up causal exploration, iterative causal verification and refinement, and causal comparison through a set of novel visualizations and interactions. We report two forms of evaluation: a quantitative evaluation of the model improvements resulting from the user-feedback mechanism, and a qualitative evaluation through case studies in different application domains to demonstrate the usefulness of the system.", "keywords": ["seqcausal", "feedback", "pattern", "term", "user", "graph", "mechanism", "record", "type", "effect", "node", "model", "system", "doctor", "hawkes", "number", "support", "granger", "update", "-", "value", "patient", "circle", "fig", "process", "expert", "occurrence", "modeling", "dataset", "according", "algorithm", "increase", "help", "relation", "based", "group", "data", "sequence", "relationship", "causality", "view", "method", "time", "cause", "result", "domain", "subsequence", "likelihood", "event", "design", "causal", "set", "analysis", "study"], "document_vector": [-142.893646, 55.255069], "paragraphs": [{"paragraph_vector": [-77.089889, -26.019369], "paragraph_keywords": ["events", "relationships", "causality", "sequences"]}, {"paragraph_vector": [-82.08583, -33.397468], "paragraph_keywords": ["event", "causality", "data", "analysis"]}, {"paragraph_vector": [-77.062042, -29.301195], "paragraph_keywords": ["event", "causalities", "relations", "causality"]}, {"paragraph_vector": [-78.039299, -28.902818], "paragraph_keywords": ["data", "modeling", "techniques", "event"]}, {"paragraph_vector": [-81.259613, -24.786983], "paragraph_keywords": ["based", "hawkes", "processes", "relationships"]}, {"paragraph_vector": [-80.870933, -23.769264], "paragraph_keywords": ["causality", "analysis", "support", "graph"]}, {"paragraph_vector": [-77.246955, -26.316223], "paragraph_keywords": ["event", "sequence", "relationships", "causalities"]}, {"paragraph_vector": [-78.822257, -23.348442], "paragraph_keywords": ["model", "causalities", "analysis", "system"]}, {"paragraph_vector": [-76.625, -26.181537], "paragraph_keywords": ["model", "module", "analysis", "ieee"]}, {"paragraph_vector": [-75.880729, -30.457397], "paragraph_keywords": ["event", "events", "processes", "hawkes"]}, {"paragraph_vector": [-81.693283, -26.559261], "paragraph_keywords": ["event", "v", "causal", "causality"]}, {"paragraph_vector": [-79.043296, -26.844278], "paragraph_keywords": ["user", "model", "causality", "number"]}, {"paragraph_vector": [-74.572914, -26.466323], "paragraph_keywords": ["sequence", "event", "view", "sequences"]}, {"paragraph_vector": [-73.301673, -27.334304], "paragraph_keywords": ["sequences", "event", "events", "analysis"]}, {"paragraph_vector": [-80.148628, -29.222341], "paragraph_keywords": ["graph", "causal", "algorithm", "layout"]}, {"paragraph_vector": [6.705206, -69.753997], "paragraph_keywords": ["circle", "nodes", "causality", "term"]}, {"paragraph_vector": [-78.844612, -28.689994], "paragraph_keywords": ["event", "user", "effect", "relations"]}, {"paragraph_vector": [-78.523315, -22.426589], "paragraph_keywords": ["user", "sequences", "model", "relations"]}, {"paragraph_vector": [-65.60218, -37.017288], "paragraph_keywords": ["subsequences", "causes", "occurrence", "j"]}, {"paragraph_vector": [-55.595497, -68.911773], "paragraph_keywords": ["cause", "view", "sequence", "events"]}, {"paragraph_vector": [-78.834991, -24.305406], "paragraph_keywords": ["graph", "update", "user", "analysis"]}, {"paragraph_vector": [-79.251876, -25.481641], "paragraph_keywords": ["model", "user", "circles", "regression"]}, {"paragraph_vector": [-77.098114, -25.620969], "paragraph_keywords": ["relation", "dataset", "groups", "study"]}, {"paragraph_vector": [-78.418479, -29.79762], "paragraph_keywords": ["model", "causality", "feedback", "websites"]}, {"paragraph_vector": [-79.850097, -20.546873], "paragraph_keywords": ["causality", "analysis", "user", "auroc"]}, {"paragraph_vector": [-63.538909, -19.880676], "paragraph_keywords": ["doctors", "bun", "system", "patients"]}, {"paragraph_vector": [-56.345794, -13.080049], "paragraph_keywords": ["patients", "doctors", "middle", "view"]}, {"paragraph_vector": [-73.997909, -23.360797], "paragraph_keywords": ["relations", "view", "sequence", "analysis"]}, {"paragraph_vector": [-74.834678, -36.388591], "paragraph_keywords": ["donald", "event", "subreddits", "sequences"]}, {"paragraph_vector": [-77.015342, -31.041576], "paragraph_keywords": ["time", "event", "algorithm", "analysis"]}, {"paragraph_vector": [-75.707954, -22.543039], "paragraph_keywords": ["analysis", "system", "causality", "event"]}, {"paragraph_vector": [-80.865036, -22.887212], "paragraph_keywords": ["events", "experts", "ieee", "system"]}], "content": {}, "doi": "10.1109/TVCG.2020.3030464"}, {"uri": "18", "title": "CNNPruner: Pruning Convolutional Neural Networks with Visual Analytics", "timestamp": "2020", "rating": "0.0", "annotation": "", "tags": [], "authors": ["Guan Li", "Junpeng Wang", "Han-Wei Shen", "Kaixin Chen", "Guihua Shan", "Zhonghua Lu"], "summary": "Convolutional neural networks (CNNs) have demonstrated extraordinarily good performance in many computer vision tasks. The increasing size of CNN models, however, prevents them from being widely deployed to devices with limited computational resources, e.g., mobile/embedded devices. The emerging topic of model pruning strives to address this problem by removing less important neurons and fine-tuning the pruned networks to minimize the accuracy loss. Nevertheless, existing automated pruning solutions often rely on a numerical threshold of the pruning criteria, lacking the flexibility to optimally balance the trade-off between efficiency and accuracy. Moreover, the complicated interplay between the stages of neuron pruning and model fine-tuning makes this process opaque, and therefore becomes difficult to optimize. In this paper, we address these challenges through a visual analytics approach, named CNNPruner. It considers the importance of convolutional filters through both instability and sensitivity, and allows users to interactively create pruning plans according to a desired goal on model size or accuracy. Also, CNNPruner integrates state-of-the-art filter visualization techniques to help users understand the roles that different filters played and refine their pruning plans. Through comprehensive case studies on CNNs with real-world sizes, we validate the effectiveness of CNNPruner.", "keywords": ["use", "filter", "image", "sensitivity", "user", "tuning", "node", "cnn", "model", "system", "learning", "plan", "research", "recognition", "number", "tree", "network", "understand", "instability", "li", "-", "size", "information", "accuracy", "value", "pruning", "fig", "process", "expert", "removing", "work", "show", "iteration", "parameter", "instance", "plot", "dataset", "help", "visualization", "based", "pruned", "layer", "represents", "data", "view", "prediction", "loss", "training", "cnns", "analysis", "cnnpruner", "recovery"], "document_vector": [-22.337247, -25.128465], "paragraphs": [{"paragraph_vector": [-12.666819, 59.092845], "paragraph_keywords": ["model", "parameters", "information", "cnns"]}, {"paragraph_vector": [-19.474813, 47.938907], "paragraph_keywords": ["pruning", "filters", "model", "models"]}, {"paragraph_vector": [-27.321149, 48.716648], "paragraph_keywords": ["pruning", "model", "filters", "cnnpruner"]}, {"paragraph_vector": [-19.510807, 45.769088], "paragraph_keywords": ["model", "pruning", "method", "filters"]}, {"paragraph_vector": [-27.034814, 51.268474], "paragraph_keywords": ["model", "use", "process", "networks"]}, {"paragraph_vector": [-29.202629, 44.910358], "paragraph_keywords": ["model", "visualization", "pruning", "training"]}, {"paragraph_vector": [-23.462097, 44.161331], "paragraph_keywords": ["filters", "model", "fi", "removing"]}, {"paragraph_vector": [-16.695503, 51.021236], "paragraph_keywords": ["filter", "image", "visualization", "cnn"]}, {"paragraph_vector": [-16.577928, 40.734088], "paragraph_keywords": ["model", "instability", "f", "filter"]}, {"paragraph_vector": [-20.792325, 45.20829], "paragraph_keywords": ["model", "pruning", "criteria", "process"]}, {"paragraph_vector": [-26.44314, 43.498718], "paragraph_keywords": ["model", "pruning", "pruned", "tree"]}, {"paragraph_vector": [-26.160991, 42.956455], "paragraph_keywords": ["model", "process", "use", "users"]}, {"paragraph_vector": [-25.092466, 41.859867], "paragraph_keywords": ["pruning", "model", "users", "tree"]}, {"paragraph_vector": [-23.52659, 43.87461], "paragraph_keywords": ["model", "process", "tuning", "accuracy"]}, {"paragraph_vector": [-23.991365, 43.177879], "paragraph_keywords": ["loss", "model", "tuning", "accuracy"]}, {"paragraph_vector": [-23.640003, 44.761951], "paragraph_keywords": ["model", "view", "process", "users"]}, {"paragraph_vector": [-18.460458, 51.096694], "paragraph_keywords": ["filter", "filters", "represents", "box"]}, {"paragraph_vector": [-16.19383, 54.370162], "paragraph_keywords": ["layers", "view", "plot", "model"]}, {"paragraph_vector": [-15.618412, 41.044013], "paragraph_keywords": ["view", "image", "instances", "filter"]}, {"paragraph_vector": [-19.5046, 44.740898], "paragraph_keywords": ["pruning", "parameters", "model", "cnn"]}, {"paragraph_vector": [-21.395917, 44.322753], "paragraph_keywords": ["pruning", "model", "accuracy", "tuning"]}, {"paragraph_vector": [-19.586055, 46.460468], "paragraph_keywords": ["model", "pruning", "images", "ieee"]}, {"paragraph_vector": [-26.798336, 41.351497], "paragraph_keywords": ["model", "filters", "pruning", "tuning"]}, {"paragraph_vector": [-23.052272, 44.999382], "paragraph_keywords": ["model", "pruning", "number", "changes"]}, {"paragraph_vector": [-21.974748, 43.958007], "paragraph_keywords": ["pruning", "model", "use", "removing"]}, {"paragraph_vector": [-21.298879, 45.331493], "paragraph_keywords": ["model", "pruning", "accuracy", "structure"]}, {"paragraph_vector": [-24.007133, 44.472801], "paragraph_keywords": ["filter", "buildings", "model", "pruning"]}, {"paragraph_vector": [-22.721162, 45.504653], "paragraph_keywords": ["pruning", "model", "plan", "mountain"]}, {"paragraph_vector": [-17.137262, 60.640884], "paragraph_keywords": ["model", "cnnpruner", "pruning", "image"]}, {"paragraph_vector": [-20.53339, 45.145095], "paragraph_keywords": ["pruning", "experts", "cnnpruner", "model"]}, {"paragraph_vector": [-21.637359, 44.807727], "paragraph_keywords": ["pruning", "networks", "conference", "ieee"]}, {"paragraph_vector": [-17.192386, 71.222328], "paragraph_keywords": ["learning", "li", "pruning", "liu"]}, {"paragraph_vector": [-15.128675, 68.093559], "paragraph_keywords": ["networks", "learning", "classifiers", "computer"]}, {"paragraph_vector": [-14.445424, 64.985786], "paragraph_keywords": ["ieee", "machine", "networks", "pattern"]}], "content": {}, "doi": "10.1109/TVCG.2020.3030447"}, {"uri": "19", "title": "Data-Driven Space-Filling Curves", "timestamp": "2020", "rating": "0.0", "annotation": "", "tags": [], "authors": ["Liang Zhou", "Chris R. Johnson", "Daniel Weiskopf"], "summary": "We propose a data-driven space-filling curve method for 2D and 3D visualization. Our flexible curve traverses the data elements in the spatial domain in a way that the resulting linearization better preserves features in space compared to existing methods. We achieve such data coherency by calculating a Hamiltonian path that approximately minimizes an objective function that describes the similarity of data values and location coherency in a neighborhood. Our extended variant even supports multiscale data via quadtrees and octrees. Our method is useful in many areas of visualization, including multivariate or comparative visualization, ensemble visualization of 2D and 3D data on regular grids, or multiscale visual analysis of particle simulations. The effectiveness of our method is evaluated with numerical comparisons to existing techniques and through examples of ensemble and multivariate datasets.", "keywords": ["use", "image", "feature", "locality", "graph", "spanning", "node", "block", "path", "tree", "curve", "linearization", "technique", "filling", "function", "level", "value", "driven", "fig", "space", "c", "dataset", "vertex", "visualization", "edge", "based", "shown", "volume", "datasets", "data", "method", "hilbert", "line", "coherency", "grid"], "document_vector": [-77.067817, -65.198211], "paragraphs": [{"paragraph_vector": [39.03054, -29.191162], "paragraph_keywords": ["data", "visualization", "curve", "filling"]}, {"paragraph_vector": [35.764755, -27.464235], "paragraph_keywords": ["data", "feature", "method", "fig"]}, {"paragraph_vector": [37.467281, -30.901161], "paragraph_keywords": ["data", "curve", "graph", "filling"]}, {"paragraph_vector": [35.594795, 9.256102], "paragraph_keywords": ["method", "data", "space", "curves"]}, {"paragraph_vector": [39.656902, -30.34752], "paragraph_keywords": ["nodes", "data", "path", "block"]}, {"paragraph_vector": [44.413948, -32.541774], "paragraph_keywords": ["curves", "data", "locality", "preservation"]}, {"paragraph_vector": [41.192558, -30.439939], "paragraph_keywords": ["function", "framework", "p", "preservation"]}, {"paragraph_vector": [31.591852, -63.882904], "paragraph_keywords": ["tree", "circuits", "spanning", "c"]}, {"paragraph_vector": [41.372039, -47.494556], "paragraph_keywords": ["j", "fig", "graph", "c"]}, {"paragraph_vector": [38.139778, -31.424547], "paragraph_keywords": ["data", "value", "fig", "method"]}, {"paragraph_vector": [41.475307, -40.498916], "paragraph_keywords": ["unit", "case", "cycle", "edges"]}, {"paragraph_vector": [39.732181, -30.01896], "paragraph_keywords": ["data", "driven", "pmin", "edges"]}, {"paragraph_vector": [39.543926, -31.642442], "paragraph_keywords": ["path", "level", "method", "curve"]}, {"paragraph_vector": [38.631698, -34.38533], "paragraph_keywords": ["path", "ft", "entry", "exit"]}, {"paragraph_vector": [39.488815, -29.701587], "paragraph_keywords": ["path", "space", "node", "filling"]}, {"paragraph_vector": [37.148643, -29.058355], "paragraph_keywords": ["block", "data", "autocorrelation", "ieee"]}, {"paragraph_vector": [45.332321, -23.265256], "paragraph_keywords": ["data", "datasets", "method", "autocorrelations"]}, {"paragraph_vector": [40.258003, -29.560913], "paragraph_keywords": ["data", "techniques", "grid", "coherency"]}, {"paragraph_vector": [41.460163, -28.276523], "paragraph_keywords": ["data", "line", "datasets", "plot"]}, {"paragraph_vector": [43.394016, -25.851606], "paragraph_keywords": ["dataset", "data", "method", "examples"]}, {"paragraph_vector": [45.090709, -19.228031], "paragraph_keywords": ["fig", "curve", "use", "image"]}, {"paragraph_vector": [39.361713, -27.474412], "paragraph_keywords": ["data", "method", "visualization", "methods"]}, {"paragraph_vector": [-106.105972, 68.240104], "paragraph_keywords": ["research", "ieee", "use", "project"]}], "content": {}, "doi": "10.1109/TVCG.2020.3030427"}, {"uri": "20", "title": "Semantic Discriminability for Visual Communication", "timestamp": "2020", "rating": "0.0", "annotation": "", "tags": [], "authors": ["Karen B. Schloss", "Zachary Leggon", "Laurent Lessard"], "summary": "To interpret information visualizations, observers must determine how visual features map onto concepts. First and foremost, this ability depends on perceptual discriminability; observers must be able to see the difference between different colors for those colors to communicate different meanings. However, the ability to interpret visualizations also depends on semantic discriminability, the degree to which observers can infer a unique mapping between visual features and concepts, based on the visual features and concepts alone (i.e., without help from verbal cues such as legends or labels). Previous evidence suggested that observers were better at interpreting encoding systems that maximized semantic discriminability (maximizing association strength between assigned colors and concepts while minimizing association strength between unassigned colors and concepts), compared to a system that only maximized color-concept association strength. However, increasing semantic discriminability also resulted in increased perceptual distance, so it is unclear which factor was responsible for improved performance. In the present study, we conducted two experiments that tested for independent effects of semantic distance and perceptual distance on semantic discriminability of bar graph data visualizations. Perceptual distance was large enough to ensure colors were more than just noticeably different. We found that increasing semantic distance improved performance, independent of variation in perceptual distance, and when these two factors were uncorrelated, responses were dominated by semantic distance. These results have implications for navigating trade-offs in color palette design optimization for visual communication.", "keywords": ["use", "concept", "feature", "problem", "graph", "association", "effect", "model", "participant", "strawberry", "pair", "information", "accuracy", "discriminability", "strength", "palette", "color", "fig", "people", "work", "target", "merit", "distance", "observer", "assignment", "cantaloupe", "bar", "visualization", "interpretability", "banana", "factor", "data", "given", "encoded", "design", "mapping", "experiment"], "document_vector": [45.082298, -9.88157], "paragraphs": [{"paragraph_vector": [75.900726, 61.865291], "paragraph_keywords": ["visualizations", "observers", "interpretability", "features"]}, {"paragraph_vector": [69.375625, 56.226131], "paragraph_keywords": ["discriminability", "features", "concepts", "observers"]}, {"paragraph_vector": [67.707351, 60.861267], "paragraph_keywords": ["interpretability", "mapping", "encoded", "discriminability"]}, {"paragraph_vector": [65.280403, 57.56604], "paragraph_keywords": ["discriminability", "color", "interpretability", "factors"]}, {"paragraph_vector": [63.479698, 59.958984], "paragraph_keywords": ["distance", "information", "visualizations", "biases"]}, {"paragraph_vector": [59.186462, 59.872676], "paragraph_keywords": ["merit", "color", "assignment", "use"]}, {"paragraph_vector": [58.604213, 60.870117], "paragraph_keywords": ["assignment", "merit", "concept", "color"]}, {"paragraph_vector": [66.940139, 60.629955], "paragraph_keywords": ["assignment", "participants", "inference", "trash"]}, {"paragraph_vector": [61.394889, 58.493457], "paragraph_keywords": ["colors", "participants", "discriminability", "concept"]}, {"paragraph_vector": [65.104293, 60.846923], "paragraph_keywords": ["colors", "association", "fruits", "color"]}, {"paragraph_vector": [61.982299, 58.413967], "paragraph_keywords": ["assignment", "colors", "discriminability", "association"]}, {"paragraph_vector": [62.073181, 62.756439], "paragraph_keywords": ["association", "concepts", "colors", "distance"]}, {"paragraph_vector": [56.71492, 62.042728], "paragraph_keywords": ["distance", "colors", "strawberry", "respect"]}, {"paragraph_vector": [60.769092, 59.731716], "paragraph_keywords": ["distance", "colors", "experiment", "fig"]}, {"paragraph_vector": [61.250102, 62.595645], "paragraph_keywords": ["bar", "monitor", "colors", "graph"]}, {"paragraph_vector": [68.997856, 61.198184], "paragraph_keywords": ["trials", "color", "participants", "bar"]}, {"paragraph_vector": [65.426597, 59.403144], "paragraph_keywords": ["color", "distance", "assignment", "accuracy"]}, {"paragraph_vector": [78.879356, 54.023517], "paragraph_keywords": ["distance", "model", "target", "accuracy"]}, {"paragraph_vector": [74.086486, 56.693016], "paragraph_keywords": ["color", "rt", "model", "distance"]}, {"paragraph_vector": [61.726852, 58.641151], "paragraph_keywords": ["experiment", "colors", "accuracy", "distance"]}, {"paragraph_vector": [66.704666, 56.272319], "paragraph_keywords": ["distance", "model", "accuracy", "correlated"]}, {"paragraph_vector": [65.871116, 58.28202], "paragraph_keywords": ["distance", "colors", "discriminability", "associated"]}, {"paragraph_vector": [66.625984, 58.9188], "paragraph_keywords": ["color", "assignments", "distance", "images"]}, {"paragraph_vector": [65.962524, 54.927494], "paragraph_keywords": ["color", "colors", "work", "palette"]}, {"paragraph_vector": [-107.714004, 69.365608], "paragraph_keywords": ["ieee", "republication", "redistribution", "requires"]}], "content": {}, "doi": "10.1109/TVCG.2020.3030393"}, {"uri": "21", "title": "Githru: Visual Analytics for Understanding Software Development History Through Git Metadata Analysis", "timestamp": "2020", "rating": "0.0", "annotation": "", "tags": [], "authors": ["Youngtaek Kim", "Jaeyoung Kim", "Hyeon Jeon", "Young-Ho Kim", "Hyunjoo Song", "Bohyoung Kim", "Jinwook Seo"], "summary": "Git metadata contains rich information for developers to understand the overall context of a large software development project. Thus it can help new developers, managers, and testers understand the history of development without needing to dig into a large pile of unfamiliar source code. However, the current tools for Git visualization are not adequate to analyze and explore the metadata: They focus mainly on improving the usability of Git commands instead of on helping users understand the development history. Furthermore, they do not scale for large and complex Git commit graphs, which can play an important role in understanding the overall development history. In this paper, we present Githru, an interactive visual analytics system that enables developers to effectively understand the context of development history through the interactive exploration of Git metadata. We design an interactive visual encoding idiom to represent a large Git graph in a scalable manner while preserving the topological structures in the Git graph. To enable scalable exploration of a large Git commit graph, we propose novel techniques (graph reconstruction, clustering, and Context-Preserving Squash Merge (CSM) methods) to abstract a large-scale Git commit graph. Based on these Git commit graph abstraction techniques, Githru provides an interactive summary view to help users gain an overview of the development history and a comparison view in which users can compare different clusters of commits. The efficacy of Githru has been demonstrated by case studies with domain experts using real-world, in-house datasets from a large software development team at a major international IT company. A controlled user study with 12 developers comparing Githru to previous tools also confirms the effectiveness of Githru in terms of task completion time.", "keywords": ["project", "branch", "user", "graph", "csm", "comparison", "cluster", "type", "structure", "system", "provides", "number", "pr", "github", "technique", "understand", "merge", "information", "code", "clustering", "context", "development", "fig", "expert", "question", "commits", "similarity", "work", "developer", "list", "task", "history", "requirement", "visualization", "stem", "based", "criterion", "dag", "file", "repository", "metadata", "data", "release", "view", "interview", "line", "selected", "git", "overview", "commit", "analysis", "source", "set", "study", "topology", "githru", "software"], "document_vector": [163.277938, -17.452445], "paragraphs": [{"paragraph_vector": [-122.867202, 29.482215], "paragraph_keywords": ["information", "kim", "commit", "university"]}, {"paragraph_vector": [-118.635696, 31.705253], "paragraph_keywords": ["git", "githru", "users", "scalability"]}, {"paragraph_vector": [-119.475769, 29.795558], "paragraph_keywords": ["git", "commits", "commit", "metadata"]}, {"paragraph_vector": [-120.879974, 27.544885], "paragraph_keywords": ["development", "commit", "information", "git"]}, {"paragraph_vector": [-122.676918, 28.931852], "paragraph_keywords": ["need", "overview", "history", "tools"]}, {"paragraph_vector": [-120.969398, 31.450241], "paragraph_keywords": ["git", "software", "visualization", "evolution"]}, {"paragraph_vector": [-119.341674, 29.539466], "paragraph_keywords": ["git", "data", "proposed", "commit"]}, {"paragraph_vector": [-124.324508, 26.3362], "paragraph_keywords": ["developers", "questions", "software", "code"]}, {"paragraph_vector": [-119.977813, 16.695039], "paragraph_keywords": ["development", "criteria", "software", "changed"]}, {"paragraph_vector": [-118.34761, 30.120429], "paragraph_keywords": ["git", "graph", "commits", "github"]}, {"paragraph_vector": [-118.57817, 31.83888], "paragraph_keywords": ["branch", "nodes", "branches", "links"]}, {"paragraph_vector": [-121.108474, 30.955776], "paragraph_keywords": ["commit", "merge", "stems", "csm"]}, {"paragraph_vector": [-121.553718, 32.22507], "paragraph_keywords": ["csm", "commits", "stems", "stem"]}, {"paragraph_vector": [-120.815223, 31.364919], "paragraph_keywords": ["commits", "cluster", "similarity", "commit"]}, {"paragraph_vector": [-120.88874, 30.454904], "paragraph_keywords": ["stems", "block", "number", "csm"]}, {"paragraph_vector": [-120.14611, 31.623182], "paragraph_keywords": ["clusters", "commit", "stem", "selected"]}, {"paragraph_vector": [-119.787635, 30.867626], "paragraph_keywords": ["file", "cluster", "users", "tree"]}, {"paragraph_vector": [-118.599807, 30.555513], "paragraph_keywords": ["set", "commit", "commits", "clustering"]}, {"paragraph_vector": [-120.90921, 31.202304], "paragraph_keywords": ["stem", "keyword", "comparison", "githru"]}, {"paragraph_vector": [-121.661514, 31.457605], "paragraph_keywords": ["commit", "view", "number", "comparison"]}, {"paragraph_vector": [-120.552658, 32.238338], "paragraph_keywords": ["repository", "word", "words", "git"]}, {"paragraph_vector": [-120.520927, 30.428171], "paragraph_keywords": ["githru", "find", "cluster", "commits"]}, {"paragraph_vector": [-118.77111, 28.572216], "paragraph_keywords": ["release", "github", "manager", "githru"]}, {"paragraph_vector": [-120.044464, 27.398487], "paragraph_keywords": ["release", "hotspot", "commits", "experts"]}, {"paragraph_vector": [-120.69384, 30.507429], "paragraph_keywords": ["stem", "stems", "project", "b"]}, {"paragraph_vector": [-123.056892, 26.901536], "paragraph_keywords": ["githru", "graph", "commits", "features"]}, {"paragraph_vector": [-123.257835, 24.482069], "paragraph_keywords": ["subjects", "github", "git", "githru"]}, {"paragraph_vector": [-120.190193, 30.155809], "paragraph_keywords": ["githru", "analysis", "team", "commit"]}, {"paragraph_vector": [-120.723892, 29.326774], "paragraph_keywords": ["branch", "metadata", "source", "information"]}, {"paragraph_vector": [-119.561485, 30.867298], "paragraph_keywords": ["git", "githru", "metadata", "issues"]}, {"paragraph_vector": [169.571228, 48.574729], "paragraph_keywords": ["research", "funded", "korea", "authors"]}], "content": {}, "doi": "10.1109/TVCG.2020.3030360"}, {"uri": "22", "title": "Advanced Rendering of Line Data with Ambient Occlusion and Transparency", "timestamp": "2020", "rating": "0.0", "annotation": "", "tags": [], "authors": ["David Gro\u00df", "Stefan Gumhold"], "summary": "3D Lines are a widespread rendering primitive for the visualization of data from research fields like fluid dynamics or fiber tractography. Global illumination effects and transparent rendering improve the perception of three-dimensional features and decrease occlusion within the data set, thus enabling better understanding of complex line data. We present an efficient approach for high quality GPU-based rendering of line data with ambient occlusion and transparency effects. Our approach builds on GPU-based raycasting of rounded cones, which are geometric primitives similar to truncated cones, but with spherical endcaps. Object space ambient occlusion is provided by an efficient voxel cone tracing approach. Our core contribution is a new fragment visibility sorting strategy that allows for interactive visualization of line data sets with millions of line segments. We improve performance further by exploiting hierarchical opacity maps.", "keywords": ["use", "transparency", "image", "density", "mipmap", "order", "primitive", "depth", "generated", "visibility", "segment", "frame", "blending", "sorting", "render", "voxel", "cone", "value", "k", "shader", "pixel", "color", "approach", "space", "process", "implementation", "work", "fragment", "need", "radius", "based", "rendering", "position", "buffer", "data", "view", "method", "et", "performance", "gpu", "point", "line", "culling", "given", "ray", "occlusion", "set", "geometry"], "document_vector": [-66.048133, -54.666221], "paragraphs": [{"paragraph_vector": [62.19342, -9.203974], "paragraph_keywords": ["lines", "line", "data", "geometry"]}, {"paragraph_vector": [60.598609, -10.072523], "paragraph_keywords": ["generated", "voxel", "geometry", "fragments"]}, {"paragraph_vector": [63.230613, -7.639743], "paragraph_keywords": ["line", "method", "rendering", "data"]}, {"paragraph_vector": [63.960132, -8.030383], "paragraph_keywords": ["geometry", "bounding", "ray", "casting"]}, {"paragraph_vector": [63.155632, -4.512354], "paragraph_keywords": ["geometry", "line", "lines", "rendering"]}, {"paragraph_vector": [58.888851, -6.329999], "paragraph_keywords": ["voxel", "rendering", "occlusion", "tracing"]}, {"paragraph_vector": [60.725742, -11.081998], "paragraph_keywords": ["buffer", "fragments", "transparency", "depth"]}, {"paragraph_vector": [61.395969, -11.695677], "paragraph_keywords": ["order", "data", "line", "approach"]}, {"paragraph_vector": [61.279937, -12.732282], "paragraph_keywords": ["buffer", "sect", "shader", "geometry"]}, {"paragraph_vector": [62.685958, -10.153898], "paragraph_keywords": ["buffer", "use", "segment", "line"]}, {"paragraph_vector": [60.08382, -12.528217], "paragraph_keywords": ["buffer", "pixel", "k", "gpu"]}, {"paragraph_vector": [62.914363, -10.806843], "paragraph_keywords": ["buffer", "depth", "bit", "order"]}, {"paragraph_vector": [58.031414, -12.395055], "paragraph_keywords": ["rendering", "buffer", "shader", "fragments"]}, {"paragraph_vector": [58.209682, -11.479146], "paragraph_keywords": ["mipmap", "opacity", "bounding", "level"]}, {"paragraph_vector": [60.302165, -10.218463], "paragraph_keywords": ["culling", "depth", "rendering", "shader"]}, {"paragraph_vector": [61.377799, -7.772837], "paragraph_keywords": ["cone", "silhouette", "space", "cones"]}, {"paragraph_vector": [62.114196, 11.629937], "paragraph_keywords": ["radius", "e", "sphere", "point"]}, {"paragraph_vector": [71.994522, 2.78566], "paragraph_keywords": ["direction", "view", "cone", "image"]}, {"paragraph_vector": [65.461509, -5.723717], "paragraph_keywords": ["cone", "point", "intersection", "angles"]}, {"paragraph_vector": [54.060523, -14.914509], "paragraph_keywords": ["grid", "voxel", "cone", "tracing"]}, {"paragraph_vector": [62.678234, -7.813504], "paragraph_keywords": ["segments", "line", "positions", "x"]}, {"paragraph_vector": [57.150947, -14.349358], "paragraph_keywords": ["data", "resolution", "view", "set"]}, {"paragraph_vector": [58.658489, -11.154213], "paragraph_keywords": ["streamlines", "transparency", "data", "segments"]}, {"paragraph_vector": [55.836826, -14.440788], "paragraph_keywords": ["data", "culling", "sets", "occlusion"]}, {"paragraph_vector": [57.553592, -12.659555], "paragraph_keywords": ["culling", "k", "rendering", "render"]}, {"paragraph_vector": [56.187004, -13.796672], "paragraph_keywords": ["data", "method", "vrc", "set"]}, {"paragraph_vector": [56.918819, -12.272686], "paragraph_keywords": ["layers", "mlabdb", "vrc", "approach"]}, {"paragraph_vector": [60.756881, -8.775898], "paragraph_keywords": ["method", "order", "lines", "artifacts"]}, {"paragraph_vector": [60.080917, -11.693109], "paragraph_keywords": ["grant", "sorting", "rendering", "number"]}], "content": {}, "doi": "10.1109/TVCG.2020.3030355"}, {"uri": "23", "title": "Context-aware Sampling of Large Networks via Graph Representation Learning", "timestamp": "2020", "rating": "0.0", "annotation": "", "tags": [], "authors": ["Zhiguang Zhou", "Chen Shi", "Xilong Shen", "Lihong Cai", "Haoxuan Wang", "Yuhua Liu", "Ying Zhao", "Wei Chen"], "summary": "Numerous sampling strategies have been proposed to simplify large-scale networks for highly readable visualizations. It is of great challenge to preserve contextual structures formed by nodes and edges with tight relationships in a sampled graph, because they are easily overlooked during the process of sampling due to their irregular distribution and immunity to scale. In this paper, a new graph sampling method is proposed oriented to the preservation of contextual structures. We first utilize a graph representation learning (GRL) model to transform nodes into vectors so that the contextual structures in a network can be effectively extracted and organized. Then, we propose a multi-objective blue noise sampling model to select a subset of nodes in the vectorized space to preserve contextual structures with the retention of relative data and cluster densities in addition to those features of significance, such as bridging nodes and graph connections. We also design a set of visual interfaces enabling users to interactively conduct context-aware sampling, visually compare results with various sampling strategies, and deeply explore large networks. Case studies and quantitative comparisons based on real-world datasets have demonstrated the effectiveness of our method in the abstraction and exploration of large networks.", "keywords": ["exploration", "feature", "user", "graph", "sampled", "node", "structure", "model", "community", "network", "al", "strategy", "poisson", "betweenness", "space", "preservation", "edge", "visualization", "based", "connection", "figure", "shown", "data", "metric", "relationship", "method", "vectorized", "representation", "result", "grl", "proposed", "sampling", "walk", "degree", "set", "analysis", "interest"], "document_vector": [-157.790161, -15.250774], "paragraphs": [{"paragraph_vector": [-20.637529, -52.642719], "paragraph_keywords": ["chen", "graph", "relationships", "university"]}, {"paragraph_vector": [-17.932016, -51.081455], "paragraph_keywords": ["structures", "nodes", "networks", "network"]}, {"paragraph_vector": [-14.01989, -50.109577], "paragraph_keywords": ["structures", "space", "vectorized", "sampling"]}, {"paragraph_vector": [-16.687129, -49.807365], "paragraph_keywords": ["graph", "sampling", "node", "networks"]}, {"paragraph_vector": [-18.299278, -52.771411], "paragraph_keywords": ["graph", "nodes", "graphs", "node"]}, {"paragraph_vector": [-18.356025, -52.060775], "paragraph_keywords": ["nodes", "degree", "node", "based"]}, {"paragraph_vector": [-13.740791, -53.437313], "paragraph_keywords": ["metrics", "al", "based", "graph"]}, {"paragraph_vector": [-15.40954, -53.953636], "paragraph_keywords": ["graph", "network", "mining", "analysis"]}, {"paragraph_vector": [-13.773989, -50.755065], "paragraph_keywords": ["sampling", "graph", "structures", "strategies"]}, {"paragraph_vector": [-12.478991, -50.766151], "paragraph_keywords": ["sampling", "strategies", "structures", "networks"]}, {"paragraph_vector": [-14.4883, -51.549644], "paragraph_keywords": ["node", "structures", "walk", "networks"]}, {"paragraph_vector": [-11.885452, -51.249889], "paragraph_keywords": ["nodes", "space", "function", "vectors"]}, {"paragraph_vector": [-15.139301, -50.877582], "paragraph_keywords": ["nodes", "poisson", "sampling", "disk"]}, {"paragraph_vector": [-11.097048, -50.694229], "paragraph_keywords": ["betweenness", "sampling", "nodes", "poisson"]}, {"paragraph_vector": [-12.69833, -53.283412], "paragraph_keywords": ["connection", "graph", "set", "sampled"]}, {"paragraph_vector": [-17.028512, -53.377693], "paragraph_keywords": ["structures", "sampling", "based", "networks"]}, {"paragraph_vector": [-16.363719, -52.569698], "paragraph_keywords": ["based", "users", "metrics", "feature"]}, {"paragraph_vector": [-11.849421, -49.648685], "paragraph_keywords": ["sampling", "method", "metrics", "isrw"]}, {"paragraph_vector": [-14.977986, -51.492576], "paragraph_keywords": ["method", "sampling", "structures", "based"]}, {"paragraph_vector": [-28.228237, -58.9622], "paragraph_keywords": ["communities", "structures", "figure", "results"]}, {"paragraph_vector": [-13.167844, -49.688735], "paragraph_keywords": ["nodes", "sampling", "network", "graph"]}, {"paragraph_vector": [-14.531187, -50.341739], "paragraph_keywords": ["sampling", "graph", "method", "connectivity"]}, {"paragraph_vector": [-13.093762, -51.463333], "paragraph_keywords": ["sampling", "graph", "users", "learning"]}, {"paragraph_vector": [-19.134925, -52.328304], "paragraph_keywords": ["zhejiang", "university", "ieee", "foundation"]}], "content": {}, "doi": "10.1109/TVCG.2020.3030391"}, {"uri": "24", "title": "Homomorphic-Encrypted Volume Rendering", "timestamp": "2020", "rating": "0.0", "annotation": "", "tags": [], "authors": ["Sebastian Mazza", "Daniel Patel", "Ivan Viola"], "summary": "Computationally demanding tasks are typically calculated in dedicated data centers, and real-time visualizations also follow this trend. Some rendering tasks, however, require the highest level of confidentiality so that no other party, besides the owner, can read or see the sensitive data. Here we present a direct volume rendering approach that performs volume rendering directly on encrypted volume data by using the homomorphic Paillier encryption algorithm. This approach ensures that the volume data and rendered image are uninterpretable to the rendering server. Our volume rendering pipeline introduces novel approaches for encrypted-data compositing, interpolation, and opacity modulation, as well as simple transfer function design, where each of these routines maintains the highest level of privacy. We present performance and memory overhead analysis that is associated with our privacy-preserving scheme. Our approach is open and secure by design, as opposed to secure through obscurity. Owners of the data only have to keep their secure key confidential to guarantee the privacy of their volume data and the rendered images. Our work is, to our knowledge, the first privacy-preserving remote volume-rendering approach that does not require that any server involved be trustworthy; even in cases when the server is compromised, no sensitive data will be leaked to a foreign party.", "keywords": ["use", "operation", "image", "density", "sample", "encrypted", "system", "number", "compositing", "memory", "function", "voxel", "-", "value", "information", "color", "approach", "privacy", "work", "dataset", "vector", "cloud", "algorithm", "need", "defined", "paillier", "rendering", "figure", "server", "volume", "data", "required", "result", "point", "client", "ray", "transfer", "floating", "encryption", "plaintext", "product", "dot", "cryptosystem", "camera", "exponent"], "document_vector": [-60.296703, -46.755371], "paragraphs": [{"paragraph_vector": [29.332443, -4.742675], "paragraph_keywords": ["rendering", "data", "hardware", "volume"]}, {"paragraph_vector": [28.165218, -3.112437], "paragraph_keywords": ["data", "algorithm", "image", "rendering"]}, {"paragraph_vector": [27.638597, -3.084741], "paragraph_keywords": ["rendering", "volume", "server", "data"]}, {"paragraph_vector": [29.111822, -4.774272], "paragraph_keywords": ["volume", "server", "cloud", "approach"]}, {"paragraph_vector": [28.494516, -2.214424], "paragraph_keywords": ["volume", "rendering", "data", "block"]}, {"paragraph_vector": [24.966999, -0.671462], "paragraph_keywords": ["values", "paillier", "volume", "cryptosystem"]}, {"paragraph_vector": [30.023368, -5.238206], "paragraph_keywords": ["client", "data", "needs", "voxel"]}, {"paragraph_vector": [28.733531, -3.00549], "paragraph_keywords": ["volume", "rendering", "server", "client"]}, {"paragraph_vector": [32.812797, -1.274644], "paragraph_keywords": ["image", "rendering", "paillier", "number"]}, {"paragraph_vector": [48.735569, -6.068765], "paragraph_keywords": ["ray", "sampling", "viewing", "compositing"]}, {"paragraph_vector": [27.068157, -2.817073], "paragraph_keywords": ["division", "plaintext", "sum", "encoding"]}, {"paragraph_vector": [27.484733, -2.329513], "paragraph_keywords": ["point", "floating", "e", "client"]}, {"paragraph_vector": [26.015464, -2.527165], "paragraph_keywords": ["point", "floating", "number", "mantissa"]}, {"paragraph_vector": [26.404884, -2.401066], "paragraph_keywords": ["mantissa", "plaintext", "k", "paillier"]}, {"paragraph_vector": [26.287322, -4.064095], "paragraph_keywords": ["values", "voxel", "function", "transfer"]}, {"paragraph_vector": [27.377346, -6.073402], "paragraph_keywords": ["function", "y", "value", "lookup"]}, {"paragraph_vector": [26.476207, -3.658184], "paragraph_keywords": ["equation", "vector", "xi", "matrix"]}, {"paragraph_vector": [27.436786, -4.161419], "paragraph_keywords": ["vector", "density", "approach", "volume"]}, {"paragraph_vector": [34.231925, -7.558123], "paragraph_keywords": ["density", "vector", "figure", "product"]}, {"paragraph_vector": [25.214401, -2.12891], "paragraph_keywords": ["rgb", "density", "user", "volume"]}, {"paragraph_vector": [27.941072, -3.470531], "paragraph_keywords": ["rgb", "pixel", "color", "encrypted"]}, {"paragraph_vector": [25.601715, -3.758342], "paragraph_keywords": ["required", "rendering", "obfuscation", "runtime"]}, {"paragraph_vector": [28.198215, -3.229568], "paragraph_keywords": ["volume", "rendering", "approach", "privacy"]}, {"paragraph_vector": [26.945892, -4.23789], "paragraph_keywords": ["approach", "rendering", "voxel", "cryptosystem"]}, {"paragraph_vector": [30.177438, -6.446713], "paragraph_keywords": ["values", "number", "bit", "jmk"]}, {"paragraph_vector": [26.852361, -5.128991], "paragraph_keywords": ["exponent", "number", "d", "paillier"]}, {"paragraph_vector": [27.206527, -5.429376], "paragraph_keywords": ["camera", "properties", "rendering", "data"]}, {"paragraph_vector": [28.223382, -4.572549], "paragraph_keywords": ["exponents", "information", "server", "image"]}, {"paragraph_vector": [27.35827, -3.301443], "paragraph_keywords": ["data", "volume", "exponent", "image"]}, {"paragraph_vector": [29.12181, -4.505346], "paragraph_keywords": ["rendering", "magnitude", "implementation", "performance"]}, {"paragraph_vector": [26.065744, -4.527927], "paragraph_keywords": ["thank", "kaust", "support", "alpha"]}], "content": {}, "doi": "10.1109/TVCG.2020.3030368"}, {"uri": "25", "title": "VisConnect: Distributed Event Synchronization for Collaborative Visualization", "timestamp": "2020", "rating": "0.0", "annotation": "", "tags": [], "authors": ["Michail Schwab", "Yixuan Zhang", "Shash Sinha", "Cristina Nita-Rotaru", "James Tompkin", "Michelle A. Borkin"], "summary": "Tools and interfaces are increasingly expected to be synchronous and distributed to accommodate remote collaboration. Yet, adoption of these techniques for data visualization is low partly because development is difficult: existing collaboration software systems either do not support simultaneous interaction or require expensive redevelopment of existing visualizations. We contribute VisConnect: a web-based synchronous distributed collaborative visualization system that supports most web-based SVG data visualizations, balances system safety with responsiveness, and supports simultaneous interaction from many collaborators. VisConnect works with existing visualization implementations with little-to-no code changes by synchronizing low-level JavaScript events across clients such that visualization updates proceed transparently across clients. This is accomplished via a peer-to-peer system that establishes consensus among clients on the per-element sequence of events, and uses a lock service to grant access over elements to clients. We contribute collaborative extensions of traditional visualization interaction techniques, such as drag, brush, and lasso, and discuss different strategies for collaborative visualization interactions. To demonstrate the utility of VisConnect, we present novel examples of collaborative visualizations in the healthcare domain, remote collaboration with annotation, and show in an education case study for e-learning with 22 participants that students found the ability to remotely collaborate on class activities helpful and enjoyable for understanding concepts. A free copy of this paper and source code are available on OSF at osf.io/ut7e6 and at visconnect.us.", "keywords": ["use", "computer", "way", "module", "element", "connected", "user", "synchronization", "text", "system", "learning", "web", "research", "shared", "support", "network", "lock", "state", "technique", "level", "information", "code", "link", "interaction", ".", "approach", "service", "people", "work", "distributed", "including", "visualization", "coordination", "existing", "collaboration", "based", "case", "created", "group", "data", "collaborator", "change", "view", "time", "visconnect", "doi", "point", "client", "communication", "event", "application", "drag", "design", "example", "student", "interface", "conflict", "dom"], "document_vector": [113.222167, 7.734567], "paragraphs": [{"paragraph_vector": [162.911743, -3.259123], "paragraph_keywords": ["visualization", "people", "distributed", "yixuan"]}, {"paragraph_vector": [165.374328, -5.192204], "paragraph_keywords": ["permitted", "replacing", "includes", "script"]}, {"paragraph_vector": [163.635421, -4.122466], "paragraph_keywords": ["data", "lock", "use", "visualization"]}, {"paragraph_vector": [158.484603, 0.028085], "paragraph_keywords": ["distributed", "visualization", "views", "research"]}, {"paragraph_vector": [163.074432, -8.398012], "paragraph_keywords": ["visualization", "approach", "visualizations", "data"]}, {"paragraph_vector": [167.198394, -2.813056], "paragraph_keywords": ["system", "use", "architecture", "seek"]}, {"paragraph_vector": [162.786285, -5.840404], "paragraph_keywords": ["data", "visualization", "visualizations", "application"]}, {"paragraph_vector": [164.722839, -9.045186], "paragraph_keywords": ["data", "visualization", "clients", "application"]}, {"paragraph_vector": [162.072555, -6.320614], "paragraph_keywords": ["approach", "visualizations", "data", "visualization"]}, {"paragraph_vector": [162.111923, -7.852675], "paragraph_keywords": ["clients", "events", "event", "module"]}, {"paragraph_vector": [159.615692, -6.287597], "paragraph_keywords": ["events", "clients", "network", "module"]}, {"paragraph_vector": [162.769104, -5.264495], "paragraph_keywords": ["events", "visualization", "state", "client"]}, {"paragraph_vector": [163.527191, -5.341349], "paragraph_keywords": ["lock", "client", "element", "service"]}, {"paragraph_vector": [165.385467, -7.264219], "paragraph_keywords": ["lock", "element", "second", "conflicts"]}, {"paragraph_vector": [161.633102, -7.387566], "paragraph_keywords": ["use", "peer", "network", "collaborators"]}, {"paragraph_vector": [162.089126, -6.353987], "paragraph_keywords": ["events", "network", "client", "clients"]}, {"paragraph_vector": [163.225418, -7.744533], "paragraph_keywords": ["visualization", "collaboration", "values", "interaction"]}, {"paragraph_vector": [161.806045, -7.136071], "paragraph_keywords": ["visualization", "visconnect", "collaboration", "collaborators"]}, {"paragraph_vector": [162.769607, -10.869019], "paragraph_keywords": ["state", "drag", "selections", "msg"]}, {"paragraph_vector": [162.810317, -7.46222], "paragraph_keywords": ["data", "collaborators", "interactions", "visualization"]}, {"paragraph_vector": [164.135711, -8.358294], "paragraph_keywords": ["events", "diabetes", "specify", "custom"]}, {"paragraph_vector": [-61.740516, -23.71124], "paragraph_keywords": ["interaction", "visconnect", "use", "information"]}, {"paragraph_vector": [162.997009, -7.458839], "paragraph_keywords": ["visconnect", "collaboration", "extension", "shared"]}, {"paragraph_vector": [-172.245239, 26.891946], "paragraph_keywords": ["group", "students", "level", "graph"]}, {"paragraph_vector": [-176.894302, 17.879135], "paragraph_keywords": ["use", "way", "nodes", "tool"]}, {"paragraph_vector": [164.552124, -3.792021], "paragraph_keywords": ["visconnect", "data", "approach", "system"]}, {"paragraph_vector": [164.836135, -4.603852], "paragraph_keywords": ["interaction", "interactions", "visualization", "use"]}, {"paragraph_vector": [162.971084, -6.069195], "paragraph_keywords": ["techniques", "communication", "collaboration", "coordination"]}, {"paragraph_vector": [163.049438, -7.71435], "paragraph_keywords": ["visualization", "use", "interaction", "visconnect"]}, {"paragraph_vector": [164.199096, 3.975822], "paragraph_keywords": ["doi", "visualization", ".", "elmqvist"]}, {"paragraph_vector": [170.572769, 6.11941], "paragraph_keywords": ["doi", "systems", ".", "conference"]}, {"paragraph_vector": [178.970367, 12.064603], "paragraph_keywords": ["doi", "ieee", "time", "computing"]}, {"paragraph_vector": [173.086715, 14.720539], "paragraph_keywords": ["visualization", "doi", "diabetes", "computing"]}], "content": {}, "doi": "10.1109/TVCG.2020.3030405"}, {"uri": "26", "title": "Kyrix-S: Authoring Scalable Scatterplot Visualizations of Big Data", "timestamp": "2020", "rating": "0.0", "annotation": "", "tags": [], "authors": ["Wenbo Tao", "Xinli Hou", "Adam Sah", "Leilani Battle", "Remco Chang", "Michael Stonebraker"], "summary": "Static scatterplots often suffer from the overdraw problem on big datasets where object overlap causes undesirable visual clutter. The use of zooming in scatterplots can help alleviate this problem. With multiple zoom levels, more screen real estate is available, allowing objects to be placed in a less crowded way. We call this type of visualization scalable scatterplot visualizations, or SSV for short. Despite the potential of SSVs, existing systems and toolkits fall short in supporting the authoring of SSVs due to three limitations. First, many systems have limited scalability, assuming that data fits in the memory of one computer. Second, too much developer work, e.g., using custom code to generate mark layouts or render objects, is required. Third, many systems focus on only a small subset of the SSV design space (e.g. supporting a specific type of visual marks). To address these limitations, we have developed Kyrix-S, a system for easy authoring of SSVs at scale. Kyrix-S derives a declarative grammar that enables specification of a variety of SSVs in a few tens of lines of code, based on an existing survey of scatterplot tasks and designs. The declarative grammar is supported by a distributed layout algorithm which automatically places visual marks onto zoom levels. We store data in a multi-node database and use multi-node spatial indexes to achieve interactive browsing of large SSVs. Extensive experiments show that 1) Kyrix-S enables interactive browsing of SSVs of billions of objects, with response times under 500ms and 2) Kyrix-S achieves 4X-9X reduction in specification compared to a state-of-the-art authoring system.", "keywords": ["use", "density", "range", "user", "problem", "cluster", "node", "system", "partition", "number", "tree", "custom", "component", "s", "level", "object", "authoring", "pan", "ssv", "space", "section", "layout", "mark", "developer", "show", "work", "grammar", "algorithm", "overlap", "kyrix", "visualization", "based", "figure", "scatterplots", "data", "rule", "zoom", "time", "ssvs", "split", "design", "database", "example", "aggregation"], "document_vector": [172.138137, -24.024726], "paragraphs": [{"paragraph_vector": [28.828416, -5.034133], "paragraph_keywords": ["objects", "scatterplots", "university", "data"]}, {"paragraph_vector": [100.633537, -42.755935], "paragraph_keywords": ["systems", "zoom", "objects", "ssv"]}, {"paragraph_vector": [88.486404, -52.280673], "paragraph_keywords": ["developer", "marks", "layout", "systems"]}, {"paragraph_vector": [88.605705, -50.342411], "paragraph_keywords": ["kyrix", "ssvs", "zoom", "s"]}, {"paragraph_vector": [87.488166, -48.266471], "paragraph_keywords": ["kyrix", "systems", "focus", "s"]}, {"paragraph_vector": [86.787658, -43.143829], "paragraph_keywords": ["objects", "s", "aggregation", "kyrix"]}, {"paragraph_vector": [117.93721, -66.402709], "paragraph_keywords": ["level", "kyrix", "grammar", "ssvs"]}, {"paragraph_vector": [86.269401, -51.275394], "paragraph_keywords": ["objects", "section", "kyrix", "object"]}, {"paragraph_vector": [84.731376, 9.605665], "paragraph_keywords": ["players", "purchases", "cluster", "figure"]}, {"paragraph_vector": [113.167457, -55.399936], "paragraph_keywords": ["marks", "rules", "cluster", "figure"]}, {"paragraph_vector": [117.594825, -61.119251], "paragraph_keywords": ["objects", "cluster", "component", "mark"]}, {"paragraph_vector": [117.806137, -51.79734], "paragraph_keywords": ["zoom", "figure", "objects", "use"]}, {"paragraph_vector": [99.323715, -55.502819], "paragraph_keywords": ["section", "zoom", "database", "data"]}, {"paragraph_vector": [100.532463, -46.999801], "paragraph_keywords": ["marks", "density", "levels", "overlap"]}, {"paragraph_vector": [79.530303, -41.229492], "paragraph_keywords": ["layout", "objects", "data", "cluster"]}, {"paragraph_vector": [80.205688, -49.250896], "paragraph_keywords": ["marks", "ncd", "overlap", "\u03b8"]}, {"paragraph_vector": [77.501113, -49.796417], "paragraph_keywords": ["level", "cluster", "ncd", "zoom"]}, {"paragraph_vector": [38.999679, -44.019462], "paragraph_keywords": ["algorithm", "zoom", "clusters", "partition"]}, {"paragraph_vector": [36.198455, -49.893505], "paragraph_keywords": ["split", "partition", "clusters", "tree"]}, {"paragraph_vector": [34.808605, -54.828796], "paragraph_keywords": ["clusters", "algorithm", "\u03b2", "database"]}, {"paragraph_vector": [33.506088, -52.936077], "paragraph_keywords": ["partition", "j", "data", "ti"]}, {"paragraph_vector": [76.361549, -55.38129], "paragraph_keywords": ["zoom", "kyrix", "partitions", "pan"]}, {"paragraph_vector": [82.879333, -54.586563], "paragraph_keywords": ["kyrix", "database", "postgresql", "performance"]}, {"paragraph_vector": [36.961338, -58.762435], "paragraph_keywords": ["data", "time", "partitions", "times"]}, {"paragraph_vector": [85.624458, -50.374908], "paragraph_keywords": ["kyrix", "s", "time", "zoom"]}, {"paragraph_vector": [91.383949, -48.627399], "paragraph_keywords": ["mark", "layout", "s", "use"]}, {"paragraph_vector": [86.086219, -50.992534], "paragraph_keywords": ["kyrix", "s", "data", "ieee"]}], "content": {}, "doi": "10.1109/TVCG.2020.3030426"}, {"uri": "27", "title": "Chemicals in the Creek: designing a situated data physicalization of open government data with the community", "timestamp": "2020", "rating": "0.0", "annotation": "", "tags": [], "authors": ["Laura J. Perovich", "Sara Ann Wylie", "Roseann Bongiovanni"], "summary": "Over the last decade growing amounts of government data have been made available in an attempt to increase transparency and civic participation, but it is unclear if this data serves non-expert communities due to gaps in access and the technical knowledge needed to interpret this \u201copen\u201d data. We conducted a two-year design study focused on the creation of a community-based data display using the United States Environmental Protection Agency data on water permit violations by oil storage facilities on the Chelsea Creek in Massachusetts to explore whether situated data physicalization and Participatory Action Research could support meaningful engagement with open data. We selected this data as it is of interest to local groups and available online, yet remains largely invisible and inaccessible to the Chelsea community. The resulting installation, Chemicals in the Creek, responds to the call for community-engaged visualization processes and provides an application of situated methods of data representation. It proposes event-centered and power-aware modes of engagement using contextual and embodied data representations. The design of Chemicals in the Creek is grounded in interactive workshops and we analyze it through event observation, interviews, and community outcomes. We reflect on the role of community engaged research in the Information Visualization community relative to recent conversations on new approaches to design studies and evaluation.", "keywords": ["use", "project", "way", "included", "greenroots", "study", "ceremony", "us", "display", "research", "create", "oil", "community", "creating", "participant", "experience", "air", "workshop", "object", "information", "creek", "context", "color", "fig", "process", "approach", "space", "people", "work", "chelsea", "action", "including", "member", "visualization", "chemical", "power", "based", "lantern", "tool", "issue", "pollution", "health", "group", "water", "data", "led", "engagement", "making", "violation", "time", "method", "building", "performance", "quality", "physicalization", "researcher", "event", "design", "team", "par", "engage", "student", "example", "eco", "facility", "participation"], "document_vector": [-55.672565, 71.880897], "paragraphs": [{"paragraph_vector": [153.034149, 41.300815], "paragraph_keywords": ["data", "chelsea", "oil", "water"]}, {"paragraph_vector": [156.818145, 42.704586], "paragraph_keywords": ["par", "university", "ieee", "community"]}, {"paragraph_vector": [153.876068, 38.714263], "paragraph_keywords": ["design", "par", "action", "community"]}, {"paragraph_vector": [156.410156, 38.584884], "paragraph_keywords": ["research", "data", "par", "action"]}, {"paragraph_vector": [152.394638, 34.479297], "paragraph_keywords": ["data", "displays", "community", "environment"]}, {"paragraph_vector": [152.387603, 37.516273], "paragraph_keywords": ["data", "issues", "visualizations", "use"]}, {"paragraph_vector": [150.382141, 40.797458], "paragraph_keywords": ["information", "air", "data", "display"]}, {"paragraph_vector": [156.285537, 37.065223], "paragraph_keywords": ["data", "visualization", "engagement", "tools"]}, {"paragraph_vector": [157.063568, 33.358982], "paragraph_keywords": ["data", "research", "event", "community"]}, {"paragraph_vector": [152.7695, 38.018238], "paragraph_keywords": ["event", "greenroots", "group", "ieee"]}, {"paragraph_vector": [148.738052, 46.067024], "paragraph_keywords": ["data", "eco", "researchers", "water"]}, {"paragraph_vector": [145.719665, 44.979312], "paragraph_keywords": ["event", "researchers", "eco", "greenroots"]}, {"paragraph_vector": [150.075759, 42.728233], "paragraph_keywords": ["event", "team", "project", "chelsea"]}, {"paragraph_vector": [151.576278, 44.054683], "paragraph_keywords": ["data", "community", "violations", "violation"]}, {"paragraph_vector": [149.500915, 40.859176], "paragraph_keywords": ["community", "engage", "data", "space"]}, {"paragraph_vector": [150.258651, 40.051506], "paragraph_keywords": ["violation", "violations", "object", "information"]}, {"paragraph_vector": [145.370483, 46.067474], "paragraph_keywords": ["ceremony", "use", "visualization", "ieee"]}, {"paragraph_vector": [102.485115, 58.921913], "paragraph_keywords": ["data", "color", "information", "community"]}, {"paragraph_vector": [150.345504, 42.973079], "paragraph_keywords": ["event", "greenroots", "community", "lanterns"]}, {"paragraph_vector": [150.841339, 42.873359], "paragraph_keywords": ["community", "lanterns", "ieee", "participants"]}, {"paragraph_vector": [154.140151, 37.666046], "paragraph_keywords": ["data", "information", "messages", "lantern"]}, {"paragraph_vector": [157.361831, 35.498443], "paragraph_keywords": ["water", "data", "event", "lanterns"]}, {"paragraph_vector": [152.206924, 42.418128], "paragraph_keywords": ["event", "people", "nature", "contrast"]}, {"paragraph_vector": [149.629898, 39.710544], "paragraph_keywords": ["data", "community", "event", "use"]}, {"paragraph_vector": [147.912109, 40.181484], "paragraph_keywords": ["community", "eco", "time", "researchers"]}, {"paragraph_vector": [157.459884, 39.46001], "paragraph_keywords": ["data", "participation", "project", "process"]}, {"paragraph_vector": [153.334686, 41.507812], "paragraph_keywords": ["youth", "power", "community", "event"]}, {"paragraph_vector": [150.525863, 41.968769], "paragraph_keywords": ["event", "data", "greenroots", "use"]}, {"paragraph_vector": [153.30368, 41.519546], "paragraph_keywords": ["data", "community", "space", "work"]}, {"paragraph_vector": [153.037628, 41.149215], "paragraph_keywords": ["data", "par", "visualization", "methods"]}, {"paragraph_vector": [151.877471, 38.197803], "paragraph_keywords": ["data", "community", "study", "physicalization"]}, {"paragraph_vector": [152.920181, 37.049457], "paragraph_keywords": ["data", "community", "ieee", "thanks"]}], "content": {}, "doi": "10.1109/TVCG.2020.3030369"}, {"uri": "28", "title": "Shared Surfaces and Spaces: Collaborative Data Visualisation in a Co-located Immersive Environment", "timestamp": "2020", "rating": "0.0", "annotation": "", "tags": [], "authors": ["Benjamin Lee", "Xiaoyun Hu", "Maxime Cordeil", "Arnaud Prouzeau", "Bernhard Jenny", "Tim Dwyer"], "summary": "Immersive technologies offer new opportunities to support collaborative visual data analysis by providing each collaborator a personal, high-resolution view of a flexible shared visualisation space through a head mounted display. However, most prior studies of collaborative immersive analytics have focused on how groups interact with surface interfaces such as tabletops and wall displays. This paper reports on a study in which teams of three co-located participants are given flexible visualisation authoring tools to allow a great deal of control in how they structure their shared workspace. They do so using a prototype system we call FIESTA: the Free-roaming Immersive Environment to Support Team-based Analysis. Unlike traditional visualisation tools, FIESTA allows users to freely position authoring interfaces and visualisation artefacts anywhere in the virtual environment, either on virtual surfaces or suspended within the interaction space. Our participants solved visual analytics tasks on a multivariate data set, doing so individually and collaboratively by creating a large number of 2D and 3D visualisations. Their behaviours suggest that the usage of surfaces is coupled with the type of visualisation used, often using walls to organise 2D visualisations, but positioning 3D visualisations in the space around them. Outside of tightly-coupled collaboration, participants followed social protocols and did not interact with visualisations that did not belong to them even if outside of its owner\u2019s personal workspace.", "keywords": ["use", "property", "user", "alice", "ar", "display", "system", "territory", "shared", "observed", "participant", "al", "environment", "working", "object", "audience", "analytics", "interaction", "wall", "space", "fig", "surface", "work", "task", "action", "workspace", "vr", "table", "coupled", "pointer", "visualisation", "collaboration", "explore", "based", "group", "data", "stage", "awareness", "finding", "view", "time", "et", "carol", "result", "fiesta", "given", "panel", "presentation", "room", "study"], "document_vector": [99.345283, 21.46537], "paragraphs": [{"paragraph_vector": [138.750839, -5.585876], "paragraph_keywords": ["displays", "use", "environments", "data"]}, {"paragraph_vector": [142.254684, -0.346844], "paragraph_keywords": ["use", "participants", "visualisations", "groups"]}, {"paragraph_vector": [136.557601, -4.588456], "paragraph_keywords": ["visualisation", "pairs", "users", "analytics"]}, {"paragraph_vector": [138.316879, -2.428605], "paragraph_keywords": ["territories", "displays", "environment", "shared"]}, {"paragraph_vector": [135.782562, -1.665502], "paragraph_keywords": ["work", "windows", "fixed", "users"]}, {"paragraph_vector": [142.253143, -0.304271], "paragraph_keywords": ["data", "users", "fiesta", "analytics"]}, {"paragraph_vector": [121.820564, -13.851526], "paragraph_keywords": ["panel", "grasping", "visualisations", "interface"]}, {"paragraph_vector": [135.302886, -2.60548], "paragraph_keywords": ["use", "user", "users", "visualisations"]}, {"paragraph_vector": [137.708267, -0.07714], "paragraph_keywords": ["users", "interaction", "visualisations", "room"]}, {"paragraph_vector": [141.649749, -0.383392], "paragraph_keywords": ["vr", "groups", "study", "desktop"]}, {"paragraph_vector": [142.651733, 17.763719], "paragraph_keywords": ["given", "groups", "task", "controlled"]}, {"paragraph_vector": [144.606231, 4.408952], "paragraph_keywords": ["participants", "given", "questionnaire", "study"]}, {"paragraph_vector": [144.515884, 0.02314], "paragraph_keywords": ["actions", "groups", "study", "use"]}, {"paragraph_vector": [142.804183, 7.284689], "paragraph_keywords": ["participants", "reported", "study", "visualisations"]}, {"paragraph_vector": [81.818923, -24.396419], "paragraph_keywords": ["panels", "groups", "data", "ieee"]}, {"paragraph_vector": [80.149185, -25.319116], "paragraph_keywords": ["visualisations", "axis", "dimensions", "layouts"]}, {"paragraph_vector": [137.087112, -0.76494], "paragraph_keywords": ["collaboration", "visualisations", "participants", "clutching"]}, {"paragraph_vector": [136.651428, 2.087375], "paragraph_keywords": ["table", "visualisations", "visualisation", "participants"]}, {"paragraph_vector": [133.210052, 6.496847], "paragraph_keywords": ["table", "visualisation", "room", "visualisations"]}, {"paragraph_vector": [138.553634, 2.918562], "paragraph_keywords": ["participants", "visualisation", "visualisations", "coupled"]}, {"paragraph_vector": [129.577102, -4.649969], "paragraph_keywords": ["visualisations", "audience", "presentation", "findings"]}, {"paragraph_vector": [138.938552, 5.077], "paragraph_keywords": ["visualisations", "audience", "participants", "analytics"]}, {"paragraph_vector": [144.860549, 3.413936], "paragraph_keywords": ["visualisations", "use", "participants", "dimensions"]}, {"paragraph_vector": [142.723312, 2.011062], "paragraph_keywords": ["visualisations", "participants", "performance", "task"]}, {"paragraph_vector": [145.377273, 0.586877], "paragraph_keywords": ["use", "work", "table", "authoring"]}, {"paragraph_vector": [139.964904, -1.093712], "paragraph_keywords": ["participants", "work", "territories", "coupled"]}, {"paragraph_vector": [142.913009, 1.240383], "paragraph_keywords": ["awareness", "privacy", "visualisations", "stage"]}, {"paragraph_vector": [143.255493, 0.881122], "paragraph_keywords": ["participants", "work", "environment", "presentation"]}, {"paragraph_vector": [144.73117, 1.250171], "paragraph_keywords": ["research", "ieee", "distributed", "use"]}], "content": {}, "doi": "10.1109/TVCG.2020.3030372"}, {"uri": "29", "title": "Evaluation of Sampling Methods for Scatterplots", "timestamp": "2020", "rating": "0.0", "annotation": "", "tags": [], "authors": ["Jun Yuan", "Shouxing Xiang", "Jiazhi Xia", "Lingyun Yu", "Shixia Liu"], "summary": "Given a scatterplot with tens of thousands of points or even more, a natural question is which sampling method should be used to create a small but \u201dgood\u201d scatterplot for a better abstraction. We present the results of a user study that investigates the influence of different sampling strategies on multi-class scatterplots. The main goal of this study is to understand the capability of sampling methods in preserving the density, outliers, and overall shape of a scatterplot. To this end, we comprehensively review the literature and select seven typical sampling strategies as well as eight representative datasets. We then design four experiments to understand the performance of different strategies in maintaining: 1) region density; 2) class density; 3) outliers; and 4) overall shape in the sampling results. The results show that: 1) random sampling is preferred for preserving region density; 2) blue noise sampling and random sampling have comparable performance with the three multi-class sampling strategies in preserving class density; 3) outlier biased density based sampling, recursive subdivision based sampling, and blue noise sampling perform the best in keeping outliers; and 4) blue noise sampling outperforms the others in maintaining the overall shape of a scatterplot.", "keywords": ["use", "density", "region", "term", "order", "sample", "number", "test", "participant", "shape", "strategy", "perception", "color", "space", "task", "dataset", "outlier", "noise", "visualization", "based", "preserving", "scatterplots", "datasets", "scatterplot", "factor", "data", "recall", "evaluation", "method", "asked", "result", "point", "selected", "design", "sampling", "class", "study", "experiment"], "document_vector": [-98.011329, -23.341522], "paragraphs": [{"paragraph_vector": [14.949248, 8.803213], "paragraph_keywords": ["data", "sampling", "visualization", "scatterplots"]}, {"paragraph_vector": [19.686517, 23.31925], "paragraph_keywords": ["sampling", "based", "density", "strategies"]}, {"paragraph_vector": [20.516994, 21.251775], "paragraph_keywords": ["sampling", "strategies", "participants", "based"]}, {"paragraph_vector": [21.811691, 22.329635], "paragraph_keywords": ["sampling", "samples", "space", "strategies"]}, {"paragraph_vector": [23.673515, 20.75604], "paragraph_keywords": ["sampling", "density", "outliers", "samples"]}, {"paragraph_vector": [20.576223, 21.218479], "paragraph_keywords": ["sampling", "samples", "class", "based"]}, {"paragraph_vector": [14.009849, 17.391931], "paragraph_keywords": ["sampling", "visualization", "methods", "data"]}, {"paragraph_vector": [19.863618, 20.909692], "paragraph_keywords": ["sampling", "strategies", "selected", "based"]}, {"paragraph_vector": [20.914806, 19.322895], "paragraph_keywords": ["points", "sampling", "strategies", "shape"]}, {"paragraph_vector": [28.821456, 22.324104], "paragraph_keywords": ["sampling", "sampled", "number", "points"]}, {"paragraph_vector": [24.727888, 23.279613], "paragraph_keywords": ["sampling", "participants", "experiment", "scatterplots"]}, {"paragraph_vector": [22.785976, 22.704286], "paragraph_keywords": ["density", "color", "region", "experiment"]}, {"paragraph_vector": [68.523895, 58.62857], "paragraph_keywords": ["participants", "experiment", "time", "test"]}, {"paragraph_vector": [22.188226, 19.702989], "paragraph_keywords": ["density", "sampling", "strategies", "region"]}, {"paragraph_vector": [22.439388, 20.312994], "paragraph_keywords": ["sampling", "experiment", "density", "strategies"]}, {"paragraph_vector": [25.621458, 16.883512], "paragraph_keywords": ["sampling", "class", "density", "color"]}, {"paragraph_vector": [22.109193, 21.125329], "paragraph_keywords": ["outliers", "participants", "class", "number"]}, {"paragraph_vector": [19.540945, 19.497659], "paragraph_keywords": ["sampling", "participants", "experiments", "shape"]}, {"paragraph_vector": [22.063899, 22.565011], "paragraph_keywords": ["experiments", "participants", "session", "sampling"]}, {"paragraph_vector": [15.452219, 21.882968], "paragraph_keywords": ["sampling", "recall", "outliers", "ratio"]}, {"paragraph_vector": [22.27717, 22.694278], "paragraph_keywords": ["sampling", "strategies", "p", "significance"]}, {"paragraph_vector": [20.846744, 21.673171], "paragraph_keywords": ["sampling", "based", "strategies", "density"]}, {"paragraph_vector": [21.183317, 20.52559], "paragraph_keywords": ["sampling", "noise", "recall", "based"]}, {"paragraph_vector": [21.301349, 22.303052], "paragraph_keywords": ["sampling", "density", "shape", "participants"]}, {"paragraph_vector": [23.877382, 21.80438], "paragraph_keywords": ["sampling", "factors", "density", "classes"]}, {"paragraph_vector": [22.533622, 19.199617], "paragraph_keywords": ["sampling", "factors", "strategies", "preservation"]}, {"paragraph_vector": [18.53803, 20.537195], "paragraph_keywords": ["sampling", "ieee", "china", "use"]}], "content": {}, "doi": "10.1109/TVCG.2020.3030367"}, {"uri": "30", "title": "Comparative Layouts Revisited: Design Space, Guidelines, and Future Directions", "timestamp": "2020", "rating": "0.0", "annotation": "", "tags": [], "authors": ["Sehi L\u2019Yi", "Jaemin Jo", "Jinwook Seo"], "summary": "We present a systematic review on three comparative layouts\u2014juxtaposition, superposition, and explicit-encoding\u2014which are information visualization (InfoVis) layouts designed to support comparison tasks. For the last decade, these layouts have served as fundamental idioms in designing many visualization systems. However, we found that the layouts have been used with inconsistent terms and confusion, and the lessons from previous studies are fragmented. The goal of our research is to distill the results from previous studies into a consistent and reusable framework. We review 127 research papers, including 15 papers with quantitative user studies, which employed comparative layouts. We first alleviate the ambiguous boundaries in the design space of comparative layouts by suggesting lucid terminology (e.g., chart-wise and item-wise juxtaposition). We then identify the diverse aspects of comparative layouts, such as the advantages and concerns of using each layout in the real-world scenarios and researchers\u2019 approaches to overcome the concerns. Building our knowledge on top of the initial insights gained from the Gleicher et al.\u2019s survey [19], we elaborate on relevant empirical evidence that we distilled from our survey (e.g., the actual effectiveness of the layouts in different study settings) and identify novel facets that the original work did not cover (e.g., the familiarity of the layouts to people). Finally, we show the consistent and contradictory results on the performance of comparative layouts and offer practical implications for using the layouts by suggesting trade-offs and seven actionable guidelines.", "keywords": ["use", "found", "element", "explicit", "term", "user", "comparison", "effectiveness", "transition", "comparing", "research", "number", "al", "link", "color", "fig", "people", "item", "layout", "task", "difference", "paper", "bar", "visualization", "data", "et", "performance", "chart", "result", "encoding", "juxtaposition", "researcher", "compared", "superposition", "design", "example", "study", "showed"], "document_vector": [63.382129, 5.687052], "paragraphs": [{"paragraph_vector": [121.805984, -31.022302], "paragraph_keywords": ["layouts", "studies", "juxtaposition", "superposition"]}, {"paragraph_vector": [123.725746, -26.928392], "paragraph_keywords": ["layouts", "study", "researchers", "design"]}, {"paragraph_vector": [127.176445, -36.294315], "paragraph_keywords": ["layouts", "visualization", "visualizations", "encoding"]}, {"paragraph_vector": [129.070968, -28.407888], "paragraph_keywords": ["papers", "layouts", "visualization", "layout"]}, {"paragraph_vector": [124.433067, -28.029426], "paragraph_keywords": ["papers", "layouts", "studies", "publications"]}, {"paragraph_vector": [119.373764, -32.075469], "paragraph_keywords": ["tasks", "visualizations", "comparison", "comparing"]}, {"paragraph_vector": [126.076881, -35.960594], "paragraph_keywords": ["juxtaposition", "chart", "layouts", "superposition"]}, {"paragraph_vector": [123.579582, -34.56837], "paragraph_keywords": ["juxtaposition", "visualizations", "chart", "item"]}, {"paragraph_vector": [125.336181, -36.312793], "paragraph_keywords": ["difference", "transition", "bar", "chart"]}, {"paragraph_vector": [125.472763, -40.082473], "paragraph_keywords": ["visualizations", "layouts", "juxtaposition", "charts"]}, {"paragraph_vector": [124.151268, -37.08707], "paragraph_keywords": ["visualizations", "juxtaposition", "people", "claimed"]}, {"paragraph_vector": [119.832687, -33.207214], "paragraph_keywords": ["visualizations", "superposition", "juxtaposition", "comparison"]}, {"paragraph_vector": [127.208358, -37.715751], "paragraph_keywords": ["encoding", "visualizations", "juxtaposition", "people"]}, {"paragraph_vector": [125.402053, -35.800979], "paragraph_keywords": ["visualizations", "comparison", "requires", "people"]}, {"paragraph_vector": [120.165565, -31.262411], "paragraph_keywords": ["juxtaposition", "chart", "studies", "user"]}, {"paragraph_vector": [117.720626, -34.207977], "paragraph_keywords": ["encoding", "visualizations", "explicit", "interference"]}, {"paragraph_vector": [126.645751, -37.227268], "paragraph_keywords": ["layouts", "encoding", "use", "visualizations"]}, {"paragraph_vector": [119.552703, -31.026222], "paragraph_keywords": ["layouts", "layout", "scalability", "ieee"]}, {"paragraph_vector": [120.402709, -31.780433], "paragraph_keywords": ["juxtaposition", "tasks", "visualizations", "relationship"]}, {"paragraph_vector": [123.451896, -33.149051], "paragraph_keywords": ["juxtaposition", "studies", "landmarks", "layouts"]}, {"paragraph_vector": [120.818038, -31.85316], "paragraph_keywords": ["use", "heatmaps", "designers", "charts"]}, {"paragraph_vector": [125.887733, -34.024982], "paragraph_keywords": ["encoding", "layout", "chart", "performance"]}, {"paragraph_vector": [124.424804, -37.501064], "paragraph_keywords": ["performance", "tasks", "comparison", "chart"]}, {"paragraph_vector": [120.425819, -35.009967], "paragraph_keywords": ["difference", "juxtaposition", "papers", "layouts"]}, {"paragraph_vector": [124.074989, -36.149562], "paragraph_keywords": ["research", "papers", "layouts", "university"]}], "content": {}, "doi": "10.1109/TVCG.2020.3030475"}, {"uri": "31", "title": "CcNav: Understanding Compiler Optimizations in Binary Code", "timestamp": "2020", "rating": "0.0", "annotation": "", "tags": [], "authors": ["Sabin Devkota", "Pascal Aschwanden", "Adam Kunen", "Matthew Legendre", "Katherine E. Isaacs"], "summary": "Program developers spend significant time on optimizing and tuning programs. During this iterative process, they apply optimizations, analyze the resulting code, and modify the compilation until they are satisfied. Understanding what the compiler did with the code is crucial to this process but is very time-consuming and labor-intensive. Users need to navigate through thousands of lines of binary code and correlate it to source code concepts to understand the results of the compilation and to identify optimizations. We present a design study in collaboration with program developers and performance analysts. Our collaborators work with various artifacts related to the program such as binary code, source code, control flow graphs, and call graphs. Through interviews, feedback, and pair-analytics sessions, we analyzed their tasks and workflow. Based on this task analysis and through a human-centric design process, we designed a visual analytics system Compilation Navigator (CcNav ) to aid exploration of the effects of compiler optimizations on the program. CcNav provides a streamlined workflow and a unified context that integrates disparate artifacts. CcNav supports consistent interactions across all the artifacts making it easy to correlate binary code with source code concepts. CcNav enables users to navigate and filter large binary code to identify and summarize optimizations such as inlining, vectorization, loop unrolling, and code hoisting. We evaluate CcNav through guided sessions and semi-structured interviews. We reflect on our design process, particularly the immersive elements, and on the transferability of design studies through our experience with a previous design study on program analysis.", "keywords": ["use", "compiled", "project", "program", "found", "study", "graph", "cfg", "ccnav", "node", "system", "performed", "instruction", "support", "participant", "function", "session", "level", "-", "compiler", "information", "code", "link", "pair", "analytics", "workflow", "context", "expert", "process", "selection", "item", "task", "visualization", "understanding", "compilation", "file", "hierarchy", "data", "evaluation", "view", "time", "performance", "domain", "limitation", "selected", "cfgexplorer", "design", "automated", "source", "analysis", "interest", "loop", "optimization"], "document_vector": [-45.904842, 7.041207], "paragraphs": [{"paragraph_vector": [179.14859, -30.526321], "paragraph_keywords": ["compiler", "use", "ieee", "application"]}, {"paragraph_vector": [-178.823425, -27.901224], "paragraph_keywords": ["analysis", "design", "experts", "data"]}, {"paragraph_vector": [179.140289, -28.83856], "paragraph_keywords": ["machine", "sect", "compilation", "program"]}, {"paragraph_vector": [178.033386, -28.617998], "paragraph_keywords": ["loop", "code", "instructions", "function"]}, {"paragraph_vector": [179.516952, -28.607753], "paragraph_keywords": ["blocks", "function", "code", "instructions"]}, {"paragraph_vector": [179.117614, -29.47706], "paragraph_keywords": ["source", "tools", "code", "files"]}, {"paragraph_vector": [177.373413, -32.048683], "paragraph_keywords": ["source", "design", "instructions", "tool"]}, {"paragraph_vector": [-179.192901, -30.006242], "paragraph_keywords": ["analysis", "expert", "visualization", "experts"]}, {"paragraph_vector": [-178.713836, -27.925395], "paragraph_keywords": ["compiler", "source", "code", "mapping"]}, {"paragraph_vector": [178.242477, -29.172889], "paragraph_keywords": ["code", "source", "optimizations", "tasks"]}, {"paragraph_vector": [-178.630142, -27.619949], "paragraph_keywords": ["code", "optimizations", "debug", "compilation"]}, {"paragraph_vector": [-178.944992, -26.723535], "paragraph_keywords": ["tasks", "data", "analysis", "program"]}, {"paragraph_vector": [-177.601226, -29.67756], "paragraph_keywords": ["source", "view", "use", "views"]}, {"paragraph_vector": [-177.33403, -28.097913], "paragraph_keywords": ["cfg", "cfgexplorer", "nodes", "loops"]}, {"paragraph_vector": [-178.512298, -28.236982], "paragraph_keywords": ["use", "inlining", "view", "function"]}, {"paragraph_vector": [-178.812789, -28.286508], "paragraph_keywords": ["view", "graph", "views", "highlighted"]}, {"paragraph_vector": [-178.03894, -28.090332], "paragraph_keywords": ["code", "address", "source", "views"]}, {"paragraph_vector": [-178.681579, -27.551101], "paragraph_keywords": ["automated", "analysis", "limitations", "ccnav"]}, {"paragraph_vector": [-178.04512, -25.277145], "paragraph_keywords": ["participants", "analysis", "facilitator", "analytics"]}, {"paragraph_vector": [-178.202987, -28.12354], "paragraph_keywords": ["loop", "tasks", "code", "source"]}, {"paragraph_vector": [179.560531, -28.399581], "paragraph_keywords": ["loop", "vector", "source", "vectorization"]}, {"paragraph_vector": [-178.802093, -28.43585], "paragraph_keywords": ["instructions", "code", "loop", "view"]}, {"paragraph_vector": [-178.368591, -27.102685], "paragraph_keywords": ["loop", "view", "base", "raja"]}, {"paragraph_vector": [-177.211578, -28.518053], "paragraph_keywords": ["participants", "loop", "compilation", "views"]}, {"paragraph_vector": [-177.577636, -27.051116], "paragraph_keywords": ["loop", "participants", "tasks", "views"]}, {"paragraph_vector": [179.750808, -22.944387], "paragraph_keywords": ["participant", "use", "views", "loop"]}, {"paragraph_vector": [179.473159, -21.240308], "paragraph_keywords": ["design", "cfgexplorer", "experts", "sessions"]}, {"paragraph_vector": [-179.325271, -21.315618], "paragraph_keywords": ["design", "experts", "view", "prototype"]}, {"paragraph_vector": [-178.49057, -15.481968], "paragraph_keywords": ["analysis", "design", "stages", "visualization"]}, {"paragraph_vector": [177.729003, -30.424755], "paragraph_keywords": ["analysis", "study", "experts", "ieee"]}], "content": {}, "doi": "10.1109/TVCG.2020.3030423"}, {"uri": "32", "title": "SEQUENCE BRAIDING: Visual Overviews of Temporal Event Sequences and Attributes", "timestamp": "2020", "rating": "0.0", "annotation": "", "tags": [], "authors": ["Sara Di Bartolomeo", "Yixuan Zhang", "Fangfang Sheng", "Cody Dunne"], "summary": "Temporal event sequence alignment has been used in many domains to visualize nuanced changes and interactions over time. Existing approaches align one or two sentinel events. Overview tasks require examining all alignments of interest using interaction and time or juxtaposition of many visualizations. Furthermore, any event attribute overviews are not closely tied to sequence visualizations. We present SEQUENCE BRAIDING, a novel overview visualization for temporal event sequences and attributes using a layered directed acyclic network. SEQUENCE BRAIDING visually aligns many temporal events and attribute groups simultaneously and supports arbitrary ordering, absence, and duplication of events. In a controlled experiment we compare SEQUENCE BRAIDING and IDMVis on user task completion time, correctness, error, and confidence. Our results provide good evidence that users of SEQUENCE BRAIDING can understand high-level patterns and trends faster and with similar error. A full version of this paper with all appendices; the evaluation stimuli, data, and analysis code; and source code are available at osf.io/mq2wt.", "keywords": ["use", "supersequence", "pattern", "rank", "order", "graph", "type", "node", "attribute", "number", "support", "blood", "participant", "level", "value", "meal", "constraint", "approach", "ordering", "task", "algorithm", "difference", "represent", "visualization", "glucose", "based", "group", "sequence", "data", "diabetes", "braiding", "intersection", "error", "method", "time", "et", "given", "event", "design", "find", "set", "analysis", "reduction", "alignment", "study", "correctness"], "document_vector": [-171.368988, 45.441925], "paragraphs": [{"paragraph_vector": [-65.423538, -32.750328], "paragraph_keywords": ["event", "data", "sequence", "attributes"]}, {"paragraph_vector": [-62.364212, -40.049839], "paragraph_keywords": ["sequence", "braiding", "visualization", "design"]}, {"paragraph_vector": [-50.7579, -13.888586], "paragraph_keywords": ["glucose", "data", "blood", "meal"]}, {"paragraph_vector": [-55.809707, -25.815925], "paragraph_keywords": ["data", "event", "events", "sequence"]}, {"paragraph_vector": [-63.283042, -33.955329], "paragraph_keywords": ["events", "alignment", "layer", "event"]}, {"paragraph_vector": [-63.236713, -38.094177], "paragraph_keywords": ["use", "sequence", "visualizations", "attribute"]}, {"paragraph_vector": [-53.736965, -63.686653], "paragraph_keywords": ["visualization", "event", "data", "problem"]}, {"paragraph_vector": [-61.515811, -37.54916], "paragraph_keywords": ["sequences", "attribute", "events", "event"]}, {"paragraph_vector": [-64.933677, -39.565628], "paragraph_keywords": ["events", "sequences", "rank", "assignment"]}, {"paragraph_vector": [-65.148162, -38.269348], "paragraph_keywords": ["sequences", "alignment", "sequence", "use"]}, {"paragraph_vector": [-64.404518, -38.813602], "paragraph_keywords": ["events", "supersequence", "sequences", "event"]}, {"paragraph_vector": [-58.33559, -60.181148], "paragraph_keywords": ["node", "group", "assigned", "rank"]}, {"paragraph_vector": [74.671775, -72.357551], "paragraph_keywords": ["nodes", "ranks", "rank", "ordering"]}, {"paragraph_vector": [6.604631, -68.809623], "paragraph_keywords": ["solution", "edges", "approach", "ieee"]}, {"paragraph_vector": [-63.612216, -60.979286], "paragraph_keywords": ["nodes", "sequence", "algorithm", "visualization"]}, {"paragraph_vector": [-62.076038, -35.427955], "paragraph_keywords": ["support", "tasks", "sequence", "visualization"]}, {"paragraph_vector": [-61.103507, -33.64495], "paragraph_keywords": ["tasks", "task", "reference", "questions"]}, {"paragraph_vector": [-61.239898, -33.818222], "paragraph_keywords": ["events", "participants", "study", "sequence"]}, {"paragraph_vector": [135.020767, 8.506714], "paragraph_keywords": ["participants", "use", "ieee", "numeracy"]}, {"paragraph_vector": [134.616653, 83.385543], "paragraph_keywords": ["p", "hypothesis", "participant", "evaluation"]}, {"paragraph_vector": [140.764465, -19.822456], "paragraph_keywords": ["time", "test", "effect", "participants"]}, {"paragraph_vector": [143.435226, -15.553645], "paragraph_keywords": ["time", "avg", "error", "ieee"]}, {"paragraph_vector": [146.042709, -22.526939], "paragraph_keywords": ["time", "sequence", "braiding", "task"]}, {"paragraph_vector": [-64.826644, -38.508342], "paragraph_keywords": ["sequence", "braiding", "participants", "patterns"]}, {"paragraph_vector": [-65.910621, -60.029094], "paragraph_keywords": ["sequences", "events", "sequence", "interactivity"]}, {"paragraph_vector": [-64.946662, -37.261638], "paragraph_keywords": ["sequence", "braiding", "use", "visualization"]}], "content": {}, "doi": "10.1109/TVCG.2020.3030365"}, {"uri": "33", "title": "ChemVA: Interactive Visual Analysis of Chemical Compound Similarity in Virtual Screening", "timestamp": "2020", "rating": "0.0", "annotation": "", "tags": [], "authors": ["Mar\u0131\u0301a Virginia Sabando", "Pavol Ulbrich", "Mat\u0131\u0301as Selzer", "Jan By\u0161ka", "Jan Mi\u010dan", "Ignacio Ponzoni", "Axel J. Soto", "Mar\u0131\u0301a Luj\u00e1n Ganuza", "Barbora Kozl\u0131\u0301kov\u00e1"], "summary": "In the modern drug discovery process, medicinal chemists deal with the complexity of analysis of large ensembles of candidate molecules. Computational tools, such as dimensionality reduction (DR) and classification, are commonly used to efficiently process the multidimensional space of features. These underlying calculations often hinder interpretability of results and prevent experts from assessing the impact of individual molecular features on the resulting representations. To provide a solution for scrutinizing such complex data, we introduce ChemVA, an interactive application for the visual exploration of large molecular ensembles and their features. Our tool consists of multiple coordinated views: Hexagonal view, Detail view, 3D view, Table view, and a newly proposed Difference view designed for the comparison of DR projections. These views display DR projections combined with biological activity, selected molecular features, and confidence scores for each of these projections. This conjunction of views allows the user to drill down through the dataset and to efficiently select candidate compounds. Our approach was evaluated on two case studies of finding structurally similar ligands with similar binding affinity to a target protein, as well as on an external qualitative evaluation. The results suggest that our system allows effective visual inspection and comparison of different high-dimensional molecular representations. Furthermore, ChemVA assists in the identification of candidate compounds while providing information on the certainty behind different molecular representations.", "keywords": ["property", "feature", "user", "trustworthiness", "structure", "research", "technique", "detail", "information", "allows", "color", "expert", "space", "process", "similarity", "section", "compound", "dataset", "molecule", "table", "difference", "requirement", "hexagon", "visualization", "chemical", "chemva", "based", "tool", "case", "figure", "dr", "screening", "data", "view", "mean", "representation", "domain", "drug", "t", "sne", "selected", "proposed", "study", "projection"], "document_vector": [-121.873825, -35.304088], "paragraphs": [{"paragraph_vector": [91.025474, -14.757408], "paragraph_keywords": ["compounds", "drug", "screening", "engineering"]}, {"paragraph_vector": [93.803321, -17.466386], "paragraph_keywords": ["screening", "tools", "space", "chemical"]}, {"paragraph_vector": [92.506446, -13.002219], "paragraph_keywords": ["compounds", "allows", "information", "chemva"]}, {"paragraph_vector": [92.890258, -14.868098], "paragraph_keywords": ["visualization", "data", "dr", "proposed"]}, {"paragraph_vector": [1.765574, 3.805354], "paragraph_keywords": ["data", "t", "scatter", "sne"]}, {"paragraph_vector": [91.62635, -14.617614], "paragraph_keywords": ["tools", "visualization", "compounds", "structure"]}, {"paragraph_vector": [92.053573, -15.542293], "paragraph_keywords": ["compounds", "chemical", "tool", "introduced"]}, {"paragraph_vector": [93.039863, -16.499975], "paragraph_keywords": ["representations", "based", "compounds", "provide"]}, {"paragraph_vector": [92.321464, -15.517765], "paragraph_keywords": ["drug", "features", "based", "compounds"]}, {"paragraph_vector": [92.145973, -14.988719], "paragraph_keywords": ["compounds", "dr", "requirements", "group"]}, {"paragraph_vector": [91.383125, -14.39896], "paragraph_keywords": ["compounds", "features", "views", "users"]}, {"paragraph_vector": [92.702064, -13.419168], "paragraph_keywords": ["views", "user", "view", "dr"]}, {"paragraph_vector": [91.912284, -16.793169], "paragraph_keywords": ["view", "compounds", "data", "shape"]}, {"paragraph_vector": [93.949211, -12.978572], "paragraph_keywords": ["view", "detail", "compounds", "views"]}, {"paragraph_vector": [90.988899, -14.19477], "paragraph_keywords": ["view", "projection", "color", "correlation"]}, {"paragraph_vector": [91.083717, -12.66771], "paragraph_keywords": ["table", "view", "compounds", "ieee"]}, {"paragraph_vector": [93.351875, -16.33299], "paragraph_keywords": ["compounds", "atoms", "bonds", "view"]}, {"paragraph_vector": [91.106895, -15.440127], "paragraph_keywords": ["t", "sne", "compounds", "coordinates"]}, {"paragraph_vector": [91.739501, -14.749615], "paragraph_keywords": ["chemva", "models", "views", "compound"]}, {"paragraph_vector": [90.756683, -15.41799], "paragraph_keywords": ["compounds", "receptors", "serotonin", "dopamine"]}, {"paragraph_vector": [91.360221, -14.926647], "paragraph_keywords": ["compounds", "expert", "structures", "domain"]}, {"paragraph_vector": [91.256317, -15.03022], "paragraph_keywords": ["compounds", "view", "find", "use"]}, {"paragraph_vector": [91.746932, -15.14886], "paragraph_keywords": ["experts", "difference", "domain", "chemva"]}, {"paragraph_vector": [91.600723, -14.973467], "paragraph_keywords": ["compounds", "chemva", "domain", "case"]}, {"paragraph_vector": [92.062431, -14.859642], "paragraph_keywords": ["compounds", "case", "view", "ieee"]}, {"paragraph_vector": [91.991744, -14.649504], "paragraph_keywords": ["tool", "chemva", "data", "domain"]}, {"paragraph_vector": [-129.165298, 76.024665], "paragraph_keywords": ["research", "tool", "ieee", "chemva"]}], "content": {}, "doi": "10.1109/TVCG.2020.3030373"}, {"uri": "34", "title": "Embodied Navigation in Immersive Abstract Data Visualization: Is Overview+Detail or Zooming Better for 3D Scatterplots?", "timestamp": "2020", "rating": "0.0", "annotation": "", "tags": [], "authors": ["Yalong Yang", "Maxime Cordeil", "Johanna Beyer", "Tim Dwyer", "Kim Marriott", "Hanspeter Pfister"], "summary": "Abstract data has no natural scale and so interactive data visualizations must provide techniques to allow the user to choose their viewpoint and scale. Such techniques are well established in desktop visualization tools. The two most common techniques are zoom+pan and overview+detail. However, how best to enable the analyst to navigate and view abstract data at different levels of scale in immersive environments has not previously been studied. We report the findings of the first systematic study of immersive navigation techniques for 3D scatterplots. We tested four conditions that represent our best attempt to adapt standard 2D navigation techniques to data visualization in an immersive environment while still providing standard immersive navigation techniques through physical movement and teleportation. We compared room-sized visualization versus a zooming interface, each with and without an overview. We find significant differences in participants\u2019 response times and accuracy for a number of standard visual analysis tasks. Both zoom and overview provide benefits over standard locomotion support alone (i.e., physical movement and pointer teleportation). However, which variation is superior, depends on the task. We obtain a more nuanced understanding of the results by analyzing them in terms of a time-cost model for the different components of navigation: way-finding, travel, number of travel steps, and context switching.", "keywords": ["use", "rm", "found", "rmo", "user", "display", "navigation", "number", "participant", "technique", "teleportation", "al", "environment", "detail", "level", "size", "information", "context", "interaction", "fig", "space", "task", "zooming", "distance", "vr", "movement", "provide", "visualization", "position", "based", "scatterplots", "data", "zoom", "view", "time", "performance", "et", "asked", "zmo", "point", "compared", "overview", "condition", "walking", "cost", "study", "interface"], "document_vector": [71.721664, 1.036174], "paragraphs": [{"paragraph_vector": [115.825904, -22.566165], "paragraph_keywords": ["data", "methods", "-", "navigation"]}, {"paragraph_vector": [120.199523, -10.353995], "paragraph_keywords": ["data", "vr", "navigation", "environments"]}, {"paragraph_vector": [116.908653, -13.624654], "paragraph_keywords": ["overview", "navigation", "data", "zooming"]}, {"paragraph_vector": [112.958602, -17.208942], "paragraph_keywords": ["found", "navigation", "display", "explored"]}, {"paragraph_vector": [116.248298, -3.969859], "paragraph_keywords": ["walking", "teleportation", "allow", "devices"]}, {"paragraph_vector": [125.260398, -6.22467], "paragraph_keywords": ["scatterplots", "scatterplot", "participants", "selecting"]}, {"paragraph_vector": [121.219436, -10.077262], "paragraph_keywords": ["node", "user", "flying", "visualization"]}, {"paragraph_vector": [120.580825, -17.289581], "paragraph_keywords": ["display", "techniques", "overview", "view"]}, {"paragraph_vector": [118.912467, -5.713519], "paragraph_keywords": ["overview", "user", "default", "users"]}, {"paragraph_vector": [117.911895, -10.726692], "paragraph_keywords": ["user", "overview", "position", "view"]}, {"paragraph_vector": [118.612419, -13.119998], "paragraph_keywords": ["position", "zoom", "use", "users"]}, {"paragraph_vector": [118.300292, -10.147347], "paragraph_keywords": ["user", "space", "object", "display"]}, {"paragraph_vector": [118.19931, -6.380754], "paragraph_keywords": ["overview", "participants", "centimeters", "controller"]}, {"paragraph_vector": [123.498741, -1.658596], "paragraph_keywords": ["kept", "executed", "data", "iterations"]}, {"paragraph_vector": [22.316928, 24.19333], "paragraph_keywords": ["tasks", "task", "level", "points"]}, {"paragraph_vector": [114.226982, -3.810197], "paragraph_keywords": ["distance", "pair", "points", "participants"]}, {"paragraph_vector": [115.23558, -1.950949], "paragraph_keywords": ["participants", "points", "count", "task"]}, {"paragraph_vector": [121.346176, 2.619564], "paragraph_keywords": ["participants", "training", "vr", "asked"]}, {"paragraph_vector": [121.200447, 15.387425], "paragraph_keywords": ["participants", "visualization", "study", "asked"]}, {"paragraph_vector": [120.021583, 25.885093], "paragraph_keywords": ["variables", "reported", "task", "evaluate"]}, {"paragraph_vector": [114.885795, -3.226153], "paragraph_keywords": ["participants", "view", "points", "visualization"]}, {"paragraph_vector": [114.164039, -0.022901], "paragraph_keywords": ["overview", "participants", "zmo", "found"]}, {"paragraph_vector": [115.315414, -4.27812], "paragraph_keywords": ["found", "performance", "experience", "tasks"]}, {"paragraph_vector": [115.617347, -3.202022], "paragraph_keywords": ["participants", "found", "rmo", "zm"]}, {"paragraph_vector": [117.2257, 0.405932], "paragraph_keywords": ["participants", "rm", "fig", "visualization"]}, {"paragraph_vector": [114.903388, -3.532085], "paragraph_keywords": ["participants", "time", "rm", "distance"]}, {"paragraph_vector": [116.533699, -6.523505], "paragraph_keywords": ["visualization", "time", "term", "cost"]}, {"paragraph_vector": [115.503242, -2.41403], "paragraph_keywords": ["rm", "targets", "rmo", "participants"]}, {"paragraph_vector": [115.781127, -3.53817], "paragraph_keywords": ["teleportation", "movement", "number", "rmo"]}, {"paragraph_vector": [114.963058, -2.418326], "paragraph_keywords": ["participants", "task", "time", "switching"]}, {"paragraph_vector": [115.627883, -2.420873], "paragraph_keywords": ["tasks", "performance", "navigation", "techniques"]}, {"paragraph_vector": [117.991836, -3.341044], "paragraph_keywords": ["navigation", "overview", "research", "ieee"]}], "content": {}, "doi": "10.1109/TVCG.2020.3030378"}, {"uri": "35", "title": "Selection-Bias-Corrected Visualization via Dynamic Reweighting", "timestamp": "2020", "rating": "0.0", "annotation": "", "tags": [], "authors": ["David Borland", "Jonathan Zhang", "Smiti Kaul", "David Gotz"], "summary": "The collection and visual analysis of large-scale data from complex systems, such as electronic health records or clickstream data, has become increasingly common across a wide range of industries. This type of retrospective visual analysis, however, is prone to a variety of selection bias effects, especially for high-dimensional data where only a subset of dimensions is visualized at any given time. The risk of selection bias is even higher when analysts dynamically apply filters or perform grouping operations during ad hoc analyses. These bias effects threaten the validity and generalizability of insights discovered during visual analysis as the basis for decision making. Past work has focused on bias transparency, helping users understand when selection bias may have occurred. However, countering the effects of selection bias via bias mitigation is typically left for the user to accomplish as a separate process. Dynamic reweighting (DR) is a novel computational approach to selection bias mitigation that helps users craft bias-corrected visualizations. This paper describes the DR workflow, introduces key DR visualization designs, and presents statistical methods that support the DR process. Use cases from the medical domain, as well as findings from domain expert user interviews, are also reported.", "keywords": ["use", "score", "b", "user", "sample", "cadence", "node", "system", "research", "participant", "technique", "correction", "danger", "focus", "value", "information", "dimension", "analytics", "workflow", "process", "selection", "section", "work", "reweighting", "plot", "cohort", "distance", "table", "help", "paper", "defined", "visualization", "based", "figure", "shown", "health", "dr", "bias", "dk", "data", "salient", "reweight", "interview", "method", "weight", "distribution", "applied", "icicle", "weighted", "set", "analysis", "subgroup", "correlation"], "document_vector": [-111.04882, 28.153783], "paragraphs": [{"paragraph_vector": [-59.08052, 0.88899], "paragraph_keywords": ["data", "university", "demographics", "bias"]}, {"paragraph_vector": [-64.208175, 5.438604], "paragraph_keywords": ["bias", "data", "users", "selection"]}, {"paragraph_vector": [-64.556159, 5.919388], "paragraph_keywords": ["bias", "dr", "research", "correction"]}, {"paragraph_vector": [-64.052597, 4.573969], "paragraph_keywords": ["bias", "data", "analysis", "methods"]}, {"paragraph_vector": [-62.852836, 4.173454], "paragraph_keywords": ["hierarchies", "cadence", "icicle", "bias"]}, {"paragraph_vector": [-65.378532, 5.427115], "paragraph_keywords": ["user", "research", "sets", "visualization"]}, {"paragraph_vector": [-64.472694, 4.5154], "paragraph_keywords": ["bias", "correction", "dimensions", "user"]}, {"paragraph_vector": [-63.389198, 9.188633], "paragraph_keywords": ["data", "bias", "selection", "dr"]}, {"paragraph_vector": [-64.910865, 4.966829], "paragraph_keywords": ["dimensions", "visualizations", "bias", "cohort"]}, {"paragraph_vector": [-65.860961, 5.767191], "paragraph_keywords": ["dimensions", "reweighting", "cohort", "score"]}, {"paragraph_vector": [12.926476, -74.631668], "paragraph_keywords": ["nodes", "plot", "icicle", "table"]}, {"paragraph_vector": [-49.532745, -62.540004], "paragraph_keywords": ["dimensions", "shift", "nodes", "distribution"]}, {"paragraph_vector": [-41.383033, -54.774158], "paragraph_keywords": ["dimension", "user", "reweight", "dimensions"]}, {"paragraph_vector": [-65.996742, 6.589246], "paragraph_keywords": ["user", "correlation", "distance", "focus"]}, {"paragraph_vector": [-65.888946, 8.15], "paragraph_keywords": ["distance", "cohort", "focus", "dimension"]}, {"paragraph_vector": [-66.374168, 9.579294], "paragraph_keywords": ["cohort", "dimensions", "reweighting", "reweight"]}, {"paragraph_vector": [-66.734649, 7.445063], "paragraph_keywords": ["cohort", "subgroup", "reweighting", "b"]}, {"paragraph_vector": [-65.800811, 6.631921], "paragraph_keywords": ["b", "dk", "subgroup", "reweighting"]}, {"paragraph_vector": [-68.303665, 8.562878], "paragraph_keywords": ["dk", "d", "values", "subgroup"]}, {"paragraph_vector": [-65.404594, 5.798972], "paragraph_keywords": ["subgroup", "data", "threshold", "correlation"]}, {"paragraph_vector": [-61.375209, 2.898113], "paragraph_keywords": ["cohort", "diagnosis", "dr", "analysis"]}, {"paragraph_vector": [-63.018985, 9.851976], "paragraph_keywords": ["figure", "user", "changes", "cohort"]}, {"paragraph_vector": [-64.137855, 5.514686], "paragraph_keywords": ["participants", "health", "system", "data"]}, {"paragraph_vector": [-64.284309, 5.036961], "paragraph_keywords": ["dimensions", "data", "users", "reweighting"]}, {"paragraph_vector": [-65.490715, 6.361841], "paragraph_keywords": ["danger", "users", "score", "dimensions"]}, {"paragraph_vector": [120.774978, -78.958641], "paragraph_keywords": ["users", "interface", "use", "slider"]}, {"paragraph_vector": [-65.586891, 6.476978], "paragraph_keywords": ["dr", "bias", "analysis", "selection"]}, {"paragraph_vector": [157.564224, 78.23661], "paragraph_keywords": ["ieee", "use", "article", "supported"]}], "content": {}, "doi": "10.1109/TVCG.2020.3030345"}, {"uri": "36", "title": "Data Visceralization: Enabling Deeper Understanding of Data Using Virtual Reality", "timestamp": "2020", "rating": "0.0", "annotation": "", "tags": [], "authors": ["Benjamin Lee", "Dave Brown", "Bongshin Lee", "Christophe Hurter", "Steven Drucker", "Tim Dwyer"], "summary": "A fundamental part of data visualization is transforming data to map abstract information onto visual attributes. While this abstraction is a powerful basis for data visualization, the connection between the representation and the original underlying data (i.e., what the quantities and measurements actually correspond with in reality) can be lost. On the other hand, virtual reality (VR) is being increasingly used to represent real and abstract models as natural experiences to users. In this work, we explore the potential of using VR to help restore the basic understanding of units and measures that are often abstracted away in data visualization in an approach we call data visceralization. By building VR prototypes as design probes, we identify key themes and factors for data visceralization. We do this first through a critical reflection by the authors, then by involving external participants. We find that data visceralization is an engaging way of understanding the qualitative aspects of physical measures and their real-life form, which complements analytical and quantitative understanding commonly gained from data visualization. However, data visceralization is most effective when there is a one-to-one mapping between data and representation, with transformations such as scaling affecting this understanding. We conclude with a discussion of future directions for data visceralization.", "keywords": ["use", "scale", "version", "concept", "reality", "user", "height", "skyscraper", "comparison", "transformation", "model", "presented", "impact", "participant", "technique", "understand", "experience", "measure", "visceralization", "phenomenon", "environment", "perception", "object", "size", "information", "value", "prototype", "level", "sec", "context", "fig", "space", "people", "distance", "annotation", "vr", "help", "provide", "visualization", "understanding", "perspective", "notion", "world", "data", "view", "time", "visceralizations", "desktop", "sense", "track", "design", "example", "story", "speed", "sprinter", "bill"], "document_vector": [78.319282, 31.893575], "paragraphs": [{"paragraph_vector": [-169.08612, 11.082831], "paragraph_keywords": ["data", "visualization", "university", "help"]}, {"paragraph_vector": [139.351272, 18.69783], "paragraph_keywords": ["data", "visceralization", "skyscrapers", "truth"]}, {"paragraph_vector": [158.297515, 15.748961], "paragraph_keywords": ["data", "vr", "work", "visceralization"]}, {"paragraph_vector": [154.589996, 17.978822], "paragraph_keywords": ["data", "units", "use", "measure"]}, {"paragraph_vector": [143.051696, 20.074844], "paragraph_keywords": ["presence", "data", "vr", "use"]}, {"paragraph_vector": [131.387207, 24.66152], "paragraph_keywords": ["data", "vr", "visceralization", "scenarios"]}, {"paragraph_vector": [88.685821, 4.945129], "paragraph_keywords": ["track", "fig", "sprinter", "bolt"]}, {"paragraph_vector": [88.296676, 4.355178], "paragraph_keywords": ["track", "record", "basketball", "use"]}, {"paragraph_vector": [96.065933, 8.79365], "paragraph_keywords": ["fig", "jumpers", "skyscrapers", "use"]}, {"paragraph_vector": [118.225433, 18.932468], "paragraph_keywords": ["people", "fig", "scale", "user"]}, {"paragraph_vector": [118.205734, 18.917037], "paragraph_keywords": ["crowd", "experience", "ieee", "photographs"]}, {"paragraph_vector": [132.68074, 18.31027], "paragraph_keywords": ["bills", "pallets", "fig", "statue"]}, {"paragraph_vector": [96.129486, 5.757301], "paragraph_keywords": ["athletes", "objects", "view", "user"]}, {"paragraph_vector": [122.490394, 20.823421], "paragraph_keywords": ["moving", "data", "understanding", "ieee"]}, {"paragraph_vector": [133.753845, 18.603961], "paragraph_keywords": ["perception", "data", "experience", "need"]}, {"paragraph_vector": [112.367378, 18.242118], "paragraph_keywords": ["skyscrapers", "scaling", "planets", "models"]}, {"paragraph_vector": [118.075538, 25.380821], "paragraph_keywords": ["people", "quantity", "understanding", "ieee"]}, {"paragraph_vector": [144.592361, 18.860162], "paragraph_keywords": ["participants", "version", "data", "visceralization"]}, {"paragraph_vector": [139.811477, 14.178344], "paragraph_keywords": ["participants", "vr", "transcripts", "desktop"]}, {"paragraph_vector": [131.021255, 19.291067], "paragraph_keywords": ["participants", "data", "sec", "agreed"]}, {"paragraph_vector": [134.730606, 19.515588], "paragraph_keywords": ["annotations", "participants", "thought", "ieee"]}, {"paragraph_vector": [131.352691, 22.699893], "paragraph_keywords": ["participants", "felt", "saying", "data"]}, {"paragraph_vector": [139.953399, 23.960344], "paragraph_keywords": ["participants", "bills", "data", "discuss"]}, {"paragraph_vector": [139.063537, 20.438089], "paragraph_keywords": ["data", "understanding", "values", "experience"]}, {"paragraph_vector": [158.138122, 17.243074], "paragraph_keywords": ["user", "participants", "comparisons", "value"]}, {"paragraph_vector": [136.441574, 19.738615], "paragraph_keywords": ["data", "phenomena", "visceralization", "gain"]}, {"paragraph_vector": [151.054672, 18.046407], "paragraph_keywords": ["data", "visceralization", "height", "sensations"]}, {"paragraph_vector": [135.718948, 18.163524], "paragraph_keywords": ["vr", "data", "size", "objects"]}, {"paragraph_vector": [148.973526, 18.824312], "paragraph_keywords": ["data", "development", "visceralization", "phenomena"]}, {"paragraph_vector": [158.13768, 18.703622], "paragraph_keywords": ["data", "ieee", "experiences", "understanding"]}], "content": {}, "doi": "10.1109/TVCG.2020.3030406"}, {"uri": "37", "title": "MobileVisFixer: Tailoring Web Visualizations for Mobile Phones Leveraging an Explainable Reinforcement Learning Framework", "timestamp": "2020", "rating": "0.0", "annotation": "", "tags": [], "authors": ["Aoyu Wu", "Wai Tong", "Tim Dwyer", "Bongshin Lee", "Petra Isenberg", "Huamin Qu"], "summary": "We contribute MobileVisFixer, a new method to make visualizations more mobile-friendly. Although mobile devices have become the primary means of accessing information on the web, many existing visualizations are not optimized for small screens and can lead to a frustrating user experience. Currently, practitioners and researchers have to engage in a tedious and time-consuming process to ensure that their designs scale to screens of different sizes, and existing toolkits and libraries provide little support in diagnosing and repairing issues. To address this challenge, MobileVisFixer automates a mobile-friendly visualization re-design process with a novel reinforcement learning framework. To inform the design of MobileVisFixer, we first collected and analyzed SVG-based visualizations on the web, and identified five common mobile-friendly issues. MobileVisFixer addresses four of these issues on single-view Cartesian visualizations with linear or discrete scales by a Markov Decision Process model that is both generalizable across various visualizations and fully explainable. MobileVisFixer deconstructs charts into declarative formats, and uses a greedy heuristic based on Policy Gradient methods to find solutions to this difficult, multi-criteria optimization problem in reasonable time. In addition, MobileVisFixer can be easily extended with the incorporation of optimization algorithms for data visualizations. Quantitative evaluation on two real-world datasets demonstrates the effectiveness and generalizability of our method.", "keywords": ["property", "scale", "label", "element", "study", "solution", "problem", "text", "learning", "model", "web", "research", "number", "content", "decision", "state", "learned", "-", "value", "size", "agent", "svg", "process", "space", "approach", "work", "layout", "action", "reinforcement", "visualization", "penalty", "existing", "mobilevisfixer", "based", "policy", "solving", "issue", "group", "data", "rule", "device", "chart", "result", "encoding", "related", "mobile", "design", "set", "cost", "optimization"], "document_vector": [90.604331, -4.806599], "paragraphs": [{"paragraph_vector": [151.757202, -32.02074], "paragraph_keywords": ["websites", "visualizations", "mobile", "problems"]}, {"paragraph_vector": [153.888092, -30.644229], "paragraph_keywords": ["visualizations", "rules", "practitioners", "problems"]}, {"paragraph_vector": [162.636184, -26.348543], "paragraph_keywords": ["visualizations", "web", "mobilevisfixer", "based"]}, {"paragraph_vector": [152.471115, -33.506229], "paragraph_keywords": ["visualizations", "research", "work", "design"]}, {"paragraph_vector": [-158.738891, -24.317537], "paragraph_keywords": ["visualizations", "data", "based", "rules"]}, {"paragraph_vector": [159.19873, -25.99439], "paragraph_keywords": ["functions", "learned", "quality", "cost"]}, {"paragraph_vector": [149.624633, -33.645462], "paragraph_keywords": ["visualizations", "problems", "content", "problem"]}, {"paragraph_vector": [146.584396, -31.921669], "paragraph_keywords": ["visualizations", "content", "web", "use"]}, {"paragraph_vector": [144.768051, -31.703224], "paragraph_keywords": ["text", "issues", "visualization", "data"]}, {"paragraph_vector": [154.397048, -30.344985], "paragraph_keywords": ["visualization", "values", "optimization", "minimizes"]}, {"paragraph_vector": [139.670654, -40.23212], "paragraph_keywords": ["g", "mobilevisfixer", "encoding", "properties"]}, {"paragraph_vector": [120.19799, -48.775497], "paragraph_keywords": ["group", "labels", "properties", "vega"]}, {"paragraph_vector": [125.760131, -45.506271], "paragraph_keywords": ["mobilevisfixer", "titles", "dependency", "scale"]}, {"paragraph_vector": [156.032287, -26.240318], "paragraph_keywords": ["process", "mobilevisfixer", "visualizations", "ieee"]}, {"paragraph_vector": [156.638748, -24.283325], "paragraph_keywords": ["states", "issues", "mobilevisfixer", "visualization"]}, {"paragraph_vector": [160.044128, -25.492139], "paragraph_keywords": ["space", "state", "ieee", "action"]}, {"paragraph_vector": [146.377059, -29.935707], "paragraph_keywords": ["cost", "length", "size", "viewport"]}, {"paragraph_vector": [173.36737, -30.499347], "paragraph_keywords": ["policy", "algorithm", "rt", "cost"]}, {"paragraph_vector": [156.903533, -23.792295], "paragraph_keywords": ["mobilevisfixer", "term", "use", "penalty"]}, {"paragraph_vector": [157.978775, -27.218246], "paragraph_keywords": ["rate", "steps", "learning", "training"]}, {"paragraph_vector": [158.768798, -25.752985], "paragraph_keywords": ["mobilevisfixer", "penalty", "dataset", "actions"]}, {"paragraph_vector": [158.405883, -24.709461], "paragraph_keywords": ["agent", "learned", "ieee", "probability"]}, {"paragraph_vector": [158.382324, -24.65751], "paragraph_keywords": ["mobilevisfixer", "states", "rules", "visualizations"]}, {"paragraph_vector": [162.336471, -25.609865], "paragraph_keywords": ["rules", "mobilevisfixer", "model", "based"]}, {"paragraph_vector": [157.878631, -24.62676], "paragraph_keywords": ["visualizations", "mobilevisfixer", "studies", "problem"]}, {"paragraph_vector": [-104.83683, 70.098815], "paragraph_keywords": ["ieee", "republication", "redistribution", "requires"]}], "content": {}, "doi": "10.1109/TVCG.2020.3030460"}, {"uri": "38", "title": "Humane Visual AI: Telling the Stories Behind a Medical Condition", "timestamp": "2020", "rating": "0.0", "annotation": "", "tags": [], "authors": ["Wonyoung So", "Edyta P. Bogucka", "Sanja \u0160\u0107epanovi\u0107", "Sagar Joglekar", "Ke Zhou", "Daniele Quercia"], "summary": "A biological understanding is key for managing medical conditions, yet psychological and social aspects matter too. The main problem is that these two aspects are hard to quantify and inherently difficult to communicate. To quantify psychological aspects, this work mined around half a million Reddit posts in the sub-communities specialised in 14 medical conditions, and it did so with a new deep-learning framework. In so doing, it was able to associate mentions of medical conditions with those of emotions. To then quantify social aspects, this work designed a probabilistic approach that mines open prescription data from the National Health Service in England to compute the prevalence of drug prescriptions, and to relate such a prevalence to census data. To finally visually communicate each medical condition\u2019s biological, psychological, and social aspects through storytelling, we designed a narrative-style layered Martini Glass visualization. In a user study involving 52 participants, after interacting with our visualization, a considerable number of them changed their mind on previously held opinions: 10% gave more importance to the psychological aspects of medical conditions, and 27% were more favourable to the use of social media data in healthcare, suggesting the importance of persuasive elements in interactive visualizations.", "keywords": ["use", "importance", "region", "user", "showing", "model", "doctor", "number", "aspect", "participant", "state", "control", "pain", "expected", "reddit", "value", "information", "patient", "treatment", "symptom", "socio", "prevalence", "part", "people", "healthcare", "mentioned", "mention", "associated", "self", "map", "arthritis", "visualization", "disease", "body", "based", "figure", "tool", "health", "medium", "data", "statement", "dashboard", "asked", "drug", "narrative", "related", "emotion", "condition", "post", "story", "study", "instance", "opinion"], "document_vector": [-157.495025, 12.474251], "paragraphs": [{"paragraph_vector": [-95.113296, 34.180995], "paragraph_keywords": ["aspects", "cambridge", "labs", "disadvantage"]}, {"paragraph_vector": [-96.285247, 36.287754], "paragraph_keywords": ["data", "aspects", "experiences", "ieee"]}, {"paragraph_vector": [-100.269889, 35.477104], "paragraph_keywords": ["visualization", "aspects", "health", "posts"]}, {"paragraph_vector": [-101.582496, 34.303939], "paragraph_keywords": ["data", "health", "story", "visualization"]}, {"paragraph_vector": [-94.796401, 33.199043], "paragraph_keywords": ["symptoms", "conditions", "subreddits", "condition"]}, {"paragraph_vector": [-95.790336, 33.418949], "paragraph_keywords": ["symptoms", "drugs", "instance", "concept"]}, {"paragraph_vector": [-97.987762, 33.805538], "paragraph_keywords": ["emotions", "symptoms", "emotion", "condition"]}, {"paragraph_vector": [-102.031257, 30.013469], "paragraph_keywords": ["body", "people", "words", "prescription"]}, {"paragraph_vector": [-53.628105, 0.899645], "paragraph_keywords": ["visualization", "drugs", "code", "practice"]}, {"paragraph_vector": [-90.486396, 31.22244], "paragraph_keywords": ["dashboard", "experts", "asked", "prevalence"]}, {"paragraph_vector": [-89.880493, 31.207946], "paragraph_keywords": ["symptoms", "tool", "curiosity", "novelty"]}, {"paragraph_vector": [-88.375923, 35.398258], "paragraph_keywords": ["symptoms", "health", "disease", "users"]}, {"paragraph_vector": [-164.083541, 4.949036], "paragraph_keywords": ["visualization", "users", "adopted", "animations"]}, {"paragraph_vector": [-133.428497, 34.594482], "paragraph_keywords": ["user", "symptoms", "model", "driven"]}, {"paragraph_vector": [-98.485916, 35.391525], "paragraph_keywords": ["symptoms", "user", "figure", "data"]}, {"paragraph_vector": [-101.63655, 34.218723], "paragraph_keywords": ["body", "regions", "prevalence", "visualization"]}, {"paragraph_vector": [-98.822288, 35.933143], "paragraph_keywords": ["asked", "visualization", "chart", "method"]}, {"paragraph_vector": [-96.127769, 35.263458], "paragraph_keywords": ["conditions", "statement", "participants", "asked"]}, {"paragraph_vector": [-94.758041, 37.794536], "paragraph_keywords": ["participants", "opinion", "polarized", "f"]}, {"paragraph_vector": [-95.960594, 37.399696], "paragraph_keywords": ["participants", "use", "opinion", "health"]}, {"paragraph_vector": [-97.773933, 35.295818], "paragraph_keywords": ["participants", "switched", "suggested", "importance"]}, {"paragraph_vector": [-98.88668, 33.966457], "paragraph_keywords": ["model", "media", "participants", "use"]}, {"paragraph_vector": [-100.491462, 35.469291], "paragraph_keywords": ["data", "visualization", "media", "ai"]}, {"paragraph_vector": [-101.896995, 36.859714], "paragraph_keywords": ["visualization", "media", "data", "narrative"]}], "content": {}, "doi": "10.1109/TVCG.2020.3030467"}, {"uri": "39", "title": "Visualization of Human Spine Biomechanics for Spinal Surgery", "timestamp": "2020", "rating": "0.0", "annotation": "", "tags": [], "authors": ["Pepe Eulzer", "Sabine Bauer", "Francis Kilian", "Kai Lawonn"], "summary": "We propose a visualization application, designed for the exploration of human spine simulation data. Our goal is to support research in biomechanical spine simulation and advance efforts to implement simulation-backed analysis in surgical applications. Biomechanical simulation is a state-of-the-art technique for analyzing load distributions of spinal structures. Through the inclusion of patient-specific data, such simulations may facilitate personalized treatment and customized surgical interventions. Difficulties in spine modelling and simulation can be partly attributed to poor result representation, which may also be a hindrance when introducing such techniques into a clinical environment. Comparisons of measurements across multiple similar anatomical structures and the integration of temporal data make commonly available diagrams and charts insufficient for an intuitive and systematic display of results. Therefore, we facilitate methods such as multiple coordinated views, abstraction and focus and context to display simulation outcomes in a dedicated tool. By linking the result data with patient-specific anatomy, we make relevant parameters tangible for clinicians. Furthermore, we introduce new concepts to show the directions of impact force vectors, which were not accessible before. We integrated our toolset into a spine segmentation and simulation pipeline and evaluated our methods with both surgeons and biomechanical researchers. When comparing our methods against standard representations that are currently in use, we found increases in accuracy and speed in data exploration tasks. In a qualitative review, domain experts deemed the tool highly useful when dealing with simulation result data, which typically combines time-dependent patient movement and the resulting force distributions on spinal structures.", "keywords": ["use", "exploration", "found", "force", "view", "user", "structure", "model", "participant", "technique", "framework", "value", "color", "expert", "simulated", "vertebra", "task", "simulation", "glyph", "plot", "parameter", "scene", "movement", "direction", "visualization", "displayed", "tool", "shown", "physician", "data", "disc", "method", "time", "spine", "chart", "representation", "result", "domain", "point", "resulting", "researcher", "proposed", "overview", "set", "geometry", "analysis", "instance", "anatomy"], "document_vector": [13.656731, 44.878936], "paragraphs": [{"paragraph_vector": [93.640579, 29.009784], "paragraph_keywords": ["pain", "surgery", "germany", "university"]}, {"paragraph_vector": [92.016044, 28.835424], "paragraph_keywords": ["simulation", "exploration", "ieee", "spine"]}, {"paragraph_vector": [94.937385, 30.930791], "paragraph_keywords": ["vertebrae", "spine", "simulation", "structure"]}, {"paragraph_vector": [90.40599, 29.608329], "paragraph_keywords": ["data", "brushing", "simulation", "views"]}, {"paragraph_vector": [89.131568, 28.201063], "paragraph_keywords": ["data", "glyphs", "al", "et"]}, {"paragraph_vector": [90.01934, 29.991886], "paragraph_keywords": ["data", "domain", "techniques", "methods"]}, {"paragraph_vector": [93.383445, 30.036596], "paragraph_keywords": ["simulation", "spine", "line", "charts"]}, {"paragraph_vector": [92.576858, 30.74419], "paragraph_keywords": ["simulation", "simulated", "researchers", "ieee"]}, {"paragraph_vector": [92.08097, 29.532732], "paragraph_keywords": ["data", "forces", "simulation", "compare"]}, {"paragraph_vector": [88.235031, 32.543617], "paragraph_keywords": ["data", "visualization", "simulation", "framework"]}, {"paragraph_vector": [93.913574, 30.420576], "paragraph_keywords": ["use", "simulation", "system", "spine"]}, {"paragraph_vector": [90.886054, 28.118444], "paragraph_keywords": ["charts", "data", "plots", "option"]}, {"paragraph_vector": [91.235504, 29.898941], "paragraph_keywords": ["use", "values", "data", "value"]}, {"paragraph_vector": [91.836433, 26.143699], "paragraph_keywords": ["time", "animation", "data", "symmetry"]}, {"paragraph_vector": [88.555709, 27.43205], "paragraph_keywords": ["value", "direction", "force", "area"]}, {"paragraph_vector": [91.906143, 26.913339], "paragraph_keywords": ["force", "disc", "glyphs", "orientation"]}, {"paragraph_vector": [89.605018, 29.052242], "paragraph_keywords": ["simulation", "use", "tool", "participants"]}, {"paragraph_vector": [89.767967, 28.529008], "paragraph_keywords": ["participants", "data", "asked", "assessments"]}, {"paragraph_vector": [90.366844, 30.319475], "paragraph_keywords": ["participants", "results", "experts", "tool"]}, {"paragraph_vector": [91.745719, 31.333116], "paragraph_keywords": ["data", "representation", "physicians", "ieee"]}, {"paragraph_vector": [90.506126, 28.094034], "paragraph_keywords": ["time", "experts", "direction", "forces"]}, {"paragraph_vector": [91.652442, 29.884351], "paragraph_keywords": ["data", "simulation", "experts", "encoding"]}, {"paragraph_vector": [91.187332, 27.510871], "paragraph_keywords": ["direction", "force", "surface", "isolines"]}, {"paragraph_vector": [90.917327, 29.599983], "paragraph_keywords": ["simulation", "data", "spine", "color"]}, {"paragraph_vector": [90.442237, 29.756858], "paragraph_keywords": ["data", "simulation", "visualization", "adapt"]}, {"paragraph_vector": [90.057014, 31.009611], "paragraph_keywords": ["data", "forces", "ieee", "spine"]}], "content": {}, "doi": "10.1109/TVCG.2020.3030424"}, {"uri": "40", "title": "A Visual Analytics Approach for Ecosystem Dynamics based on Empirical Dynamic Modeling", "timestamp": "2020", "rating": "0.0", "annotation": "", "tags": [], "authors": ["Hiroaki Natsukawa", "Ethan R. Deyle", "Gerald M. Pao", "Koji Koyamada", "George Sugihara"], "summary": "An important approach for scientific inquiry across many disciplines involves using observational time series data to understand the relationships between key variables to gain mechanistic insights into the underlying rules that govern the given system. In real systems, such as those found in ecology, the relationships between time series variables are generally not static; instead, these relationships are dynamical and change in a nonlinear or state-dependent manner. To further understand such systems, we investigate integrating methods that appropriately characterize these dynamics (i.e., methods that measure interactions as they change with time-varying system states) with visualization techniques that can help analyze the behavior of the system. Here, we focus on empirical dynamic modeling (EDM) as a state-of-the-art method that specifically identifies causal variables and measures changing state-dependent relationships between time series variables. Instead of using approaches centered on parametric equations, EDM is an equation-free approach that studies systems based on their dynamic attractors. We propose a visual analytics system to support the identification and mechanistic interpretation of system states using an EDM-constructed dynamic graph. This work, as detailed in four analysis tasks and demonstrated with a GUI, provides a novel synthesis of EDM and visualization techniques such as brush-link visualization and visual summarization to interpret dynamic graphs representing ecosystem dynamics. We applied our proposed system to ecological simulation data and real data from a marine mesocosm study as two key use cases. Our case studies show that our visual analytics tools support the identification and interpretation of the system state by the user, and enable us to discover both confirmatory and new findings in ecosystem dynamics. Overall, we demonstrated that our system can facilitate an understanding of how systems function beyond the intuitive analysis of high-dimensional information based on specific domain knowledge.", "keywords": ["use", "feature", "user", "graph", "node", "transition", "system", "number", "dynamic", "state", "network", "series", "snapshot", "dimension", "analytics", "interaction", "approach", "space", "competition", "plot", "ecosystem", "visualization", "edge", "based", "figure", "shown", "tool", "attractor", "data", "variable", "relationship", "view", "method", "time", "result", "t", "point", "selected", "proposed", "mapping", "analysis", "reduction", "study", "edm"], "document_vector": [-127.9607, 35.350349], "paragraphs": [{"paragraph_vector": [-25.651546, -28.056514], "paragraph_keywords": ["university", "state", "relationships", "changes"]}, {"paragraph_vector": [-25.743314, -28.955034], "paragraph_keywords": ["time", "edm", "data", "relationships"]}, {"paragraph_vector": [-36.322383, -33.35923], "paragraph_keywords": ["system", "graph", "time", "analysis"]}, {"paragraph_vector": [-36.987781, -37.838531], "paragraph_keywords": ["networks", "method", "analytics", "et"]}, {"paragraph_vector": [-34.328598, -35.047363], "paragraph_keywords": ["analytics", "analysis", "method", "ccm"]}, {"paragraph_vector": [-26.495578, -27.787317], "paragraph_keywords": ["system", "edm", "variables", "state"]}, {"paragraph_vector": [-27.077327, -27.608503], "paragraph_keywords": ["system", "attractor", "variables", "time"]}, {"paragraph_vector": [-29.177394, -27.293056], "paragraph_keywords": ["results", "edm", "analytics", "system"]}, {"paragraph_vector": [-28.67403, -27.269451], "paragraph_keywords": ["variables", "edm", "data", "figure"]}, {"paragraph_vector": [-28.097486, -25.251893], "paragraph_keywords": ["calculated", "map", "s", "parameter"]}, {"paragraph_vector": [-26.959199, -28.047519], "paragraph_keywords": ["network", "algorithms", "snapshots", "sne"]}, {"paragraph_vector": [-30.226724, -29.168504], "paragraph_keywords": ["network", "time", "plot", "states"]}, {"paragraph_vector": [-28.666824, -27.610202], "paragraph_keywords": ["time", "color", "snapshot", "displayed"]}, {"paragraph_vector": [-29.143873, -29.211118], "paragraph_keywords": ["use", "user", "view", "users"]}, {"paragraph_vector": [-28.025342, -28.10794], "paragraph_keywords": ["users", "point", "plot", "time"]}, {"paragraph_vector": [-19.62281, -35.783569], "paragraph_keywords": ["time", "stgs", "selected", "stg"]}, {"paragraph_vector": [-30.969266, -28.977191], "paragraph_keywords": ["figure", "food", "results", "ecosystem"]}, {"paragraph_vector": [-24.038604, -26.390707], "paragraph_keywords": ["competition", "states", "figure", "results"]}, {"paragraph_vector": [-25.742218, -28.488845], "paragraph_keywords": ["state", "group", "species", "figure"]}, {"paragraph_vector": [-25.978595, -28.660037], "paragraph_keywords": ["rotifers", "interaction", "use", "nanoflagellates"]}, {"paragraph_vector": [-27.824506, -28.945697], "paragraph_keywords": ["states", "figure", "simulation", "groups"]}, {"paragraph_vector": [-26.798658, -27.977552], "paragraph_keywords": ["analysis", "pico", "dynamics", "ecosystem"]}, {"paragraph_vector": [-27.383583, -28.081766], "paragraph_keywords": ["system", "features", "use", "results"]}, {"paragraph_vector": [-29.843004, -31.260334], "paragraph_keywords": ["stgs", "algorithms", "interactions", "representations"]}, {"paragraph_vector": [-39.1497, -33.798549], "paragraph_keywords": ["networks", "analysis", "calculations", "need"]}, {"paragraph_vector": [-29.632976, -29.854728], "paragraph_keywords": ["grant", "program", "system", "use"]}], "content": {}, "doi": "10.1109/TVCG.2020.3030439"}, {"uri": "41", "title": "DECE: Decision Explorer with Counterfactual Explanations for Machine Learning Models", "timestamp": "2020", "rating": "0.0", "annotation": "", "tags": [], "authors": ["Furui Cheng", "Yao Ming", "Huamin Qu"], "summary": "With machine learning models being increasingly applied to various decision-making scenarios, people have spent growing efforts to make machine learning models more transparent and explainable. Among various explanation techniques, counterfactual explanations have the advantages of being human-friendly and actionable\u2014a counterfactual explanation tells the user how to gain the desired prediction with minimal changes to the input. Besides, counterfactual explanations can also serve as efficient probes to the models\u2019 decisions. In this work, we exploit the potential of counterfactual explanations to understand and explore the behavior of machine learning models. We design DECE, an interactive visualization system that helps understand and explore a model\u2019s decisions on individual instances and data subsets, supporting users ranging from decision-subjects to model developers. DECE supports exploratory analysis of model decisions by combining the strengths of counterfactual explanations at instanceand subgroup-levels. We also introduce a set of interactions that enable users to customize the generation of counterfactual explanations to find more actionable ones that can suit their needs. Through three use cases and an expert interview, we demonstrate the effectiveness of DECE in supporting decision exploration tasks and instance explanations.", "keywords": ["use", "range", "feature", "user", "machine", "explanation", "f", "model", "learning", "system", "research", "knowledge", "credit", "support", "decision", "understand", "level", "value", "patient", "information", "constraint", "cf", "fig", "work", "refine", "distance", "dataset", "bmi", "table", "help", "visualization", "based", "gender", "r", "group", "data", "diabetes", "change", "view", "prediction", "hypothesis", "design", "find", "set", "counterfactuals", "example", "analysis", "instance", "subgroup", "dece"], "document_vector": [-16.8059, 13.814581], "paragraphs": [{"paragraph_vector": [-34.505607, 29.984113], "paragraph_keywords": ["decisions", "ieee", "models", "ml"]}, {"paragraph_vector": [-36.521209, 34.00756], "paragraph_keywords": ["explanations", "model", "decision", "loan"]}, {"paragraph_vector": [-40.673496, 20.673], "paragraph_keywords": ["model", "users", "data", "decision"]}, {"paragraph_vector": [-36.554374, 29.368234], "paragraph_keywords": ["explanations", "generate", "designed", "solving"]}, {"paragraph_vector": [-154.981369, 10.759995], "paragraph_keywords": ["networks", "cnn", "visualization", "work"]}, {"paragraph_vector": [-36.831642, 30.779624], "paragraph_keywords": ["user", "explanations", "counterfactuals", "model"]}, {"paragraph_vector": [-36.430114, 26.762063], "paragraph_keywords": ["subgroup", "users", "level", "model"]}, {"paragraph_vector": [-39.313461, 29.502748], "paragraph_keywords": ["examples", "model", "subgroup", "instance"]}, {"paragraph_vector": [-23.072029, 27.57151], "paragraph_keywords": ["examples", "cf", "loss", "prediction"]}, {"paragraph_vector": [-15.465283, 23.096954], "paragraph_keywords": ["feature", "values", "gradient", "k"]}, {"paragraph_vector": [-32.575386, 21.085201], "paragraph_keywords": ["feature", "cf", "value", "examples"]}, {"paragraph_vector": [-37.309459, 24.464721], "paragraph_keywords": ["hypothesis", "bmi", "prediction", "subgroup"]}, {"paragraph_vector": [-33.335464, 27.280298], "paragraph_keywords": ["feature", "examples", "cf", "counterfactuals"]}, {"paragraph_vector": [-38.457752, 24.702445], "paragraph_keywords": ["view", "users", "cf", "instance"]}, {"paragraph_vector": [-36.850444, 25.431814], "paragraph_keywords": ["use", "counterfactuals", "subgroup", "instances"]}, {"paragraph_vector": [-38.644622, 23.838268], "paragraph_keywords": ["instances", "cf", "subgroup", "flow"]}, {"paragraph_vector": [-45.58699, 21.209121], "paragraph_keywords": ["table", "information", "ieee", "users"]}, {"paragraph_vector": [-36.10799, 22.944694], "paragraph_keywords": ["users", "subgroup", "bar", "feature"]}, {"paragraph_vector": [-36.509155, 25.992315], "paragraph_keywords": ["instance", "cf", "users", "feature"]}, {"paragraph_vector": [-35.730873, 23.789596], "paragraph_keywords": ["users", "examples", "diabetes", "feature"]}, {"paragraph_vector": [-38.396068, 20.67873], "paragraph_keywords": ["subgroup", "bmi", "hypothesis", "fig"]}, {"paragraph_vector": [-35.840461, 18.261611], "paragraph_keywords": ["subgroup", "examples", "cf", "bmi"]}, {"paragraph_vector": [-37.073959, 25.909273], "paragraph_keywords": ["model", "blood", "dataset", "pressure"]}, {"paragraph_vector": [-39.675205, 23.197849], "paragraph_keywords": ["credit", "gender", "examples", "duration"]}, {"paragraph_vector": [-39.216949, 24.714494], "paragraph_keywords": ["subgroup", "credit", "examples", "cf"]}, {"paragraph_vector": [-41.129386, 24.588886], "paragraph_keywords": ["model", "credit", "dataset", "profile"]}, {"paragraph_vector": [-41.823818, 19.985588], "paragraph_keywords": ["examples", "gre", "ieee", "score"]}, {"paragraph_vector": [-47.514366, 14.489814], "paragraph_keywords": ["patients", "cells", "view", "use"]}, {"paragraph_vector": [-38.28778, 20.539728], "paragraph_keywords": ["patients", "dece", "help", "users"]}, {"paragraph_vector": [-39.681617, 23.469322], "paragraph_keywords": ["subgroup", "users", "data", "dece"]}, {"paragraph_vector": [-43.983108, 16.849163], "paragraph_keywords": ["ieee", "analysis", "level", "users"]}], "content": {}, "doi": "10.1109/TVCG.2020.3030371"}, {"uri": "42", "title": "NL4DV: A Toolkit for Generating Analytic Specifications for Data Visualization from Natural Language Queries", "timestamp": "2020", "rating": "0.0", "annotation": "", "tags": [], "authors": ["Arpit Narechania", "Arjun Srinivasan", "John Stasko"], "summary": "Natural language interfaces (NLIs) have shown great promise for visual data analysis, allowing people to flexibly specify and interact with visualizations. However, developing visualization NLIs remains a challenging task, requiring low-level implementation of natural language processing (NLP) techniques as well as knowledge of visual analytic tasks and visualization design. We present NL4DV, a toolkit for natural language-driven data visualization. NL4DV is a Python package that takes as input a tabular dataset and a natural language query about that dataset. In response, the toolkit returns an analytic specification modeled as a JSON object containing data attributes, analytic tasks, and a list of Vega-Lite specifications relevant to the input query. In doing so, NL4DV aids visualization developers who may not have a background in NLP, enabling them to create new visualization NLIs or incorporate natural language input within their existing systems. We demonstrate NL4DV\u2019s usage and capabilities through four examples: 1) rendering visualizations using natural language in a Jupyter notebook, 2) developing a NLI to specify and edit Vega-Lite charts, 3) recreating data ambiguity widgets from the DataTone system, and 4) incorporating speech input to create a multimodal visualization system.", "keywords": ["use", "user", "us", "type", "system", "attribute", "create", "support", "technique", "vega", "interpretation", "level", "capability", "toolkits", "value", "code", "development", "query", "taskmap", "nl", "nlis", "task", "developer", "output", "dataset", "rating", "toolkit", "ambiguity", "vislist", "input", "visualization", "dependency", "based", "figure", "data", "chart", "response", "line", "design", "budget", "example", "interface", "parsing", "inferred", "specification"], "document_vector": [130.998779, -12.15843], "paragraphs": [{"paragraph_vector": [-171.231689, -50.088268], "paragraph_keywords": ["visualization", "nlis", "implementing", "data"]}, {"paragraph_vector": [-155.142868, -49.999984], "paragraph_keywords": ["query", "visualization", "type", "attribute"]}, {"paragraph_vector": [-173.919967, -53.143081], "paragraph_keywords": ["visualization", "attributes", "visualizations", "nl"]}, {"paragraph_vector": [-172.745544, -52.312282], "paragraph_keywords": ["data", "capabilities", "visualization", "people"]}, {"paragraph_vector": [-169.391128, -49.815288], "paragraph_keywords": ["nl", "toolkits", "development", "visualizations"]}, {"paragraph_vector": [-177.029373, -51.896705], "paragraph_keywords": ["visualization", "developers", "visualizations", "use"]}, {"paragraph_vector": [-173.751754, -54.122585], "paragraph_keywords": ["visualization", "developers", "specifications", "nl"]}, {"paragraph_vector": [-176.630264, -53.340854], "paragraph_keywords": ["output", "attributes", "tasks", "developers"]}, {"paragraph_vector": [-138.738098, -53.520946], "paragraph_keywords": ["dataset", "use", "attributes", "functions"]}, {"paragraph_vector": [-172.615661, -53.072574], "paragraph_keywords": ["query", "attribute", "attributes", "values"]}, {"paragraph_vector": [-155.942367, -44.074729], "paragraph_keywords": ["query", "grams", "attributes", "n"]}, {"paragraph_vector": [-146.821456, -36.115135], "paragraph_keywords": ["tasks", "attribute", "attributes", "n"]}, {"paragraph_vector": [-173.632644, -53.311237], "paragraph_keywords": ["task", "query", "visualization", "tasks"]}, {"paragraph_vector": [-155.448867, -41.726467], "paragraph_keywords": ["task", "budget", "rating", "attributes"]}, {"paragraph_vector": [-172.344406, -51.815254], "paragraph_keywords": ["attributes", "visualization", "bar", "visualizations"]}, {"paragraph_vector": [-174.687042, -53.671825], "paragraph_keywords": ["visualization", "query", "attributes", "tasks"]}, {"paragraph_vector": [-179.037048, -52.373977], "paragraph_keywords": ["visualization", "visualizations", "inferred", "python"]}, {"paragraph_vector": [-176.145401, -52.91896], "paragraph_keywords": ["chart", "vega", "users", "visualization"]}, {"paragraph_vector": [-177.250961, -54.610935], "paragraph_keywords": ["query", "vega", "widgets", "ambiguity"]}, {"paragraph_vector": [148.203323, -25.067914], "paragraph_keywords": ["speech", "interactions", "touchplot", "touch"]}, {"paragraph_vector": [-179.261825, -52.599567], "paragraph_keywords": ["query", "code", "queries", "lines"]}, {"paragraph_vector": [178.725509, -56.362453], "paragraph_keywords": ["use", "toolkit", "query", "visualization"]}, {"paragraph_vector": [-177.675735, -53.603012], "paragraph_keywords": ["query", "context", "queries", "type"]}, {"paragraph_vector": [-151.048202, -39.196075], "paragraph_keywords": ["query", "data", "visualization", "task"]}, {"paragraph_vector": [-168.964447, -49.033454], "paragraph_keywords": ["developers", "attribute", "query", "visualization"]}, {"paragraph_vector": [-171.132522, -56.664627], "paragraph_keywords": ["visualization", "use", "nl", "nlis"]}], "content": {}, "doi": "10.1109/TVCG.2020.3030448"}, {"uri": "43", "title": "Gemini: A Grammar and Recommender System for Animated Transitions in Statistical Graphics", "timestamp": "2020", "rating": "0.0", "annotation": "", "tags": [], "authors": ["Younghoon Kim", "Jeffrey Heer"], "summary": "Animated transitions help viewers follow changes between related visualizations. Specifying effective animations demands significant effort: authors must select the elements and properties to animate, provide transition parameters, and coordinate the timing of stages. To facilitate this process, we present Gemini, a declarative grammar and recommendation system for animated transitions between single-view statistical graphics. Gemini specifications define transition \u201csteps\u201d in terms of high-level visual components (marks, axes, legends) and composition rules to synchronize and concatenate steps. With this grammar, Gemini can recommend animation designs to augment and accelerate designers\u2019 work. Gemini enumerates staged animation designs for given start and end states, and ranks those designs using a cost function informed by prior perceptual studies. To evaluate Gemini, we conduct both a formative study on Mechanical Turk to assess and tune our ranking function, and a summative study in which 8 experienced visualization developers implement animations in D3 that we then compare to Gemini\u2019s suggestions. We find that most designs (9/11) are exactly replicable in Gemini, with many (8/11) achievable via edits to suggestions, and that Gemini suggestions avoid multiple participant errors.", "keywords": ["use", "scale", "y", "element", "user", "us", "animation", "transition", "system", "end", "support", "component", "participant", "state", "function", "level", "value", "timing", "specification", "fig", "approach", "people", "mark", "grammar", "visualization", "tool", "axis", "data", "stage", "change", "view", "time", "point", "duration", "given", "gemini", "design", "start", "example", "cost", "step", "recommendation", "animated"], "document_vector": [116.919609, -36.737396], "paragraphs": [{"paragraph_vector": [-119.191711, -80.515327], "paragraph_keywords": ["transition", "transitions", "animation", "animated"]}, {"paragraph_vector": [-122.918502, -80.696304], "paragraph_keywords": ["gemini", "user", "transitions", "designs"]}, {"paragraph_vector": [142.244949, -80.249107], "paragraph_keywords": ["animation", "transitions", "found", "object"]}, {"paragraph_vector": [-67.73278, -64.223258], "paragraph_keywords": ["event", "events", "level", "users"]}, {"paragraph_vector": [-149.852066, -80.230331], "paragraph_keywords": ["designs", "visualization", "data", "use"]}, {"paragraph_vector": [-116.255661, -80.676315], "paragraph_keywords": ["transition", "asked", "animated", "uses"]}, {"paragraph_vector": [-149.303955, -79.833969], "paragraph_keywords": ["visualization", "transitions", "animation", "specify"]}, {"paragraph_vector": [-112.189666, -80.608161], "paragraph_keywords": ["end", "states", "animation", "state"]}, {"paragraph_vector": [-102.267547, -82.950172], "paragraph_keywords": ["component", "mark", "location", "timing"]}, {"paragraph_vector": [147.377563, -74.057144], "paragraph_keywords": ["data", "gemini", "fields", "changes"]}, {"paragraph_vector": [-69.527519, -89.286537], "paragraph_keywords": ["changes", "data", "change", "scales"]}, {"paragraph_vector": [172.096405, -65.005729], "paragraph_keywords": ["staggering", "overlap", "elements", "element"]}, {"paragraph_vector": [-2.386424, -25.65769], "paragraph_keywords": ["step", "data", "enumerator", "concat"]}, {"paragraph_vector": [-145.148788, -74.686454], "paragraph_keywords": ["autoscaleorder", "scale", "data", "concat"]}, {"paragraph_vector": [-164.302551, -81.093185], "paragraph_keywords": ["gemini", "changes", "scale", "component"]}, {"paragraph_vector": [-144.645828, -82.458129], "paragraph_keywords": ["gemini", "sequences", "data", "stage"]}, {"paragraph_vector": [155.321685, -87.223999], "paragraph_keywords": ["cost", "capacity", "changes", "function"]}, {"paragraph_vector": [-144.7639, -81.184486], "paragraph_keywords": ["designs", "transition", "scale", "participants"]}, {"paragraph_vector": [148.869537, -66.570716], "paragraph_keywords": ["data", "use", "scales", "bars"]}, {"paragraph_vector": [155.734527, -81.437393], "paragraph_keywords": ["user", "tool", "stage", "express"]}, {"paragraph_vector": [-86.048027, -81.676452], "paragraph_keywords": ["participants", "subjects", "visualization", "animation"]}, {"paragraph_vector": [146.991241, -85.94728], "paragraph_keywords": ["gemini", "y", "recommendations", "steps"]}, {"paragraph_vector": [-129.943511, -82.998252], "paragraph_keywords": ["gemini", "authoring", "recommendations", "time"]}, {"paragraph_vector": [-135.890686, -82.312362], "paragraph_keywords": ["states", "visualization", "gemini", "points"]}, {"paragraph_vector": [-129.559265, -82.717773], "paragraph_keywords": ["data", "support", "timings", "gemini"]}, {"paragraph_vector": [-106.137802, 70.041854], "paragraph_keywords": ["republication", "redistribution", "requires", "ieee"]}], "content": {}, "doi": "10.1109/TVCG.2020.3030429"}, {"uri": "44", "title": "Communicative Visualizations as a Learning Problem", "timestamp": "2020", "rating": "0.0", "annotation": "", "tags": [], "authors": ["Eytan Adar", "Elsie Lee"], "summary": "Significant research has provided robust task and evaluation languages for the analysis of exploratory visualizations. Unfortunately, these taxonomies fail when applied to communicative visualizations. Instead, designers often resort to evaluating communicative visualizations from the cognitive efficiency perspective: \u201ccan the recipient accurately decode my message/insight?\u201d However, designers are unlikely to be satisfied if the message went \u2018in one ear and out the other.\u2019 The consequence of this inconsistency is that it is difficult to design or select between competing options in a principled way. The problem we address is the fundamental mismatch between how designers want to describe their intent, and the language they have. We argue that visualization designers can address this limitation through a learning lens: that the recipient is a student and the designer a teacher. By using learning objectives, designers can better define, assess, and compare communicative visualizations. We illustrate how the learning-based approach provides a framework for understanding a wide array of communicative goals. To understand how the framework can be applied (and its limitations), we surveyed and interviewed members of the Data Visualization Society using their own visualizations as a probe. Through this study we identified the broad range of objectives in communicative visualizations and the prevalence of certain objective types.", "keywords": ["use", "goal", "way", "insight", "intent", "learning", "model", "assessment", "research", "create", "participant", "understand", "framework", "category", "focus", "level", "information", "think", "context", "process", "question", "people", "work", "help", "need", "visualization", "taxonomy", "based", "figure", "tool", "describe", "recall", "want", "data", "evaluation", "interview", "idea", "viewer", "time", "change", "requires", "response", "given", "communication", "design", "survey", "objective", "designer", "example", "knowledge"], "document_vector": [52.62154, 16.347738], "paragraphs": [{"paragraph_vector": [-174.765182, 29.657821], "paragraph_keywords": ["visualization", "use", "context", "research"]}, {"paragraph_vector": [-171.969268, 30.729997], "paragraph_keywords": ["evaluation", "visualization", "tools", "objectives"]}, {"paragraph_vector": [-169.999923, 30.000463], "paragraph_keywords": ["learning", "visualization", "want", "readers"]}, {"paragraph_vector": [-171.687026, 32.038619], "paragraph_keywords": ["use", "visualization", "visualizations", "graphics"]}, {"paragraph_vector": [-163.03569, 37.935184], "paragraph_keywords": ["level", "meaning", "literacy", "use"]}, {"paragraph_vector": [-168.414413, 32.67633], "paragraph_keywords": ["model", "information", "knowledge", "bertin"]}, {"paragraph_vector": [-168.730117, 35.672668], "paragraph_keywords": ["viewer", "verb", "examples", "curve"]}, {"paragraph_vector": [-167.575714, 30.151739], "paragraph_keywords": ["objective", "figure", "requires", "visualization"]}, {"paragraph_vector": [-166.535858, 31.462207], "paragraph_keywords": ["insights", "viewer", "example", "insight"]}, {"paragraph_vector": [-172.788833, 28.916566], "paragraph_keywords": ["insight", "knowledge", "viewer", "recall"]}, {"paragraph_vector": [-140.390716, 78.240058], "paragraph_keywords": ["viewer", "visualizations", "knowledge", "learning"]}, {"paragraph_vector": [179.868331, 27.779134], "paragraph_keywords": ["objectives", "visualization", "survey", "create"]}, {"paragraph_vector": [-170.653366, 37.658615], "paragraph_keywords": ["objectives", "learning", "objective", "participants"]}, {"paragraph_vector": [-178.36911, 51.099594], "paragraph_keywords": ["interviews", "ieee", "category", "objectives"]}, {"paragraph_vector": [179.616012, 23.392625], "paragraph_keywords": ["participants", "process", "goals", "design"]}, {"paragraph_vector": [-174.463287, 32.782852], "paragraph_keywords": ["think", "objectives", "want", "learning"]}, {"paragraph_vector": [-169.981933, 36.462619], "paragraph_keywords": ["objectives", "learning", "participants", "use"]}, {"paragraph_vector": [-170.306411, 30.241296], "paragraph_keywords": ["objectives", "learning", "visualization", "design"]}, {"paragraph_vector": [-172.274108, 31.489597], "paragraph_keywords": ["objectives", "visualization", "viewers", "response"]}, {"paragraph_vector": [170.713119, 29.578166], "paragraph_keywords": ["objective", "objectives", "taxonomy", "deaths"]}, {"paragraph_vector": [-170.324844, 28.592256], "paragraph_keywords": ["objectives", "visualizations", "visualization", "recall"]}, {"paragraph_vector": [-170.794967, 29.855157], "paragraph_keywords": ["objectives", "learning", "visualization", "expertise"]}, {"paragraph_vector": [-167.448486, 30.962947], "paragraph_keywords": ["viewer", "objective", "visualization", "objectives"]}, {"paragraph_vector": [-169.851547, 29.619873], "paragraph_keywords": ["assessment", "visualization", "designer", "visualizations"]}, {"paragraph_vector": [-171.887557, 31.619358], "paragraph_keywords": ["objectives", "learning", "designer", "work"]}], "content": {}, "doi": "10.1109/TVCG.2020.3030400"}, {"uri": "45", "title": "A Design Space of Vision Science Methods for Visualization Research", "timestamp": "2020", "rating": "0.0", "annotation": "", "tags": [], "authors": ["Madison A. Elliott", "Christine Nothelfer", "Cindy Xiong", "Danielle Albers Szafir"], "summary": "A growing number of efforts aim to understand what people see when using a visualization. These efforts provide scientific grounding to complement design intuitions, leading to more effective visualization practice. However, published visualization research currently reflects a limited set of available methods for understanding how people process visualized data. Alternative methods from vision science offer a rich suite of tools for understanding visualizations, but no curated collection of these methods exists in either perception or visualization research. We introduce a design space of experimental methods for empirically investigating the perceptual processes involved with viewing data visualizations to ultimately inform visualization design guidelines. This paper provides a shared lexicon for facilitating experimental visualization research. We discuss popular experimental paradigms, adjustment types, response types, and dependent measures used in vision science research, rooting each in visualization examples. We then discuss the advantages and limitations of each technique. Researchers can use this design space to create innovative studies and progress scientific understanding of design choices and evaluations in visualization. We highlight a history of collaborative success between visualization and vision science research and advocate for a deeper relationship between the two fields that can elaborate on and extend the methodological design space for understanding visualization and vision.", "keywords": ["use", "trial", "type", "search", "display", "research", "number", "technique", "memory", "measure", "control", "understand", "level", "perception", "value", "information", "accuracy", "identify", "color", "stimulus", "space", "people", "paradigm", "work", "target", "task", "modeling", "provide", "visualization", "understanding", "bias", "scatterplot", "data", "adjustment", "change", "method", "viewer", "time", "detection", "response", "performance", "limitation", "researcher", "vision", "design", "science", "screen", "set", "attention", "example", "study", "experiment", "correlation"], "document_vector": [45.290245, 26.685907], "paragraphs": [{"paragraph_vector": [139.093795, 42.344676], "paragraph_keywords": ["visualization", "psychology", "university", "knowledge"]}, {"paragraph_vector": [143.971313, 40.477138], "paragraph_keywords": ["visualization", "methods", "design", "vision"]}, {"paragraph_vector": [140.182891, 40.770069], "paragraph_keywords": ["methods", "visualization", "understanding", "studies"]}, {"paragraph_vector": [131.908172, 42.503547], "paragraph_keywords": ["correlation", "design", "visualization", "studies"]}, {"paragraph_vector": [139.915802, 42.32312], "paragraph_keywords": ["visualization", "design", "visualizations", "practices"]}, {"paragraph_vector": [128.458251, 41.573123], "paragraph_keywords": ["design", "provide", "methods", "approaches"]}, {"paragraph_vector": [124.09732, 45.495281], "paragraph_keywords": ["viewers", "researchers", "methods", "memory"]}, {"paragraph_vector": [117.831192, 41.680644], "paragraph_keywords": ["set", "images", "tasks", "recognition"]}, {"paragraph_vector": [115.091667, 43.375747], "paragraph_keywords": ["viewers", "localization", "attention", "screen"]}, {"paragraph_vector": [119.976585, 42.942031], "paragraph_keywords": ["search", "viewers", "scatterplot", "screen"]}, {"paragraph_vector": [124.072738, 38.710998], "paragraph_keywords": ["search", "size", "set", "target"]}, {"paragraph_vector": [124.639556, 43.567703], "paragraph_keywords": ["display", "memory", "change", "working"]}, {"paragraph_vector": [123.968299, 42.760158], "paragraph_keywords": ["tasks", "image", "change", "memory"]}, {"paragraph_vector": [124.398918, 41.103958], "paragraph_keywords": ["attention", "target", "correlation", "matches"]}, {"paragraph_vector": [121.639068, 40.422168], "paragraph_keywords": ["viewers", "data", "bar", "correlation"]}, {"paragraph_vector": [111.921821, 42.548831], "paragraph_keywords": ["viewers", "categories", "tasks", "identification"]}, {"paragraph_vector": [119.96025, 44.688171], "paragraph_keywords": ["viewers", "estimation", "level", "correlation"]}, {"paragraph_vector": [109.371635, 35.449581], "paragraph_keywords": ["viewers", "mol", "level", "stimulus"]}, {"paragraph_vector": [111.7499, 39.94762], "paragraph_keywords": ["viewers", "target", "stimuli", "level"]}, {"paragraph_vector": [111.629302, 42.833404], "paragraph_keywords": ["viewers", "mocs", "stimuli", "researchers"]}, {"paragraph_vector": [116.413078, 42.705612], "paragraph_keywords": ["staircasing", "response", "viewers", "experiments"]}, {"paragraph_vector": [115.739898, 46.852825], "paragraph_keywords": ["viewers", "researchers", "choice", "bias"]}, {"paragraph_vector": [119.103179, 41.92052], "paragraph_keywords": ["viewers", "use", "dashboard", "designs"]}, {"paragraph_vector": [120.315414, 42.925079], "paragraph_keywords": ["task", "tasks", "viewer", "researcher"]}, {"paragraph_vector": [122.694892, 41.69474], "paragraph_keywords": ["measures", "accuracy", "assessing", "people"]}, {"paragraph_vector": [125.909919, 44.417572], "paragraph_keywords": ["accuracy", "visualization", "people", "time"]}, {"paragraph_vector": [130.819305, 42.511116], "paragraph_keywords": ["use", "visualization", "thresholds", "tasks"]}, {"paragraph_vector": [123.660011, 41.163448], "paragraph_keywords": ["function", "task", "changes", "performance"]}, {"paragraph_vector": [124.036033, 43.173286], "paragraph_keywords": ["detection", "bias", "sensitivity", "decision"]}, {"paragraph_vector": [128.257827, 42.854755], "paragraph_keywords": ["design", "methods", "visualization", "experiments"]}, {"paragraph_vector": [150.478683, 30.450246], "paragraph_keywords": ["visualization", "ieee", "provide", "use"]}], "content": {}, "doi": "10.1109/TVCG.2020.3030382"}, {"uri": "46", "title": "Designing Narrative-Focused Role-Playing Games for Visualization Literacy in Young Children", "timestamp": "2020", "rating": "0.0", "annotation": "", "tags": [], "authors": ["Elaine Huynh", "Angela Nyhout", "Patricia Ganea", "Fanny Chevalier"], "summary": "Building on game design and education research, this paper introduces narrative-focused role-playing games as a way to promote visualization literacy in young children. Visualization literacy skills are vital in understanding the world around us and constructing meaningful visualizations, yet, how to better develop these skills at an early age remains largely overlooked and understudied. Only recently has the visualization community started to fill this gap, resulting in preliminary studies and development of educational tools for use in early education. We add to these efforts through the exploration of gamification to support learning, and identify an opportunity to apply role-playing game-based designs by leveraging the presence of narratives in data-related problems involving visualizations. We study the effects of including narrative elements on learning through a technology probe, grounded in a set of design considerations stemming from visualization, game design and education science. We create two versions of a game \u2013 one with narrative elements and one without \u2013 and evaluate our instances on 33 child participants between 11to 13-years old using a between-subjects study design. Despite participants requiring double the amount of time to complete their game due to additional narrative elements, the inclusion of such elements were found to improve engagement without sacrificing learning; our results indicate no significant differences in development of graph-reading skills, but significant differences in engagement and overall enjoyment of the game. We report observations and qualitative feedback collected, and note areas for improvement and room for future work.", "keywords": ["feedback", "exploration", "use", "computer", "play", "found", "element", "way", "concept", "consider", "dialogue", "order", "problem", "child", "text", "choice", "effect", "learning", "research", "presented", "content", "test", "support", "impact", "participant", "player", "focus", "-", "information", "activity", "context", "arc", "question", "education", "work", "appeared", "topic", "presence", "visualization", "answer", "based", "explore", "fantasy", "world", "factor", "data", "motivation", "role", "engagement", "making", "required", "method", "time", "literacy", "doi", "chart", "character", "student", "narrative", "address", "game", "given", "exercise", "design", "condition", "science", "designer", "grade", "reading", "story", "study", "set", "performance", "read", "school"], "document_vector": [67.663612, -21.818784], "paragraphs": [{"paragraph_vector": [-158.952407, 46.354942], "paragraph_keywords": ["visualization", "data", "education", "support"]}, {"paragraph_vector": [-141.253784, 49.99422], "paragraph_keywords": ["game", "learning", "visualization", "design"]}, {"paragraph_vector": [-151.93222, 45.754554], "paragraph_keywords": ["visualization", "literacy", "design", "elements"]}, {"paragraph_vector": [-158.798675, 45.402282], "paragraph_keywords": ["visualization", "children", "games", "literacy"]}, {"paragraph_vector": [-140.524749, 48.288993], "paragraph_keywords": ["games", "players", "increased", "learning"]}, {"paragraph_vector": [-137.830749, 49.237094], "paragraph_keywords": ["narratives", "effects", "learning", "games"]}, {"paragraph_vector": [-145.059906, 48.930225], "paragraph_keywords": ["children", "game", "question", "charts"]}, {"paragraph_vector": [-140.973022, 59.385314], "paragraph_keywords": ["players", "story", "arc", "feedback"]}, {"paragraph_vector": [-140.454025, 52.208312], "paragraph_keywords": ["view", "figure", "exploration", "players"]}, {"paragraph_vector": [-140.303726, 54.619232], "paragraph_keywords": ["participants", "-", "time", "study"]}, {"paragraph_vector": [-141.734603, 54.668834], "paragraph_keywords": ["questions", "participants", "game", "appeared"]}, {"paragraph_vector": [-144.495544, 54.88718], "paragraph_keywords": ["participants", "condition", "narrative", "test"]}, {"paragraph_vector": [-141.017913, 55.845008], "paragraph_keywords": ["participants", "improved", "males", "charts"]}, {"paragraph_vector": [-139.387817, 55.327892], "paragraph_keywords": ["feedback", "participants", "found", "box"]}, {"paragraph_vector": [-152.17752, 46.284755], "paragraph_keywords": ["story", "problems", "participants", "characters"]}, {"paragraph_vector": [-165.308822, 65.368606], "paragraph_keywords": ["questions", "participants", "feedback", "think"]}, {"paragraph_vector": [-141.928421, 53.94617], "paragraph_keywords": ["participants", "narrative", "story", "ieee"]}, {"paragraph_vector": [-140.34642, 54.850543], "paragraph_keywords": ["learning", "performance", "game", "post"]}, {"paragraph_vector": [-149.367477, 49.741531], "paragraph_keywords": ["game", "designers", "participants", "story"]}, {"paragraph_vector": [-157.327911, 46.019744], "paragraph_keywords": ["children", "grade", "grades", "learning"]}, {"paragraph_vector": [-141.386611, 49.41909], "paragraph_keywords": ["time", "ieee", "game", "support"]}, {"paragraph_vector": [-152.095748, 54.473667], "paragraph_keywords": ["questions", "data", "chart", "activities"]}, {"paragraph_vector": [-157.879745, 53.953727], "paragraph_keywords": ["questions", "data", "problem", "formats"]}, {"paragraph_vector": [-141.701553, 50.412094], "paragraph_keywords": ["activities", "game", "design", "narratives"]}, {"paragraph_vector": [-138.440917, 53.505981], "paragraph_keywords": ["players", "models", "story", "characters"]}, {"paragraph_vector": [-144.113647, 49.364151], "paragraph_keywords": ["arc", "characters", "stories", "time"]}, {"paragraph_vector": [-140.219299, 47.137588], "paragraph_keywords": ["fantasy", "game", "learning", "player"]}, {"paragraph_vector": [-144.941665, 50.874183], "paragraph_keywords": ["feedback", "game", "designers", "players"]}, {"paragraph_vector": [-142.132858, 47.998821], "paragraph_keywords": ["players", "children", "requires", "items"]}, {"paragraph_vector": [-140.26773, 47.481216], "paragraph_keywords": ["play", "game", "features", "learning"]}, {"paragraph_vector": [-149.338409, 48.04943], "paragraph_keywords": ["learning", "game", "visualization", "design"]}, {"paragraph_vector": [-153.26799, 50.472579], "paragraph_keywords": ["questions", "game", "participants", "time"]}, {"paragraph_vector": [-140.763763, 48.087749], "paragraph_keywords": ["learning", "visualization", "narrative", "game"]}, {"paragraph_vector": [-150.451828, 47.886585], "paragraph_keywords": ["doi", "mathematics", "game", "computer"]}, {"paragraph_vector": [-149.628082, 47.710628], "paragraph_keywords": ["visualization", "learning", "doi", "literacy"]}, {"paragraph_vector": [-146.080764, 48.90102], "paragraph_keywords": ["games", "learning", "education", "children"]}, {"paragraph_vector": [-147.176147, 50.747566], "paragraph_keywords": ["games", "learning", "computer", "doi"]}, {"paragraph_vector": [-164.05043, 41.953334], "paragraph_keywords": ["doi", "learning", "computer", "visualization"]}, {"paragraph_vector": [-149.118026, 48.942577], "paragraph_keywords": ["doi", "role", "data", "games"]}, {"paragraph_vector": [-147.070693, 50.647735], "paragraph_keywords": ["learning", "gamers", "doi", "ad"]}], "content": {}, "doi": "10.1109/TVCG.2020.3030459"}, {"uri": "47", "title": "A Generic Framework and Library for Exploration of Small Multiples through Interactive Piling", "timestamp": "2020", "rating": "0.0", "annotation": "", "tags": [], "authors": ["Fritz Lekschas", "Xinyi Zhou", "Wei Chen", "Nils Gehlenborg", "Benjamin Bach", "Hanspeter Pfister"], "summary": "Small multiples are miniature representations of visual information used generically across many domains. Handling large numbers of small multiples imposes challenges on many analytic tasks like inspection, comparison, navigation, or annotation. To address these challenges, we developed a framework and implemented a library called PILING.JS for designing interactive piling interfaces. Based on the piling metaphor, such interfaces afford flexible organization, exploration, and comparison of large numbers of small multiples by interactively aggregating visual objects into piles. Based on a systematic analysis of previous work, we present a structured design space to guide the design of visual piling interfaces. To enable designers to efficiently build their own visual piling interfaces, PILING.JS provides a declarative interface to avoid having to write low-level code and implements common aspects of the design space. An accompanying GUI additionally supports the dynamic configuration of the piling interface. We demonstrate the expressiveness of PILING.JS with examples from machine learning, immunofluorescence microscopy, genomics, and public health.", "keywords": ["use", "exploration", "property", "image", "feature", "pattern", "user", "grouping", "comparison", "provides", "pile", "number", "content", "support", "frame", "technique", "state", "compare", "-", "size", "information", "previewing", "multiple", "interaction", "identify", "fig", "approach", "space", "arrangement", "subset", "preview", "item", "work", "cover", "provide", "browsing", "visualization", "based", "rendering", "shown", "datasets", "group", "data", "view", "involves", "time", "requires", "design", "grid", "aggregation", "piling"], "document_vector": [-131.854141, -36.034919], "paragraphs": [{"paragraph_vector": [-5.62964, -26.001876], "paragraph_keywords": ["multiples", "visualization", "subsets", "university"]}, {"paragraph_vector": [101.342018, -42.896259], "paragraph_keywords": ["piling", "use", "browsing", "data"]}, {"paragraph_vector": [97.527793, -43.196952], "paragraph_keywords": ["piling", "multiples", "data", "exploration"]}, {"paragraph_vector": [100.728637, -40.593612], "paragraph_keywords": ["piling", "items", "interaction", "based"]}, {"paragraph_vector": [102.987052, -47.37152], "paragraph_keywords": ["piling", "items", "pile", "matrix"]}, {"paragraph_vector": [102.226112, -40.041633], "paragraph_keywords": ["pile", "items", "piling", "piles"]}, {"paragraph_vector": [107.377555, -40.685398], "paragraph_keywords": ["piling", "identify", "items", "exploration"]}, {"paragraph_vector": [33.843772, 17.227712], "paragraph_keywords": ["items", "cluster", "approaches", "fig"]}, {"paragraph_vector": [98.168876, -40.689662], "paragraph_keywords": ["grouping", "items", "interactions", "grouped"]}, {"paragraph_vector": [102.118026, -41.69841], "paragraph_keywords": ["items", "pile", "grouping", "item"]}, {"paragraph_vector": [100.172012, -41.970729], "paragraph_keywords": ["items", "arrangements", "pile", "item"]}, {"paragraph_vector": [107.272857, -41.772136], "paragraph_keywords": ["items", "browsing", "item", "previews"]}, {"paragraph_vector": [100.866477, -40.335144], "paragraph_keywords": ["pile", "items", "browsing", "use"]}, {"paragraph_vector": [109.697998, -40.425453], "paragraph_keywords": ["items", "pile", "information", "provide"]}, {"paragraph_vector": [111.554023, -41.310657], "paragraph_keywords": ["data", "browsing", "pile", "piling"]}, {"paragraph_vector": [90.146766, -47.131324], "paragraph_keywords": ["items", "data", "property", "item"]}, {"paragraph_vector": [93.688423, -42.993122], "paragraph_keywords": ["renderer", "items", "provides", "renderers"]}, {"paragraph_vector": [98.508361, -42.239963], "paragraph_keywords": ["pile", "properties", "grouping", "data"]}, {"paragraph_vector": [102.153945, -41.644058], "paragraph_keywords": ["pile", "data", "mouse", "lasso"]}, {"paragraph_vector": [111.348548, -46.030899], "paragraph_keywords": ["based", "items", "view", "gui"]}, {"paragraph_vector": [90.559738, -46.355239], "paragraph_keywords": ["items", "time", "rendering", "interactions"]}, {"paragraph_vector": [30.215663, 48.429302], "paragraph_keywords": ["images", "cell", "subsets", "requires"]}, {"paragraph_vector": [-122.500068, 9.951129], "paragraph_keywords": ["temperature", "cell", "measurements", "cells"]}, {"paragraph_vector": [-21.15899, -21.852737], "paragraph_keywords": ["frames", "time", "precipitation", "frame"]}, {"paragraph_vector": [-11.048306, -28.610219], "paragraph_keywords": ["area", "piles", "patterns", "grouping"]}, {"paragraph_vector": [98.104263, -64.940139], "paragraph_keywords": ["pattern", "instances", "fig", "piling"]}, {"paragraph_vector": [90.793441, -45.223175], "paragraph_keywords": ["items", "piling", "design", "application"]}, {"paragraph_vector": [95.273681, -37.795967], "paragraph_keywords": ["use", "ieee", "development", "employed"]}], "content": {}, "doi": "10.1109/TVCG.2020.3030434"}, {"uri": "48", "title": "A Visual Analytics Approach for Exploratory Causal Analysis: Exploration, Validation, and Applications", "timestamp": "2020", "rating": "0.0", "annotation": "", "tags": [], "authors": ["Xiao Xie", "Fan Du", "Yingcai Wu"], "summary": "Using causal relations to guide decision making has become an essential analytical task across various domains, from marketing and medicine to education and social science. While powerful statistical models have been developed for inferring causal relations from data, domain practitioners still lack effective visual interface for interpreting the causal relations and applying them in their decision-making process. Through interview studies with domain experts, we characterize their current decision-making workflows, challenges, and needs. Through an iterative design process, we developed a visualization tool that allows analysts to explore, validate, and apply causal relations in real-world decision-making scenarios. The tool provides an uncertainty-aware causal graph visualization for presenting a large set of causal relations inferred from high-dimensional data. On top of the causal graph, it supports a set of intuitive user controls for performing what-if analyses and making action plans. We report on two case studies in marketing and student advising to demonstrate that users can effectively explore causal relations and design action plans for reaching their goals.", "keywords": ["use", "score", "user", "graph", "f", "node", "effect", "system", "model", "uncertainty", "number", "purchase", "marketing", "value", "link", "dimension", "fig", "expert", "process", "layout", "according", "need", "bar", "relation", "visualization", "edge", "based", "represents", "factor", "data", "variable", "intervention", "causality", "change", "detection", "student", "result", "domain", "distribution", "ge", "design", "causal", "attribution", "set", "analysis", "example", "analyst", "discovery", "commented"], "document_vector": [-89.64582, 61.39793], "paragraphs": [{"paragraph_vector": [-84.723442, -11.919749], "paragraph_keywords": ["analysis", "graph", "use", "data"]}, {"paragraph_vector": [-84.175537, -11.937166], "paragraph_keywords": ["graph", "bob", "campaign", "marketing"]}, {"paragraph_vector": [-85.048156, -17.120027], "paragraph_keywords": ["discovery", "analysis", "graph", "data"]}, {"paragraph_vector": [-85.190711, -15.222442], "paragraph_keywords": ["visualizations", "causality", "ges", "relations"]}, {"paragraph_vector": [-79.938537, -24.742456], "paragraph_keywords": ["graph", "uncertainty", "graphs", "causal"]}, {"paragraph_vector": [-88.615333, -20.612232], "paragraph_keywords": ["model", "models", "learning", "users"]}, {"paragraph_vector": [-83.083106, -13.41912], "paragraph_keywords": ["analysts", "data", "analysis", "models"]}, {"paragraph_vector": [-82.40863, -13.007792], "paragraph_keywords": ["analysis", "analysts", "needs", "marketing"]}, {"paragraph_vector": [-84.105895, -11.752894], "paragraph_keywords": ["marketing", "plans", "effect", "y"]}, {"paragraph_vector": [-81.635078, -13.047858], "paragraph_keywords": ["variables", "ci", "tests", "data"]}, {"paragraph_vector": [-80.986366, -14.401617], "paragraph_keywords": ["score", "graph", "edge", "ges"]}, {"paragraph_vector": [-80.781616, -14.178535], "paragraph_keywords": ["graph", "intervention", "variables", "values"]}, {"paragraph_vector": [-84.598663, -12.251662], "paragraph_keywords": ["attribution", "effect", "analysis", "designed"]}, {"paragraph_vector": [-78.63507, -22.749542], "paragraph_keywords": ["causal", "graph", "component", "users"]}, {"paragraph_vector": [-92.051208, -57.171413], "paragraph_keywords": ["graph", "node", "link", "direction"]}, {"paragraph_vector": [-79.116897, -35.393016], "paragraph_keywords": ["graph", "order", "layer", "nodes"]}, {"paragraph_vector": [-77.37593, -41.85461], "paragraph_keywords": ["links", "chain", "found", "layer"]}, {"paragraph_vector": [-60.465373, -58.88454], "paragraph_keywords": ["nodes", "layout", "node", "link"]}, {"paragraph_vector": [-86.161178, -12.246047], "paragraph_keywords": ["layout", "experts", "graph", "dimension"]}, {"paragraph_vector": [-45.001956, 19.365106], "paragraph_keywords": ["users", "intervention", "bar", "dimension"]}, {"paragraph_vector": [-78.854225, -18.723396], "paragraph_keywords": ["value", "users", "contribution", "change"]}, {"paragraph_vector": [-81.48394, -0.182332], "paragraph_keywords": ["student", "school", "analysts", "course"]}, {"paragraph_vector": [-85.845245, -18.019346], "paragraph_keywords": ["analysts", "student", "status", "link"]}, {"paragraph_vector": [-86.549507, -5.032999], "paragraph_keywords": ["analysts", "visit", "data", "students"]}, {"paragraph_vector": [-89.930061, -9.44924], "paragraph_keywords": ["data", "analysts", "graph", "nodes"]}, {"paragraph_vector": [-87.132461, -9.955754], "paragraph_keywords": ["purchase", "influence", "analysis", "verify"]}, {"paragraph_vector": [-84.744209, -11.214456], "paragraph_keywords": ["analysts", "results", "data", "testing"]}, {"paragraph_vector": [-82.269165, -19.107805], "paragraph_keywords": ["users", "detection", "link", "discovery"]}, {"paragraph_vector": [-80.954696, -15.469832], "paragraph_keywords": ["data", "dimensions", "detection", "observed"]}, {"paragraph_vector": [-82.140426, -18.834564], "paragraph_keywords": ["variables", "graph", "users", "analysis"]}, {"paragraph_vector": [-79.67752, -23.903572], "paragraph_keywords": ["zhejiang", "relations", "use", "ieee"]}], "content": {}, "doi": "10.1109/TVCG.2020.3030442"}, {"uri": "49", "title": "CAVA: A Visual Analytics System for Exploratory Columnar Data Augmentation Using Knowledge Graphs", "timestamp": "2020", "rating": "0.0", "annotation": "", "tags": [], "authors": ["Dylan Cashman", "Shenyu Xu", "Subhajit Das", "Florian Heimerl", "Cong Liu", "Shah Rukh Humayoun", "Michael Gleicher", "Alex Endert", "Remco Chang"], "summary": "Most visual analytics systems assume that all foraging for data happens before the analytics process; once analysis begins, the set of data attributes considered is fixed. Such separation of data construction from analysis precludes iteration that can enable foraging informed by the needs that arise in-situ during the analysis. The separation of the foraging loop from the data analysis tasks can limit the pace and scope of analysis. In this paper, we present CAVA, a system that integrates data curation and data augmentation with the traditional data exploration and analysis tasks, enabling information foraging in-situ during analysis. Identifying attributes to add to the dataset is difficult because it requires human knowledge to determine which available attributes will be helpful for the ensuing analytical tasks. CAVA crawls knowledge graphs to provide users with a a broad set of attributes drawn from external data to choose from. Users can then specify complex operations on knowledge graphs to construct additional attributes. CAVA shows how visual analytics can help users forage for attributes by letting users visually explore the set of available data, and by serving as an interface for query construction. It also provides visualizations of the knowledge graph itself to help users understand complex joins such as multi-hop aggregations. We assess the ability of our system to enable users to perform complex data combinations without programming in a user study over two datasets. We then demonstrate the generalizability of CAVA through two additional usage scenarios. The results of the evaluation confirm that CAVA is effective in helping the user perform data foraging that leads to improved analysis outcomes, and offer evidence in support of integrating data augmentation as a part of the visual analytics pipeline.", "keywords": ["use", "entity", "regression", "user", "graph", "augmentation", "machine", "row", "type", "search", "model", "system", "learning", "attribute", "usage", "add", "number", "participant", "state", "improve", "value", "information", "joining", "analytics", "workflow", "query", "process", "work", "task", "cava", "list", "dataset", "column", "wikidata", "help", "table", "country", "based", "figure", "datasets", "data", "foraging", "performance", "result", "andy", "related", "set", "analysis", "join", "knowledge", "study", "example"], "document_vector": [-128.122055, 4.718523], "paragraphs": [{"paragraph_vector": [-104.20565, -19.280923], "paragraph_keywords": ["process", "attributes", "dataset", "data"]}, {"paragraph_vector": [-106.612548, -20.641321], "paragraph_keywords": ["data", "user", "use", "ieee"]}, {"paragraph_vector": [-107.041389, -20.865966], "paragraph_keywords": ["attributes", "data", "knowledge", "cava"]}, {"paragraph_vector": [-109.475318, -19.46454], "paragraph_keywords": ["data", "knowledge", "usage", "cava"]}, {"paragraph_vector": [-121.136718, -28.864637], "paragraph_keywords": ["knowledge", "information", "entities", "graphs"]}, {"paragraph_vector": [-113.306373, -18.746427], "paragraph_keywords": ["data", "knowledge", "columns", "objects"]}, {"paragraph_vector": [-111.903198, -14.472698], "paragraph_keywords": ["data", "datasets", "knowledge", "entity"]}, {"paragraph_vector": [-111.625106, -21.190933], "paragraph_keywords": ["users", "data", "dataset", "knowledge"]}, {"paragraph_vector": [-112.482513, -18.676383], "paragraph_keywords": ["graph", "entities", "knowledge", "entity"]}, {"paragraph_vector": [-113.370429, -19.198741], "paragraph_keywords": ["data", "information", "use", "wikidata"]}, {"paragraph_vector": [-60.182392, 26.444856], "paragraph_keywords": ["data", "attribute", "attributes", "user"]}, {"paragraph_vector": [-111.282958, -20.392988], "paragraph_keywords": ["data", "user", "attributes", "goals"]}, {"paragraph_vector": [-107.347167, -18.44878], "paragraph_keywords": ["data", "attributes", "dataset", "user"]}, {"paragraph_vector": [-107.288452, -18.147773], "paragraph_keywords": ["attribute", "attributes", "data", "figure"]}, {"paragraph_vector": [-106.662261, -14.282941], "paragraph_keywords": ["user", "data", "attribute", "graph"]}, {"paragraph_vector": [-107.825973, -16.138158], "paragraph_keywords": ["user", "country", "use", "data"]}, {"paragraph_vector": [-108.638389, -16.67517], "paragraph_keywords": ["graph", "attributes", "knowledge", "column"]}, {"paragraph_vector": [-110.33081, -15.062014], "paragraph_keywords": ["attributes", "data", "rows", "knowledge"]}, {"paragraph_vector": [-100.36042, -12.006321], "paragraph_keywords": ["attributes", "data", "countries", "figure"]}, {"paragraph_vector": [-75.260284, 2.104301], "paragraph_keywords": ["generation", "user", "gdp", "types"]}, {"paragraph_vector": [-56.972209, 28.919914], "paragraph_keywords": ["model", "data", "cava", "attributes"]}, {"paragraph_vector": [-54.347778, 20.176103], "paragraph_keywords": ["andy", "model", "attributes", "data"]}, {"paragraph_vector": [-54.349658, 19.441488], "paragraph_keywords": ["cava", "data", "column", "model"]}, {"paragraph_vector": [-53.425926, 41.944263], "paragraph_keywords": ["participants", "machine", "task", "cava"]}, {"paragraph_vector": [-55.055484, 27.801879], "paragraph_keywords": ["attributes", "participants", "county", "rate"]}, {"paragraph_vector": [-57.64162, 29.161497], "paragraph_keywords": ["participants", "model", "cava", "describe"]}, {"paragraph_vector": [-57.724391, 26.482162], "paragraph_keywords": ["participants", "attributes", "data", "cava"]}, {"paragraph_vector": [-55.484436, 29.96059], "paragraph_keywords": ["participants", "attributes", "data", "task"]}, {"paragraph_vector": [-47.222156, 27.103752], "paragraph_keywords": ["participants", "study", "attributes", "results"]}, {"paragraph_vector": [-108.496818, -19.362926], "paragraph_keywords": ["data", "knowledge", "analytics", "graph"]}, {"paragraph_vector": [-108.711166, -14.710047], "paragraph_keywords": ["user", "attributes", "attribute", "cava"]}, {"paragraph_vector": [-103.238044, -20.035257], "paragraph_keywords": ["data", "cava", "user", "analytics"]}], "content": {}, "doi": "10.1109/TVCG.2020.3030450"}, {"uri": "50", "title": "Revisited: Comparison of Empirical Methods to Evaluate Visualizations Supporting Crafting and Assembly Purposes", "timestamp": "2020", "rating": "0.0", "annotation": "", "tags": [], "authors": ["Maximilian Wei\u00df", "Katrin Angerbauer", "Alexandra Voit", "Magdalena Schwarzl", "Michael Sedlmair", "Sven Mayer"], "summary": "Ubiquitous, situated, and physical visualizations create entirely new possibilities for tasks contextualized in the real world, such as doctors inserting needles. During the development of situated visualizations, evaluating visualizations is a core requirement. However, performing such evaluations is intrinsically hard as the real scenarios are safety-critical or expensive to test. To overcome these issues, researchers and practitioners adapt classical approaches from ubiquitous computing and use surrogate empirical methods such as Augmented Reality (AR), Virtual Reality (VR) prototypes, or merely online demonstrations. This approach\u2019s primary assumption is that meaningful insights can also be gained from different, usually cheaper and less cumbersome empirical methods. Nevertheless, recent efforts in the Human-Computer Interaction (HCI) community have found evidence against this assumption, which would impede the use of surrogate empirical methods. Currently, these insights rely on a single investigation of four interactive objects. The goal of this work is to investigate if these prior findings also hold for situated visualizations. Therefore, we first created a scenario where situated visualizations support users in do-it-yourself (DIY) tasks such as crafting and assembly. We then set up five empirical study methods to evaluate the four tasks using an online survey, as well as VR, AR, laboratory, and in-situ studies. Using this study design, we conducted a new study with 60 participants. Our results show that the situated visualizations we investigated in this study are not prone to the same dependency on the empirical method, as found in previous work. Our study provides the first evidence that analyzing situated visualizations through different empirical (surrogate) methods might lead to comparable results.", "keywords": ["use", "feedback", "diy", "found", "user", "lab", "setup", "ar", "effect", "questionnaire", "support", "participant", "al", "object", "usability", "context", "conducted", "p", "showed", "work", "task", "vr", "assembly", "situ", "visualization", "figure", "technology", "data", "evaluation", "finding", "method", "et", "asked", "result", "researcher", "university", "design", "condition", "analysis", "study", "instance"], "document_vector": [70.957405, 23.335081], "paragraphs": [{"paragraph_vector": [134.746917, 14.685083], "paragraph_keywords": ["visualizations", "university", "methods", "visualization"]}, {"paragraph_vector": [134.737167, 17.138168], "paragraph_keywords": ["methods", "visualizations", "evaluation", "scenarios"]}, {"paragraph_vector": [133.476104, 17.465074], "paragraph_keywords": ["visualizations", "methods", "study", "assembly"]}, {"paragraph_vector": [135.704025, 24.29635], "paragraph_keywords": ["work", "studies", "results", "community"]}, {"paragraph_vector": [132.690368, 12.687859], "paragraph_keywords": ["visualizations", "design", "work", "support"]}, {"paragraph_vector": [136.242004, 13.734443], "paragraph_keywords": ["assembly", "visualizations", "diy", "use"]}, {"paragraph_vector": [134.55902, 18.697877], "paragraph_keywords": ["tasks", "visualizations", "diy", "method"]}, {"paragraph_vector": [135.149856, 13.27962], "paragraph_keywords": ["visualizations", "visualization", "assembly", "position"]}, {"paragraph_vector": [136.700119, 15.705828], "paragraph_keywords": ["studies", "visualizations", "researchers", "situ"]}, {"paragraph_vector": [134.509796, 18.965778], "paragraph_keywords": ["use", "study", "methods", "usability"]}, {"paragraph_vector": [135.949523, 18.078023], "paragraph_keywords": ["participants", "visualizations", "asked", "wizard"]}, {"paragraph_vector": [138.443069, 7.277647], "paragraph_keywords": ["condition", "participants", "vr", "ieee"]}, {"paragraph_vector": [102.416389, 37.685882], "paragraph_keywords": ["method", "effect", "p", "way"]}, {"paragraph_vector": [139.220565, 17.797071], "paragraph_keywords": ["method", "visualization", "effect", "ieee"]}, {"paragraph_vector": [139.963455, 24.246017], "paragraph_keywords": ["reliability", "method", "effect", "attrakdiff"]}, {"paragraph_vector": [142.954986, 2.659978], "paragraph_keywords": ["method", "p", "revealed", "use"]}, {"paragraph_vector": [148.020401, -4.730804], "paragraph_keywords": ["technology", "comments", "visualization", "statements"]}, {"paragraph_vector": [125.809226, 26.87334], "paragraph_keywords": ["comments", "technology", "usability", "setup"]}, {"paragraph_vector": [133.853576, 17.125921], "paragraph_keywords": ["assistance", "example", "test", "concerning"]}, {"paragraph_vector": [135.503509, 23.582464], "paragraph_keywords": ["effect", "results", "method", "study"]}, {"paragraph_vector": [136.680725, 20.330055], "paragraph_keywords": ["visualizations", "results", "task", "performance"]}, {"paragraph_vector": [139.302719, 17.640727], "paragraph_keywords": ["technology", "difference", "questions", "analysis"]}, {"paragraph_vector": [134.468765, 23.14363], "paragraph_keywords": ["participants", "visualizations", "study", "context"]}, {"paragraph_vector": [136.426406, 17.381214], "paragraph_keywords": ["study", "participants", "use", "difference"]}, {"paragraph_vector": [135.870605, 16.507461], "paragraph_keywords": ["visualizations", "results", "methods", "visualization"]}, {"paragraph_vector": [140.533905, 35.292732], "paragraph_keywords": ["incorporate", "questions", "tailored", "analyze"]}], "content": {}, "doi": "10.1109/TVCG.2020.3030392"}, {"uri": "51", "title": "Exemplar-based Layout Fine-tuning for Node-link Diagrams", "timestamp": "2020", "rating": "0.0", "annotation": "", "tags": [], "authors": ["Jiacheng Pan", "Wei Chen", "Xiaodong Zhao", "Shuyue Zhou", "Wei Zeng", "Minfeng Zhu", "Jian Chen", "Siwei Fu", "Yingcai Wu"], "summary": "We design and evaluate a novel layout fine-tuning technique for node-link diagrams that facilitates exemplar-based adjustment of a group of substructures in batching mode. The key idea is to transfer user modifications on a local substructure to other substructures in the entire graph that are topologically similar to the exemplar. We first precompute a canonical representation for each substructure with node embedding techniques and then use it for on-the-fly substructure retrieval. We design and develop a light-weight interactive system to enable intuitive adjustment, modification transfer, and visual graph exploration. We also report some results of quantitative comparisons, three case studies, and a within-participant user study.", "keywords": ["use", "exemplar", "user", "graph", "tuning", "marker", "node", "structure", "generated", "performed", "number", "substructure", "participant", "s", "technique", "shape", "matching", "link", "j", "k", "length", "interaction", "approach", "expert", "layout", "target", "modification", "dataset", "readability", "algorithm", "edge", "based", "figure", "data", "method", "time", "result", "t", "correspondence", "transfer", "set", "completion", "study", "step"], "document_vector": [137.419998, -56.069385], "paragraphs": [{"paragraph_vector": [124.366668, -69.961555], "paragraph_keywords": ["layout", "graph", "data", "nodes"]}, {"paragraph_vector": [125.055641, -72.65586], "paragraph_keywords": ["substructures", "user", "layout", "graph"]}, {"paragraph_vector": [140.010238, -73.014755], "paragraph_keywords": ["methods", "graphs", "layout", "motifs"]}, {"paragraph_vector": [128.750274, -72.058113], "paragraph_keywords": ["layout", "user", "graph", "node"]}, {"paragraph_vector": [93.827018, -75.636932], "paragraph_keywords": ["node", "substructures", "graph", "figure"]}, {"paragraph_vector": [91.155822, -70.060531], "paragraph_keywords": ["substructures", "nodes", "layout", "user"]}, {"paragraph_vector": [105.228996, -71.734626], "paragraph_keywords": ["graph", "figure", "view", "layout"]}, {"paragraph_vector": [77.100227, -71.3087], "paragraph_keywords": ["t", "graph", "layout", "correspondences"]}, {"paragraph_vector": [73.004447, -65.049713], "paragraph_keywords": ["t", "correspondences", "markers", "target"]}, {"paragraph_vector": [67.477935, -57.170631], "paragraph_keywords": ["t", "s", "r", "markers"]}, {"paragraph_vector": [65.766479, -59.743309], "paragraph_keywords": ["t", "markers", "step", "layout"]}, {"paragraph_vector": [60.397224, -58.924831], "paragraph_keywords": ["t", "w", "j", "target"]}, {"paragraph_vector": [78.203109, -64.297195], "paragraph_keywords": ["correspondences", "performed", "node", "j"]}, {"paragraph_vector": [74.173912, -64.192657], "paragraph_keywords": ["substructures", "figure", "datasets", "generated"]}, {"paragraph_vector": [88.934425, -71.924224], "paragraph_keywords": ["figure", "nodes", "target", "substructures"]}, {"paragraph_vector": [118.136856, -72.220901], "paragraph_keywords": ["readability", "figure", "dataset", "layout"]}, {"paragraph_vector": [-80.475868, -65.811813], "paragraph_keywords": ["expert", "email", "dataset", "mouse"]}, {"paragraph_vector": [-60.582286, -62.451568], "paragraph_keywords": ["dataset", "nodes", "target", "study"]}, {"paragraph_vector": [149.288055, -19.77168], "paragraph_keywords": ["participants", "exemplar", "layouts", "targets"]}, {"paragraph_vector": [147.308731, -17.448108], "paragraph_keywords": ["completion", "performed", "interactions", "method"]}, {"paragraph_vector": [127.844764, -68.036354], "paragraph_keywords": ["participants", "time", "markers", "method"]}, {"paragraph_vector": [118.467933, -72.470054], "paragraph_keywords": ["algorithm", "interactions", "user", "graph"]}, {"paragraph_vector": [77.114006, -64.37535], "paragraph_keywords": ["correspondences", "number", "complexity", "time"]}, {"paragraph_vector": [123.206993, -73.431663], "paragraph_keywords": ["approach", "based", "readability", "user"]}], "content": {}, "doi": "10.1109/TVCG.2020.3030404"}, {"uri": "52", "title": "Cartographic Relief Shading with Neural Networks", "timestamp": "2020", "rating": "0.0", "annotation": "", "tags": [], "authors": ["Bernhard Jenny", "Magnus Heitzler", "Dilpreet Singh", "Marianna Farmakis-Serebryakova", "Jeffery Chieh Liu", "Lorenz Hurni"], "summary": "Shaded relief is an effective method for visualising terrain on topographic maps, especially when the direction of illumination is adapted locally to emphasise individual terrain features. However, digital shading algorithms are unable to fully match the expressiveness of hand-crafted masterpieces, which are created through a laborious process by highly specialised cartographers. We replicate hand-drawn relief shading using U-Net neural networks. The deep neural networks are trained with manual shaded relief images of the Swiss topographic map series and terrain models of the same area. The networks generate shaded relief that closely resemble hand-drawn shaded relief art. The networks learn essential design principles from manual relief shading such as removing unnecessary terrain details, locally adjusting the illumination direction to accentuate individual terrain features, and varying brightness to emphasise larger landforms. Neural network shadings are generated from digital elevation models in a few seconds, and a study with 18 relief shading experts found that they are of high quality.", "keywords": ["use", "shading", "scale", "computer", "image", "architecture", "feature", "range", "model", "cartography", "learning", "landforms", "generalisation", "network", "contrast", "mountain", "vol", "detail", "value", "information", "size", "trained", "pair", "tile", ".", "journal", "expert", "fig", "output", "brightness", "transaction", "shaded", "map", "direction", "proceeding", "relief", "input", "rendering", "figure", "layer", "created", "ridge", "illumination", "method", "ieee", "area", "quality", "doi", "representation", "training", "time", "style", "resulting", "result", "elevation", "transfer", "applied", "design", "terrain", "example", "u", "conference"], "document_vector": [53.996173, -75.006576], "paragraphs": [{"paragraph_vector": [4.11546, 45.953041], "paragraph_keywords": ["relief", "shaded", "terrain", "mountain"]}, {"paragraph_vector": [5.49951, 46.939033], "paragraph_keywords": ["ridges", "principles", "shaded", "terrain"]}, {"paragraph_vector": [3.915084, 44.065967], "paragraph_keywords": ["shading", "et", "illumination", "al"]}, {"paragraph_vector": [3.760121, 45.733791], "paragraph_keywords": ["style", "image", "rendering", "content"]}, {"paragraph_vector": [4.597346, 45.137241], "paragraph_keywords": ["shading", "terrain", "illumination", "relief"]}, {"paragraph_vector": [4.60789, 45.311645], "paragraph_keywords": ["network", "shading", "model", "images"]}, {"paragraph_vector": [-2.538678, 62.589183], "paragraph_keywords": ["path", "network", "dropout", "steps"]}, {"paragraph_vector": [3.684666, 46.245361], "paragraph_keywords": ["tile", "network", "tiles", "architecture"]}, {"paragraph_vector": [5.268322, 44.884277], "paragraph_keywords": ["output", "input", "layers", "relief"]}, {"paragraph_vector": [7.02455, 45.98241], "paragraph_keywords": ["areas", "shading", "lake", "training"]}, {"paragraph_vector": [5.905948, 44.543342], "paragraph_keywords": ["shading", "terrain", "training", "shaded"]}, {"paragraph_vector": [0.600871, 45.050361], "paragraph_keywords": ["shading", "terrain", "elevation", "types"]}, {"paragraph_vector": [4.705543, 45.415477], "paragraph_keywords": ["networks", "shading", "relief", "size"]}, {"paragraph_vector": [0.800005, 43.446727], "paragraph_keywords": ["shading", "network", "terrain", "illumination"]}, {"paragraph_vector": [0.788874, 45.298557], "paragraph_keywords": ["shading", "shadings", "network", "ridges"]}, {"paragraph_vector": [3.114077, 46.599967], "paragraph_keywords": ["shadings", "pair", "image", "likert"]}, {"paragraph_vector": [1.16608, 44.903594], "paragraph_keywords": ["shadings", "network", "ratings", "shading"]}, {"paragraph_vector": [-2.463195, 41.806659], "paragraph_keywords": ["shading", "experts", "shadings", "network"]}, {"paragraph_vector": [4.920039, 44.631084], "paragraph_keywords": ["shading", "network", "use", "shadings"]}, {"paragraph_vector": [0.581005, 44.969669], "paragraph_keywords": ["network", "features", "terrain", "cell"]}, {"paragraph_vector": [4.764012, 45.06282], "paragraph_keywords": ["network", "shading", "networks", "terrain"]}, {"paragraph_vector": [3.095338, 45.692264], "paragraph_keywords": ["shading", "training", "networks", "national"]}, {"paragraph_vector": [3.069219, 59.909751], "paragraph_keywords": ["image", "ieee", "computer", "relief"]}, {"paragraph_vector": [9.508462, 51.991615], "paragraph_keywords": ["relief", "shading", "vol", "cartography"]}, {"paragraph_vector": [5.070912, 44.478553], "paragraph_keywords": ["shading", "relief", ".", "vol"]}, {"paragraph_vector": [7.766681, 47.915218], "paragraph_keywords": ["shading", "relief", "vol", "terrain"]}, {"paragraph_vector": [6.220365, 44.396903], "paragraph_keywords": ["shading", ".", "computer", "terrain"]}, {"paragraph_vector": [3.102435, 53.886585], "paragraph_keywords": ["image", "ieee", "computer", "conference"]}, {"paragraph_vector": [-1.203275, 63.058349], "paragraph_keywords": ["conference", ".", "machine", "synthesis"]}, {"paragraph_vector": [-2.724394, 60.606693], "paragraph_keywords": ["ieee", "atlas", "networks", "learning"]}], "content": {}, "doi": "10.1109/TVCG.2020.3030385"}, {"uri": "53", "title": "Personal Augmented Reality for Information Visualization on Large Interactive Displays", "timestamp": "2020", "rating": "0.0", "annotation": "", "tags": [], "authors": ["Patrick Reipschl\u00e4ger", "Tamara Flemisch", "Raimund Dachselt"], "summary": "In this work we propose the combination of large interactive displays with personal head-mounted Augmented Reality (AR) for information visualization to facilitate data exploration and analysis. Even though large displays provide more display space, they are challenging with regard to perception, effective multi-user support, and managing data density and complexity. To address these issues and illustrate our proposed setup, we contribute an extensive design space comprising first, the spatial alignment of display, visualizations, and objects in AR space. Next, we discuss which parts of a visualization can be augmented. Finally, we analyze how AR can be used to display personal views in order to show additional information and to minimize the mutual disturbance of data analysts. Based on this conceptual foundation, we present a number of exemplary techniques for extending visualizations with AR and discuss their relation to our design space. We further describe how these techniques address typical visualization problems that we have identified during our literature research. To examine our concepts, we introduce a generic AR visualization framework as well as a prototype implementing several example techniques. In order to demonstrate their potential, we further present a use case walkthrough in which we analyze a movie data set. From these experiences, we conclude that the contributed techniques can be useful in exploring and understanding multivariate data. We are convinced that the extension of large displays with AR for information visualization has a great potential for data analysis and sense-making.", "keywords": ["use", "movie", "reality", "user", "zone", "showing", "setup", "ar", "display", "number", "content", "support", "technique", "al", "alignment", "focus", "perception", "object", "size", "information", "link", "interaction", "part", "potential", "space", "work", "mark", "visualization", "based", "layer", "shown", "case", "issue", "position", "axis", "data", "awareness", "view", "address", "et", "design", "overview", "screen", "analysis", "analyst", "touch"], "document_vector": [84.987106, 16.893381], "paragraphs": [{"paragraph_vector": [132.699768, -14.294717], "paragraph_keywords": ["data", "display", "displays", "users"]}, {"paragraph_vector": [133.346939, -9.248152], "paragraph_keywords": ["display", "data", "analysis", "analytics"]}, {"paragraph_vector": [130.603927, -13.114486], "paragraph_keywords": ["data", "visualization", "interaction", "information"]}, {"paragraph_vector": [133.774475, -4.270552], "paragraph_keywords": ["data", "reality", "al", "ar"]}, {"paragraph_vector": [132.25296, -10.476365], "paragraph_keywords": ["ar", "displays", "display", "visualizations"]}, {"paragraph_vector": [128.994552, -15.505205], "paragraph_keywords": ["displays", "ar", "data", "display"]}, {"paragraph_vector": [115.852676, -18.552968], "paragraph_keywords": ["space", "data", "displays", "visualization"]}, {"paragraph_vector": [118.825698, -32.702217], "paragraph_keywords": ["data", "space", "techniques", "displays"]}, {"paragraph_vector": [134.840881, -8.516686], "paragraph_keywords": ["display", "users", "displays", "data"]}, {"paragraph_vector": [127.946701, -14.388482], "paragraph_keywords": ["display", "content", "ar", "objects"]}, {"paragraph_vector": [127.504089, -14.680944], "paragraph_keywords": ["zones", "zone", "display", "content"]}, {"paragraph_vector": [128.121963, -18.420953], "paragraph_keywords": ["visualization", "content", "data", "ar"]}, {"paragraph_vector": [128.094116, -19.873805], "paragraph_keywords": ["visualization", "zones", "visualizations", "axis"]}, {"paragraph_vector": [130.774566, -15.722057], "paragraph_keywords": ["views", "content", "ar", "shared"]}, {"paragraph_vector": [130.295822, -16.101215], "paragraph_keywords": ["techniques", "data", "ar", "visualization"]}, {"paragraph_vector": [128.306564, -17.97219], "paragraph_keywords": ["data", "visualization", "ar", "analyst"]}, {"paragraph_vector": [123.57711, -18.967718], "paragraph_keywords": ["visualizations", "analyst", "display", "parts"]}, {"paragraph_vector": [125.72277, -18.791566], "paragraph_keywords": ["display", "analyst", "links", "view"]}, {"paragraph_vector": [127.66703, -20.837497], "paragraph_keywords": ["links", "data", "visualizations", "axis"]}, {"paragraph_vector": [124.007453, -22.129442], "paragraph_keywords": ["axis", "data", "ar", "visualization"]}, {"paragraph_vector": [129.705154, -19.41864], "paragraph_keywords": ["layers", "ar", "visualization", "visualizations"]}, {"paragraph_vector": [130.112304, -17.686994], "paragraph_keywords": ["data", "ar", "filters", "visualization"]}, {"paragraph_vector": [132.261428, -14.533767], "paragraph_keywords": ["ar", "techniques", "visualizations", "annotations"]}, {"paragraph_vector": [125.401451, -15.451807], "paragraph_keywords": ["display", "framework", "touch", "visualizations"]}, {"paragraph_vector": [127.555427, -16.091331], "paragraph_keywords": ["display", "techniques", "visualizations", "components"]}, {"paragraph_vector": [-150.240127, -51.617156], "paragraph_keywords": ["movies", "matt", "sue", "tasks"]}, {"paragraph_vector": [-154.62889, -48.549057], "paragraph_keywords": ["analysis", "movies", "data", "links"]}, {"paragraph_vector": [126.351287, -14.822117], "paragraph_keywords": ["ar", "content", "display", "perception"]}, {"paragraph_vector": [131.473678, -12.381299], "paragraph_keywords": ["ar", "display", "environments", "data"]}, {"paragraph_vector": [130.909332, -11.034856], "paragraph_keywords": ["display", "analysts", "ar", "content"]}, {"paragraph_vector": [134.325225, -14.245157], "paragraph_keywords": ["ar", "work", "visualizations", "content"]}, {"paragraph_vector": [129.53215, -13.095963], "paragraph_keywords": ["data", "visualization", "displays", "ar"]}], "content": {}, "doi": "10.1109/TVCG.2020.3030476"}, {"uri": "54", "title": "VizCommender: Computing Text-Based Similarity in Visualization Repositories for Content-Based Recommendations", "timestamp": "2020", "rating": "0.0", "annotation": "", "tags": [], "authors": ["Michael Oppermann", "Robert Kincaid", "Tamara Munzner"], "summary": "Cloud-based visualization services have made visual analytics accessible to a much wider audience than ever before. Systems such as Tableau have started to amass increasingly large repositories of analytical knowledge in the form of interactive visualization workbooks. When shared, these collections can form a visual analytic knowledge base. However, as the size of a collection increases, so does the difficulty in finding relevant information. Content-based recommendation (CBR) systems could help analysts in finding and managing workbooks relevant to their interests. Toward this goal, we focus on text-based content that is representative of the subject matter of visualizations rather than the visual encodings and style. We discuss the challenges associated with creating a CBR based on visualization specifications and explore more concretely how to implement the relevance measures required using Tableau workbook specifications as the source of content data. We also demonstrate what information can be extracted from these visualization specifications and how various natural language processing techniques can be used to compute similarity between workbooks as one way to measure relevance. We report on a crowd-sourced user study to determine if our similarity measure mimics human judgement. Finally, we choose latent Dirichlet allocation (LDA) as a specific model and instantiate it in a proof-of-concept recommender tool to demonstrate the basic function of our similarity measure.", "keywords": ["use", "score", "reference", "scale", "feature", "order", "user", "lda", "name", "text", "model", "system", "number", "content", "viz", "participant", "tf", "compare", "-", "information", "idf", "word", "tableau", "context", "vizrepos", "approach", "similarity", "process", "work", "item", "task", "agreement", "topic", "judgement", "sheet", "visualization", "cbr", "based", "workbook", "document", "repository", "data", "result", "domain", "recommender", "cfr", "design", "triplet", "set", "example", "study", "recommendation", "specification"], "document_vector": [143.541625, -12.137735], "paragraphs": [{"paragraph_vector": [-159.218856, -89.122711], "paragraph_keywords": ["data", "visualization", "repositories", "based"]}, {"paragraph_vector": [-158.964584, -47.340999], "paragraph_keywords": ["content", "based", "visualization", "data"]}, {"paragraph_vector": [-159.355377, -46.307067], "paragraph_keywords": ["visualization", "recommendations", "similarity", "content"]}, {"paragraph_vector": [-149.531616, -46.146366], "paragraph_keywords": ["items", "cfr", "systems", "recommendations"]}, {"paragraph_vector": [-155.684539, -46.808395], "paragraph_keywords": ["tableau", "workbooks", "cbr", "data"]}, {"paragraph_vector": [-156.981231, -46.912479], "paragraph_keywords": ["users", "study", "discussions", "use"]}, {"paragraph_vector": [-159.829879, -49.292133], "paragraph_keywords": ["workbooks", "workbook", "recommendations", "user"]}, {"paragraph_vector": [-159.590698, -47.678932], "paragraph_keywords": ["data", "sheets", "recommendation", "specification"]}, {"paragraph_vector": [-145.974365, -45.96305], "paragraph_keywords": ["algorithms", "metrics", "recommender", "ieee"]}, {"paragraph_vector": [-148.723297, -44.408046], "paragraph_keywords": ["similarity", "recommendations", "text", "study"]}, {"paragraph_vector": [-146.730834, -44.956127], "paragraph_keywords": ["users", "workbooks", "recommendations", "based"]}, {"paragraph_vector": [-151.789199, -44.110942], "paragraph_keywords": ["data", "use", "text", "workbook"]}, {"paragraph_vector": [-154.101028, -30.592977], "paragraph_keywords": ["text", "information", "stop", "workbook"]}, {"paragraph_vector": [-161.071563, -44.368629], "paragraph_keywords": ["data", "workbook", "specifications", "visualizations"]}, {"paragraph_vector": [-152.941421, -36.768058], "paragraph_keywords": ["text", "workbooks", "tableau", "challenge"]}, {"paragraph_vector": [-153.134826, -39.503711], "paragraph_keywords": ["workbook", "data", "visualizations", "text"]}, {"paragraph_vector": [-153.964843, -47.246444], "paragraph_keywords": ["workbooks", "similarity", "workbook", "visualizations"]}, {"paragraph_vector": [-144.012344, -45.691875], "paragraph_keywords": ["document", "models", "similarity", "use"]}, {"paragraph_vector": [-140.096954, -44.114116], "paragraph_keywords": ["document", "idf", "tf", "similarity"]}, {"paragraph_vector": [-139.298934, -45.229106], "paragraph_keywords": ["model", "use", "word", "similarity"]}, {"paragraph_vector": [-139.320434, -45.211498], "paragraph_keywords": ["similarity", "judgements", "word", "text"]}, {"paragraph_vector": [-151.875244, -44.669448], "paragraph_keywords": ["similarity", "workbooks", "visualizations", "scores"]}, {"paragraph_vector": [-147.536102, -42.822715], "paragraph_keywords": ["triplets", "based", "triplet", "ieee"]}, {"paragraph_vector": [-150.139053, -46.239849], "paragraph_keywords": ["results", "model", "participants", "study"]}, {"paragraph_vector": [78.834846, 63.905704], "paragraph_keywords": ["agreement", "participants", "triplets", "participant"]}, {"paragraph_vector": [-143.481155, -43.536579], "paragraph_keywords": ["triplets", "model", "consensus", "number"]}, {"paragraph_vector": [-142.143112, -47.317226], "paragraph_keywords": ["models", "lda", "results", "use"]}, {"paragraph_vector": [-151.608917, -47.210121], "paragraph_keywords": ["workbooks", "data", "similarity", "model"]}, {"paragraph_vector": [-151.240173, -46.591941], "paragraph_keywords": ["workbooks", "model", "number", "use"]}, {"paragraph_vector": [-155.860244, -46.788188], "paragraph_keywords": ["user", "workbook", "example", "recommendations"]}, {"paragraph_vector": [-153.592803, -46.020771], "paragraph_keywords": ["based", "recommendations", "ieee", "model"]}], "content": {}, "doi": "10.1109/TVCG.2020.3030462"}, {"uri": "55", "title": "Visual Analysis of Argumentation in Essays", "timestamp": "2020", "rating": "0.0", "annotation": "", "tags": [], "authors": ["Dora Kiesel", "Patrick Riehmann", "Henning Wachsmuth", "Benno Stein", "Bernd Froehlich"], "summary": "This paper presents a visual analytics system for exploring, analyzing and comparing argument structures in essay corpora. We provide an overview of the corpus by a list of ArguLines which represent the argument units of each essay by a sequence of glyphs. Each glyph encodes the stance, the depth and the relative position of an argument unit. The overview can be ordered in various ways to reveal patterns and outliers. Subsets of essays can be selected and analyzed in detail using the Argument Unit Occurrence Tree which aggregates the argument structures using hierarchical histograms. This hierarchical view facilitates the estimation of statistics and trends concerning the progression of the argumentation in the essays. It also provides insights into the commonalities and differences between selected subsets. The text view is the necessary textual basis to verify conclusions from the other views and the annotation process. Linking the views and interaction techniques for visual filtering, studying the evolution of stance within a subset of essays and scrutinizing the order of argumentative units enable a deep analysis of essay corpora. Our expert reviews confirmed the utility of the system and revealed detailed and previously unknown information about the argumentation in our sample corpus.", "keywords": ["use", "premise", "argulines", "pattern", "con", "depth", "text", "node", "structure", "system", "comparing", "research", "number", "tree", "level", "information", "allows", "subset", "expert", "fig", "unit", "countables", "process", "task", "list", "show", "stance", "annotation", "visualization", "position", "case", "argumentation", "claim", "essay", "view", "corpus", "step", "auot", "selected", "overview", "argument", "analysis", "contain"], "document_vector": [169.248733, 33.073368], "paragraphs": [{"paragraph_vector": [-105.518066, 50.412216], "paragraph_keywords": ["argument", "structures", "aspects", "argumentation"]}, {"paragraph_vector": [-105.20581, 51.120559], "paragraph_keywords": ["argument", "text", "structures", "humanists"]}, {"paragraph_vector": [-105.021186, 48.519855], "paragraph_keywords": ["argument", "units", "experts", "corpus"]}, {"paragraph_vector": [-107.127243, 50.474903], "paragraph_keywords": ["argument", "text", "claim", "annotated"]}, {"paragraph_vector": [-108.497268, 49.76123], "paragraph_keywords": ["argument", "accumulated", "tree", "trees"]}, {"paragraph_vector": [-104.39199, 50.747688], "paragraph_keywords": ["visualization", "argumentation", "argument", "system"]}, {"paragraph_vector": [-107.137489, 52.872657], "paragraph_keywords": ["stance", "system", "argument", "structures"]}, {"paragraph_vector": [-102.928558, 49.767337], "paragraph_keywords": ["structures", "list", "argulines", "argument"]}, {"paragraph_vector": [-104.048744, 53.324325], "paragraph_keywords": ["argument", "units", "color", "depth"]}, {"paragraph_vector": [-106.019454, 52.10527], "paragraph_keywords": ["units", "view", "argument", "claims"]}, {"paragraph_vector": [-108.282493, 52.575187], "paragraph_keywords": ["text", "length", "annotators", "annotation"]}, {"paragraph_vector": [-103.401336, 51.556137], "paragraph_keywords": ["argument", "texts", "essays", "con"]}, {"paragraph_vector": [-103.644882, 51.698162], "paragraph_keywords": ["node", "essays", "argument", "relation"]}, {"paragraph_vector": [-106.079696, 51.721195], "paragraph_keywords": ["stance", "units", "claim", "ieee"]}, {"paragraph_vector": [-104.664787, 51.461826], "paragraph_keywords": ["text", "queries", "annotation", "essays"]}, {"paragraph_vector": [-103.380569, 52.69704], "paragraph_keywords": ["unit", "argument", "filtering", "text"]}, {"paragraph_vector": [-103.954467, 42.398742], "paragraph_keywords": ["experts", "argulines", "overview", "approach"]}, {"paragraph_vector": [-106.783058, 50.233825], "paragraph_keywords": ["corpus", "expert", "argument", "section"]}, {"paragraph_vector": [-105.340148, 52.775489], "paragraph_keywords": ["texts", "argument", "corpus", "subsets"]}, {"paragraph_vector": [-104.521545, 50.937648], "paragraph_keywords": ["corpus", "system", "use", "essays"]}, {"paragraph_vector": [-102.898704, 52.558139], "paragraph_keywords": ["claim", "claims", "bias", "myside"]}, {"paragraph_vector": [-108.710884, 52.411125], "paragraph_keywords": ["claim", "stance", "corpus", "point"]}, {"paragraph_vector": [-104.950241, 52.119121], "paragraph_keywords": ["argument", "essay", "following", "text"]}, {"paragraph_vector": [-103.72908, 52.601947], "paragraph_keywords": ["argument", "units", "countables", "annotation"]}, {"paragraph_vector": [-104.235412, 52.191562], "paragraph_keywords": ["argument", "countables", "structures", "claims"]}, {"paragraph_vector": [-106.826332, 51.386787], "paragraph_keywords": ["argumentation", "argument", "structures", "order"]}, {"paragraph_vector": [-105.793159, 52.621959], "paragraph_keywords": ["research", "ministry", "samples", "corpora"]}], "content": {}, "doi": "10.1109/TVCG.2020.3030446"}, {"uri": "56", "title": "Rainbows Revisited: Modeling Effective Colormap Design for Graphical Inference", "timestamp": "2020", "rating": "0.0", "annotation": "", "tags": [], "authors": ["Khairi Reda", "Danielle Albers Szafir"], "summary": "Color mapping is a foundational technique for visualizing scalar data. Prior literature offers guidelines for effective colormap design, such as emphasizing luminance variation while limiting changes in hue. However, empirical studies of color are largely focused on perceptual tasks. This narrow focus inhibits our understanding of how generalizable these guidelines are, particularly to tasks like visual inference that require synthesis and judgement across multiple percepts. Furthermore, the emphasis on traditional ramp designs (e.g., sequential or diverging) may sideline other key metrics or design strategies. We study how a cognitive metric\u2014color name variation\u2014impacts people\u2019s ability to make model-based judgments. In two graphical inference experiments, participants saw a series of color-coded scalar fields sampled from different models and assessed the relationships between these models. Contrary to conventional guidelines, participants were more accurate when viewing colormaps that cross a variety of uniquely nameable colors. We modeled participants\u2019 performance using this metric and found that it provides a better fit to the experimental data than do existing design principles. Our findings indicate cognitive advantages for colorful maps like rainbow, which exhibit high color categorization, despite their traditionally undesirable perceptual properties. We also found no evidence that color categorization would lead observers to infer false data features. Our results provide empirically grounded metrics for predicting a colormap\u2019s performance and suggest alternative guidelines for designing new quantitative colormaps to support inference. The data and materials for this paper are available at: https://osf.io/tck2r/", "keywords": ["use", "property", "found", "feature", "lineup", "colormaps", "categorization", "jet", "trial", "sampled", "lab", "model", "provides", "number", "colormap", "test", "participant", "field", "decoy", "guideline", "accuracy", "color", "luminance", "people", "characteristic", "work", "task", "hue", "target", "distance", "specificity", "difference", "map", "viridis", "plasma", "variation", "visualization", "based", "figure", "variety", "data", "metric", "performance", "result", "inference", "response", "hypothesis", "rgb", "design", "rainbow", "set", "example", "ramp", "study", "experiment"], "document_vector": [36.849121, -6.22325], "paragraphs": [{"paragraph_vector": [66.525947, 48.786861], "paragraph_keywords": ["color", "use", "university", "data"]}, {"paragraph_vector": [70.53408, 51.149986], "paragraph_keywords": ["tasks", "comparing", "models", "model"]}, {"paragraph_vector": [65.455818, 52.193695], "paragraph_keywords": ["color", "colormap", "principles", "metric"]}, {"paragraph_vector": [69.836044, 47.724861], "paragraph_keywords": ["color", "colormaps", "rainbow", "data"]}, {"paragraph_vector": [70.646911, 50.287456], "paragraph_keywords": ["colormap", "colormaps", "guidelines", "design"]}, {"paragraph_vector": [68.914833, 51.403537], "paragraph_keywords": ["rainbow", "found", "task", "colormaps"]}, {"paragraph_vector": [61.897743, 49.835666], "paragraph_keywords": ["model", "data", "task", "color"]}, {"paragraph_vector": [54.799686, 52.076282], "paragraph_keywords": ["visualization", "inference", "model", "target"]}, {"paragraph_vector": [53.255603, 34.651294], "paragraph_keywords": ["target", "densities", "decoy", "model"]}, {"paragraph_vector": [68.782165, 48.750156], "paragraph_keywords": ["color", "differences", "inference", "distinguish"]}, {"paragraph_vector": [70.16352, 48.767395], "paragraph_keywords": ["color", "people", "maps", "colormaps"]}, {"paragraph_vector": [65.17617, 48.646884], "paragraph_keywords": ["color", "colors", "categorization", "people"]}, {"paragraph_vector": [65.666084, 51.346393], "paragraph_keywords": ["variation", "ramps", "colormaps", "hue"]}, {"paragraph_vector": [66.047485, 51.124507], "paragraph_keywords": ["colormap", "models", "model", "design"]}, {"paragraph_vector": [73.401504, 46.881416], "paragraph_keywords": ["divergence", "trial", "lineup", "design"]}, {"paragraph_vector": [77.387756, 50.540744], "paragraph_keywords": ["design", "participants", "ra", "trials"]}, {"paragraph_vector": [80.711906, 49.398765], "paragraph_keywords": ["model", "participants", "variation", "colormap"]}, {"paragraph_vector": [65.425041, 49.895458], "paragraph_keywords": ["variation", "jet", "z", "color"]}, {"paragraph_vector": [67.171714, 49.667381], "paragraph_keywords": ["model", "color", "length", "use"]}, {"paragraph_vector": [68.439529, 49.12635], "paragraph_keywords": ["variation", "inference", "rainbow", "colormaps"]}, {"paragraph_vector": [71.025817, 52.120201], "paragraph_keywords": ["participants", "visualizations", "model", "lineup"]}, {"paragraph_vector": [82.90007, 50.054916], "paragraph_keywords": ["participants", "colormap", "trials", "ieee"]}, {"paragraph_vector": [67.714225, 50.262046], "paragraph_keywords": ["model", "rainbow", "color", "specificity"]}, {"paragraph_vector": [67.266777, 50.924358], "paragraph_keywords": ["variation", "color", "colormaps", "colors"]}, {"paragraph_vector": [74.410987, 50.493904], "paragraph_keywords": ["features", "plasma", "model", "viridis"]}, {"paragraph_vector": [68.290466, 50.511074], "paragraph_keywords": ["colormaps", "model", "rainbow", "variation"]}, {"paragraph_vector": [65.131599, 50.835823], "paragraph_keywords": ["design", "models", "variation", "color"]}, {"paragraph_vector": [66.444458, 50.057456], "paragraph_keywords": ["inference", "people", "variation", "results"]}, {"paragraph_vector": [68.027717, 49.310066], "paragraph_keywords": ["color", "facility", "science", "found"]}], "content": {}, "doi": "10.1109/TVCG.2020.3030395"}, {"uri": "57", "title": "Visual Reasoning Strategies for Effect Size Judgments and Decisions", "timestamp": "2020", "rating": "0.0", "annotation": "", "tags": [], "authors": ["Alex Kale", "Matthew Kay", "Jessica Hullman"], "summary": "Uncertainty visualizations often emphasize point estimates to support magnitude estimates or decisions through visual comparison. However, when design choices emphasize means, users may overlook uncertainty information and misinterpret visual distance as a proxy for effect size. We present findings from a mixed design experiment on Mechanical Turk which tests eight uncertainty visualization designs: 95% containment intervals, hypothetical outcome plots, densities, and quantile dotplots, each with and without means added. We find that adding means to uncertainty visualizations has small biasing effects on both magnitude estimation and decision-making, consistent with discounting uncertainty. We also see that visualization designs that support the least biased effect size estimation do not support the best decision-making, suggesting that a chart user\u2019s sense of effect size may not necessarily be identical when they use the same information for different tasks. In a qualitative analysis of users\u2019 strategy descriptions, we find that many users switch strategies and do not employ an optimal strategy when one exists. Uncertainty visualizations which are optimally designed in theory may not be the most effective in practice because of the ways that users satisfice with heuristics, suggesting opportunities to better understand visualization effectiveness by modeling sets of potential strategies.", "keywords": ["use", "scale", "density", "adding", "order", "user", "effect", "model", "uncertainty", "utility", "decision", "participant", "player", "frame", "evidence", "strategy", "size", "value", "information", "fig", "estimate", "work", "task", "distance", "difference", "visualization", "probability", "based", "superiority", "variance", "bias", "data", "making", "rely", "mean", "chart", "response", "distribution", "design", "outcome", "interval", "hop", "dotplots"], "document_vector": [23.256935, 26.204404], "paragraphs": [{"paragraph_vector": [111.182678, 54.501697], "paragraph_keywords": ["uncertainty", "estimates", "information", "visualization"]}, {"paragraph_vector": [109.470436, 55.14487], "paragraph_keywords": ["users", "means", "distance", "visualization"]}, {"paragraph_vector": [110.421882, 56.230941], "paragraph_keywords": ["uncertainty", "users", "visualization", "analysis"]}, {"paragraph_vector": [107.102745, 54.951808], "paragraph_keywords": ["uncertainty", "effect", "size", "axis"]}, {"paragraph_vector": [124.255851, 63.161705], "paragraph_keywords": ["player", "win", "effect", "size"]}, {"paragraph_vector": [118.490074, 60.725032], "paragraph_keywords": ["participants", "account", "value", "bonus"]}, {"paragraph_vector": [119.581855, 63.866115], "paragraph_keywords": ["decisions", "decision", "cost", "problems"]}, {"paragraph_vector": [116.698989, 59.884262], "paragraph_keywords": ["users", "effect", "size", "k"]}, {"paragraph_vector": [112.397003, 52.813289], "paragraph_keywords": ["uncertainty", "intervals", "probability", "means"]}, {"paragraph_vector": [102.423156, 53.273738], "paragraph_keywords": ["probability", "represented", "ieee", "probabilities"]}, {"paragraph_vector": [107.91674, 55.059017], "paragraph_keywords": ["distributions", "effect", "f", "player"]}, {"paragraph_vector": [118.128524, 64.18798], "paragraph_keywords": ["model", "responses", "models", "probability"]}, {"paragraph_vector": [116.322456, 57.615463], "paragraph_keywords": ["effects", "order", "model", "probability"]}, {"paragraph_vector": [116.128593, 59.642307], "paragraph_keywords": ["model", "scale", "evidence", "use"]}, {"paragraph_vector": [115.498229, 60.75402], "paragraph_keywords": ["user", "log", "evidence", "model"]}, {"paragraph_vector": [112.6259, 55.193038], "paragraph_keywords": ["users", "participants", "strategies", "charts"]}, {"paragraph_vector": [107.53408, 52.130199], "paragraph_keywords": ["hops", "intervals", "densities", "e"]}, {"paragraph_vector": [108.172164, 54.103813], "paragraph_keywords": ["means", "hops", "uncertainty", "slope"]}, {"paragraph_vector": [105.088645, 54.60773], "paragraph_keywords": ["means", "jnds", "percentage", "effect"]}, {"paragraph_vector": [102.526641, 53.896583], "paragraph_keywords": ["users", "means", "distributions", "effect"]}, {"paragraph_vector": [114.213775, 57.963855], "paragraph_keywords": ["variance", "means", "users", "visualization"]}, {"paragraph_vector": [120.882102, 60.093868], "paragraph_keywords": ["making", "decision", "effect", "distributions"]}, {"paragraph_vector": [104.444778, 53.877571], "paragraph_keywords": ["distributions", "users", "distance", "use"]}, {"paragraph_vector": [125.715286, 37.995098], "paragraph_keywords": ["users", "frames", "hops", "strategies"]}, {"paragraph_vector": [103.052406, 53.013057], "paragraph_keywords": ["users", "rely", "hops", "strategy"]}, {"paragraph_vector": [99.119789, 52.268199], "paragraph_keywords": ["users", "means", "cue", "relying"]}, {"paragraph_vector": [109.025527, 53.425601], "paragraph_keywords": ["means", "design", "variance", "strategies"]}, {"paragraph_vector": [111.264762, 56.6035], "paragraph_keywords": ["rely", "users", "means", "strategies"]}, {"paragraph_vector": [116.294342, 63.328208], "paragraph_keywords": ["users", "decision", "visualization", "task"]}, {"paragraph_vector": [159.003784, 43.283145], "paragraph_keywords": ["strategies", "users", "visualization", "models"]}, {"paragraph_vector": [151.688476, 42.1734], "paragraph_keywords": ["visualization", "user", "use", "users"]}], "content": {}, "doi": "10.1109/TVCG.2020.3030422"}, {"uri": "58", "title": "Multiscale Snapshots: Visual Analysis of Temporal Summaries in Dynamic Graphs", "timestamp": "2020", "rating": "0.0", "annotation": "", "tags": [], "authors": ["Eren Cakmak", "Udo Schlegel", "Dominik J\u00e4ckle", "Daniel Keim", "Tobias Schreck"], "summary": "The overview-driven visual analysis of large-scale dynamic graphs poses a major challenge. We propose Multiscale Snapshots, a visual analytics approach to analyze temporal summaries of dynamic graphs at multiple temporal scales. First, we recursively generate temporal summaries to abstract overlapping sequences of graphs into compact snapshots. Second, we apply graph embeddings to the snapshots to learn low-dimensional representations of each sequence of graphs to speed up specific analytical tasks (e.g., similarity search). Third, we visualize the evolving data from a coarse to fine-granular snapshots to semi-automatically analyze temporal states, trends, and outliers. The approach enables us to discover similar temporal summaries (e.g., reoccurring states), reduces the temporal data to speed up automatic analysis, and to explore both structural and temporal properties of a dynamic graph. We demonstrate the usefulness of our approach by a quantitative evaluation and the application to a real-world dataset.", "keywords": ["use", "property", "scale", "metaphor", "user", "graph", "node", "search", "research", "number", "snapshot", "level", "context", "identify", "allows", "approach", "similarity", "work", "union", "election", "visualization", "hierarchy", "data", "change", "view", "method", "time", "subreddits", "result", "period", "embeddings", "event", "abstraction", "overview", "interval", "analysis", "example", "analyst", "instance", "evolving"], "document_vector": [-169.261688, 12.40666], "paragraphs": [{"paragraph_vector": [-44.017391, -43.66724], "paragraph_keywords": ["analysis", "methods", "data", "graphs"]}, {"paragraph_vector": [-42.552646, -45.979862], "paragraph_keywords": ["graph", "snapshots", "approach", "use"]}, {"paragraph_vector": [-47.029766, -41.973613], "paragraph_keywords": ["graph", "analysis", "visualization", "scale"]}, {"paragraph_vector": [-47.649066, -46.910247], "paragraph_keywords": ["abstraction", "methods", "graphs", "data"]}, {"paragraph_vector": [-43.357311, -41.636405], "paragraph_keywords": ["graphs", "scale", "levels", "abstraction"]}, {"paragraph_vector": [-45.294456, -49.879817], "paragraph_keywords": ["graph", "graphs", "overview", "data"]}, {"paragraph_vector": [-45.626674, -42.47956], "paragraph_keywords": ["graph", "time", "visualizations", "approaches"]}, {"paragraph_vector": [-39.989482, -41.236648], "paragraph_keywords": ["allows", "visualization", "overview", "graph"]}, {"paragraph_vector": [-46.199779, -48.750297], "paragraph_keywords": ["time", "abstraction", "intervals", "graph"]}, {"paragraph_vector": [-3.62827, -24.445138], "paragraph_keywords": ["intervals", "graph", "level", "time"]}, {"paragraph_vector": [-39.420578, -43.7135], "paragraph_keywords": ["interval", "graph", "nodes", "union"]}, {"paragraph_vector": [-44.403293, -39.534076], "paragraph_keywords": ["graphs", "graph", "snapshots", "embeddings"]}, {"paragraph_vector": [-39.439876, -43.970336], "paragraph_keywords": ["snapshots", "graph", "embeddings", "level"]}, {"paragraph_vector": [-41.764255, -45.068412], "paragraph_keywords": ["graph", "view", "metaphors", "nodes"]}, {"paragraph_vector": [-40.672306, -45.094261], "paragraph_keywords": ["views", "number", "displayed", "levels"]}, {"paragraph_vector": [-41.031589, -43.336631], "paragraph_keywords": ["levels", "snapshots", "map", "view"]}, {"paragraph_vector": [-39.581211, -43.289253], "paragraph_keywords": ["context", "methods", "graph", "ieee"]}, {"paragraph_vector": [-40.340286, -42.530185], "paragraph_keywords": ["graph", "snapshots", "graphs", "analyst"]}, {"paragraph_vector": [-51.604755, -43.90866], "paragraph_keywords": ["graph", "subreddits", "events", "graphs"]}, {"paragraph_vector": [-44.920459, -43.272232], "paragraph_keywords": ["election", "week", "subreddits", "nodes"]}, {"paragraph_vector": [-50.024307, -41.694984], "paragraph_keywords": ["graph", "level", "snapshots", "time"]}, {"paragraph_vector": [-46.983188, -44.282283], "paragraph_keywords": ["events", "analyst", "subreddits", "periods"]}, {"paragraph_vector": [-53.507614, -43.061004], "paragraph_keywords": ["subreddits", "events", "ieee", "analyst"]}, {"paragraph_vector": [-45.402111, -41.813083], "paragraph_keywords": ["graph", "graphs", "nodes", "search"]}, {"paragraph_vector": [-41.91209, -40.931945], "paragraph_keywords": ["graphs", "embeddings", "graph", "similarity"]}, {"paragraph_vector": [-41.309192, -42.970603], "paragraph_keywords": ["graph", "accuracy", "results", "world"]}, {"paragraph_vector": [-42.525394, -41.713096], "paragraph_keywords": ["embeddings", "graph", "methods", "use"]}, {"paragraph_vector": [-38.039463, -48.172687], "paragraph_keywords": ["methods", "snapshots", "time", "graph"]}, {"paragraph_vector": [-45.701885, -45.055706], "paragraph_keywords": ["graph", "approach", "snapshots", "methods"]}, {"paragraph_vector": [-45.128139, -43.446804], "paragraph_keywords": ["approach", "graph", "snapshots", "analysis"]}], "content": {}, "doi": "10.1109/TVCG.2020.3030343"}, {"uri": "59", "title": "PassVizor: Toward Better Understanding of the Dynamics of Soccer Passes", "timestamp": "2020", "rating": "0.0", "annotation": "", "tags": [], "authors": ["Xiao Xie", "Jiachen Wang", "Hongye Liang", "Dazhen Deng", "Shoubin Cheng", "Hui Zhang", "Wei Chen", "Yingcai Wu"], "summary": "In soccer, passing is the most frequent interaction between players and plays a significant role in creating scoring chances. Experts are interested in analyzing players\u2019 passing behavior to learn passing tactics, i.e., how players build up an attack with passing. Various approaches have been proposed to facilitate the analysis of passing tactics. However, the dynamic changes of a team\u2019s employed tactics over a match have not been comprehensively investigated. To address the problem, we closely collaborate with domain experts and characterize requirements to analyze the dynamic changes of a team\u2019s passing tactics. To characterize the passing tactic employed for each attack, we propose a topic-based approach that provides a high-level abstraction of complex passing behaviors. Based on the model, we propose a glyph-based design to reveal the multi-variate information of passing tactics within different phases of attacks, including player identity, spatial context, and formation. We further design and develop PassVizor, a visual analytics system, to support the comprehensive analysis of passing dynamics. With the system, users can detect the changing patterns of passing tactics and examine the detailed passing process for evaluating passing tactics. We invite experts to conduct analysis with PassVizor and demonstrate the usability of the system through an expert interview.", "keywords": ["use", "found", "region", "pattern", "attack", "user", "system", "sport", "pass", "number", "player", "network", "-", "information", "context", "fig", "expert", "ball", "process", "defense", "phase", "encode", "modeling", "glyph", "according", "soccer", "topic", "counter", "help", "argentina", "based", "position", "data", "sequence", "passvizor", "detection", "time", "method", "result", "behavior", "match", "passing", "design", "team", "set", "analysis", "shooting", "example", "pas"], "document_vector": [48.748271, -46.6921], "paragraphs": [{"paragraph_vector": [83.00122, 6.440512], "paragraph_keywords": ["passing", "patterns", "soccer", "analysis"]}, {"paragraph_vector": [82.181388, 5.257378], "paragraph_keywords": ["passing", "soccer", "-", "designs"]}, {"paragraph_vector": [79.781585, 6.413123], "paragraph_keywords": ["passing", "network", "players", "spain"]}, {"paragraph_vector": [79.0475, 11.415537], "paragraph_keywords": ["passing", "patterns", "sequence", "data"]}, {"paragraph_vector": [83.23442, 5.576014], "paragraph_keywords": ["players", "soccer", "phases", "al"]}, {"paragraph_vector": [82.09214, 7.458327], "paragraph_keywords": ["passing", "experts", "soccer", "behaviors"]}, {"paragraph_vector": [78.75959, 8.296815], "paragraph_keywords": ["passing", "behaviors", "players", "experts"]}, {"paragraph_vector": [81.272354, 8.143757], "paragraph_keywords": ["data", "passing", "event", "users"]}, {"paragraph_vector": [75.915229, 3.470387], "paragraph_keywords": ["passing", "phase", "passes", "ball"]}, {"paragraph_vector": [83.597625, 7.031597], "paragraph_keywords": ["passing", "pattern", "detection", "data"]}, {"paragraph_vector": [79.598007, 6.635124], "paragraph_keywords": ["passing", "players", "player", "pattern"]}, {"paragraph_vector": [79.892333, 8.644909], "paragraph_keywords": ["soccer", "players", "distribution", "topic"]}, {"paragraph_vector": [79.269706, 6.928208], "paragraph_keywords": ["passing", "word", "players", "player"]}, {"paragraph_vector": [79.54821, 6.634257], "paragraph_keywords": ["passing", "patterns", "pattern", "phases"]}, {"paragraph_vector": [82.424659, 6.34771], "paragraph_keywords": ["passing", "fig", "phase", "pattern"]}, {"paragraph_vector": [82.249374, 3.019977], "paragraph_keywords": ["use", "fig", "multivariate", "users"]}, {"paragraph_vector": [82.761665, 5.588418], "paragraph_keywords": ["users", "fig", "encode", "lines"]}, {"paragraph_vector": [82.74131, 6.160801], "paragraph_keywords": ["passing", "users", "fig", "design"]}, {"paragraph_vector": [79.646385, 6.178601], "paragraph_keywords": ["players", "player", "pass", "analysis"]}, {"paragraph_vector": [78.719284, 6.60738], "paragraph_keywords": ["experts", "passing", "patterns", "argentina"]}, {"paragraph_vector": [80.27053, 6.398649], "paragraph_keywords": ["experts", "patterns", "fig", "passing"]}, {"paragraph_vector": [77.664916, 8.604844], "paragraph_keywords": ["counter", "passing", "attack", "-"]}, {"paragraph_vector": [79.992477, 7.54923], "paragraph_keywords": ["attack", "counter", "-", "fig"]}, {"paragraph_vector": [79.379882, 6.716909], "paragraph_keywords": ["argentina", "experts", "pattern", "shooting"]}, {"paragraph_vector": [81.219169, 7.661327], "paragraph_keywords": ["experts", "phase", "passing", "analysis"]}, {"paragraph_vector": [81.969726, 6.568181], "paragraph_keywords": ["passing", "soccer", "network", "phases"]}, {"paragraph_vector": [82.789642, 8.015083], "paragraph_keywords": ["matches", "passing", "patterns", "detection"]}, {"paragraph_vector": [79.162818, 7.305979], "paragraph_keywords": ["topic", "topics", "soccer", "passing"]}, {"paragraph_vector": [82.798461, 5.681827], "paragraph_keywords": ["passing", "defense", "behaviors", "regions"]}, {"paragraph_vector": [82.606246, 5.343801], "paragraph_keywords": ["passing", "zhejiang", "soccer", "analysis"]}], "content": {}, "doi": "10.1109/TVCG.2020.3030366"}, {"uri": "60", "title": "HypoML: Visual Analysis for Hypothesis-based Evaluation of Machine Learning Models", "timestamp": "2020", "rating": "0.0", "annotation": "", "tags": [], "authors": ["Qianwen Wang", "William Alexander", "Jack Pegg", "Huamin Qu", "Min Chen"], "summary": "In this paper, we present a visual analytics tool for enabling hypothesis-based evaluation of machine learning (ML) models. We describe a novel ML-testing framework that combines the traditional statistical hypothesis testing (commonly used in empirical research) with logical reasoning about the conclusions of multiple hypotheses. The framework defines a controlled configuration for testing a number of hypotheses as to whether and how some extra information about a \u201cconcept\u201d or \u201cfeature\u201d may benefit or hinder an ML model. Because reasoning multiple hypotheses is not always straightforward, we provide HypoML as a visual analysis tool, with which, the multi-thread testing results are first transformed to analytical results using statistical and logical inferences, and then to a visual representation for rapid observation of the conclusions and the logical flow between the testing results and hypotheses. We have applied HypoML to a number of hypothesized concepts, demonstrating the intuitive and explainable nature of the visual analysis.", "keywords": ["use", "image", "concept", "feature", "rm", "ml", "user", "comparison", "cnn", "model", "learning", "testing", "test", "learned", "object", "value", "information", "accuracy", "hypoml", "p", "fig", "process", "m", "work", "developer", "dataset", "input", "visualization", "conclusion", "based", "template", "data", "method", "time", "result", "inference", "training", "rotation", "hypothesis", "encoding", "box", "specialist", "design", "d", "set", "analysis", "source", "experiment"], "document_vector": [-32.963096, 8.965996], "paragraphs": [{"paragraph_vector": [-20.635221, 64.307754], "paragraph_keywords": ["features", "ml", "model", "learning"]}, {"paragraph_vector": [-43.344795, 59.137573], "paragraph_keywords": ["analysis", "neurons", "use", "developers"]}, {"paragraph_vector": [-70.434951, 59.868007], "paragraph_keywords": ["ml", "model", "visualization", "process"]}, {"paragraph_vector": [-45.148639, 59.356151], "paragraph_keywords": ["models", "analysis", "visualization", "tools"]}, {"paragraph_vector": [-50.291088, 58.225414], "paragraph_keywords": ["model", "concept", "concepts", "plots"]}, {"paragraph_vector": [-35.695419, 53.258003], "paragraph_keywords": ["data", "testing", "model", "having"]}, {"paragraph_vector": [-51.809818, 58.614406], "paragraph_keywords": ["data", "testing", "model", "training"]}, {"paragraph_vector": [-67.056373, 63.753265], "paragraph_keywords": ["hypotheses", "model", "testing", "analysis"]}, {"paragraph_vector": [-44.08871, 58.707065], "paragraph_keywords": ["m", "data", "model", "information"]}, {"paragraph_vector": [-53.018768, 60.024143], "paragraph_keywords": ["data", "label", "fusion", "value"]}, {"paragraph_vector": [-65.618461, 56.498302], "paragraph_keywords": ["d", "rm", "accuracy", "ra"]}, {"paragraph_vector": [-69.68312, 62.781642], "paragraph_keywords": ["analysis", "rules", "test", "ieee"]}, {"paragraph_vector": [-42.68988, 59.179767], "paragraph_keywords": ["analysis", "conclusion", "rm", "conclude"]}, {"paragraph_vector": [-55.608135, 60.559822], "paragraph_keywords": ["analysis", "step", "template", "rm"]}, {"paragraph_vector": [-69.139381, 59.896774], "paragraph_keywords": ["analysis", "hypotheses", "developers", "fig"]}, {"paragraph_vector": [143.414215, -60.324394], "paragraph_keywords": ["encoding", "value", "visualization", "indicate"]}, {"paragraph_vector": [-63.848335, 65.21994], "paragraph_keywords": ["design", "users", "area", "p"]}, {"paragraph_vector": [-49.556076, 66.674621], "paragraph_keywords": ["hypoml", "value", "users", "use"]}, {"paragraph_vector": [-58.387207, 54.353813], "paragraph_keywords": ["data", "testing", "model", "cnn"]}, {"paragraph_vector": [9.565726, 58.137779], "paragraph_keywords": ["data", "d", "use", "p"]}, {"paragraph_vector": [10.256224, 60.463256], "paragraph_keywords": ["rotation", "use", "model", "ieee"]}, {"paragraph_vector": [9.176342, 59.222511], "paragraph_keywords": ["rotation", "information", "images", "color"]}, {"paragraph_vector": [-34.921638, 64.96923], "paragraph_keywords": ["rgb", "d", "hsv", "layer"]}, {"paragraph_vector": [-51.807582, 60.548458], "paragraph_keywords": ["ml", "shown", "c", "team"]}, {"paragraph_vector": [-61.571983, 60.367824], "paragraph_keywords": ["testing", "use", "hypothesis", "methods"]}, {"paragraph_vector": [-40.54013, 58.159854], "paragraph_keywords": ["model", "specialists", "models", "ml"]}, {"paragraph_vector": [-52.339809, 59.632266], "paragraph_keywords": ["testing", "hypotheses", "framework", "plots"]}, {"paragraph_vector": [83.817504, -34.567459], "paragraph_keywords": ["models", "ieee", "understanding", "use"]}], "content": {}, "doi": "10.1109/TVCG.2020.3030471"}, {"uri": "61", "title": "Staged Animation Strategies for Online Dynamic Networks", "timestamp": "2020", "rating": "0.0", "annotation": "", "tags": [], "authors": ["Tarik Crnovrsanin", "Senthil Chandrasegaran", "Kwan-Liu Ma"], "summary": "Dynamic networks\u2014networks that change over time\u2014can be categorized into two types: offline dynamic networks, where all states of the network are known, and online dynamic networks, where only the past states of the network are known. Research on staging animated transitions in dynamic networks has focused more on offline data, where rendering strategies can take into account past and future states of the network. Rendering online dynamic networks is a more challenging problem since it requires a balance between timeliness for monitoring tasks\u2014so that the animations do not lag too far behind the events\u2014and clarity for comprehension tasks\u2014to minimize simultaneous changes that may be difficult to follow. To illustrate the challenges placed by these requirements, we explore three strategies to stage animations for online dynamic networks: time-based, event-based, and a new hybrid approach that we introduce by combining the advantages of the first two. We illustrate the advantages and disadvantages of each strategy in representing lowand high-throughput data and conduct a user study involving monitoring and comprehension of dynamic networks. We also conduct a follow-up, think-aloud study combining monitoring and comprehension with experts in dynamic network visualization. Our findings show that animation staging strategies that emphasize comprehension do better for participant response times and accuracy. However, the notion of \u201ccomprehension\u201d is not always clear when it comes to complex changes in highly dynamic networks, requiring some iteration in staging that the hybrid approach affords. Based on our results, we make recommendations for balancing event-based and time-based parameters for our hybrid approach.", "keywords": ["use", "order", "user", "graph", "us", "animation", "staging", "node", "number", "participant", "network", "state", "technique", "al", "strategy", "perception", "occur", "information", "staged", "approach", "space", "comprehension", "work", "layout", "task", "algorithm", "map", "movement", "visualization", "edge", "based", "shown", "rate", "video", "data", "stage", "change", "time", "et", "result", "response", "given", "monitoring", "event", "design", "condition", "addition", "analysis", "study", "instance", "track"], "document_vector": [-177.270156, 7.066431], "paragraphs": [{"paragraph_vector": [-53.340763, -51.085189], "paragraph_keywords": ["network", "time", "networks", "changes"]}, {"paragraph_vector": [-103.339073, -52.535358], "paragraph_keywords": ["time", "events", "changes", "network"]}, {"paragraph_vector": [-122.245658, -61.781875], "paragraph_keywords": ["time", "strategy", "staging", "animation"]}, {"paragraph_vector": [-110.761444, -63.266254], "paragraph_keywords": ["time", "animation", "strategies", "network"]}, {"paragraph_vector": [-56.619461, -60.699779], "paragraph_keywords": ["time", "based", "communities", "states"]}, {"paragraph_vector": [-72.017982, -64.34085], "paragraph_keywords": ["graphs", "time", "layouts", "network"]}, {"paragraph_vector": [-58.873031, -65.516929], "paragraph_keywords": ["time", "changes", "network", "networks"]}, {"paragraph_vector": [-55.272224, -64.681091], "paragraph_keywords": ["layouts", "network", "graphs", "address"]}, {"paragraph_vector": [-61.542148, -63.561763], "paragraph_keywords": ["technique", "use", "graph", "edge"]}, {"paragraph_vector": [-94.191375, -73.608482], "paragraph_keywords": ["time", "objects", "staging", "user"]}, {"paragraph_vector": [-73.316619, -59.855117], "paragraph_keywords": ["time", "changes", "needs", "graph"]}, {"paragraph_vector": [-122.886795, -63.048206], "paragraph_keywords": ["event", "animation", "time", "network"]}, {"paragraph_vector": [-118.157951, -61.157199], "paragraph_keywords": ["time", "animation", "staging", "based"]}, {"paragraph_vector": [-119.959205, -63.676994], "paragraph_keywords": ["events", "animation", "staging", "time"]}, {"paragraph_vector": [-129.140792, -62.803752], "paragraph_keywords": ["time", "event", "events", "based"]}, {"paragraph_vector": [-103.268135, -62.003486], "paragraph_keywords": ["time", "use", "study", "animation"]}, {"paragraph_vector": [-63.57875, -63.319541], "paragraph_keywords": ["time", "layout", "nodes", "steps"]}, {"paragraph_vector": [-112.256546, -62.749256], "paragraph_keywords": ["participants", "study", "tasks", "students"]}, {"paragraph_vector": [-124.409652, -62.111156], "paragraph_keywords": ["tasks", "video", "data", "proximity"]}, {"paragraph_vector": [-123.386039, -61.194786], "paragraph_keywords": ["tasks", "use", "event", "time"]}, {"paragraph_vector": [-103.809623, -48.006652], "paragraph_keywords": ["network", "staging", "nodes", "attention"]}, {"paragraph_vector": [-131.384429, -62.366012], "paragraph_keywords": ["tasks", "condition", "video", "participants"]}, {"paragraph_vector": [-134.920883, -62.623649], "paragraph_keywords": ["time", "event", "staging", "participant"]}, {"paragraph_vector": [-130.761886, -61.695655], "paragraph_keywords": ["staging", "time", "participant", "tasks"]}, {"paragraph_vector": [-121.106262, -58.162265], "paragraph_keywords": ["network", "load", "visualization", "event"]}, {"paragraph_vector": [-122.685958, -61.781272], "paragraph_keywords": ["participants", "network", "clips", "shown"]}, {"paragraph_vector": [-120.732521, -61.114986], "paragraph_keywords": ["network", "number", "animation", "events"]}, {"paragraph_vector": [-119.020317, -61.765251], "paragraph_keywords": ["events", "time", "animation", "interval"]}, {"paragraph_vector": [-128.109436, -65.610336], "paragraph_keywords": ["based", "event", "time", "events"]}, {"paragraph_vector": [-120.980026, -62.60136], "paragraph_keywords": ["tasks", "task", "node", "monitoring"]}, {"paragraph_vector": [-120.769325, -60.982952], "paragraph_keywords": ["based", "event", "participants", "changes"]}, {"paragraph_vector": [-123.392997, -64.134796], "paragraph_keywords": ["staging", "animation", "comprehension", "time"]}, {"paragraph_vector": [-116.276626, -63.105278], "paragraph_keywords": ["strategy", "event", "strategies", "ieee"]}], "content": {}, "doi": "10.1109/TVCG.2020.3030335"}, {"uri": "62", "title": "Competing Models: Inferring Exploration Patterns and Information Relevance via Bayesian Model Selection", "timestamp": "2020", "rating": "0.0", "annotation": "", "tags": [], "authors": ["Shayan Monadjemi", "Roman Garnett", "Alvitta Ottley"], "summary": "Analyzing interaction data provides an opportunity to learn about users, uncover their underlying goals, and create intelligent visualization systems. The first step for intelligent response in visualizations is to enable computers to infer user goals and strategies through observing their interactions with a system. Researchers have proposed multiple techniques to model users, however, their frameworks often depend on the visualization design, interaction space, and dataset. Due to these dependencies, many techniques do not provide a general algorithmic solution to user exploration modeling. In this paper, we construct a series of models based on the dataset and pose user exploration modeling as a Bayesian model selection problem where we maintain a belief over numerous competing models that could explain user interactions. Each of these competing models represent an exploration strategy the user could adopt during a session. The goal of our technique is to make high-level and in-depth inferences about the user by observing their low-level interactions. Although our proposed idea is applicable to various probabilistic model spaces, we demonstrate a specific instance of encoding exploration patterns as competing models to infer information relevance. We validate our technique\u2019s ability to infer exploration bias, predict future interactions, and summarize an analytic session using user study datasets. Our results indicate that depending on the application, our method outperforms established baselines for bias detection and future interaction prediction. Finally, we discuss future research directions based on our proposed modeling paradigm and suggest how practitioners can use this method to build intelligent visualization systems that understand users\u2019 goals and adapt to improve the exploration process.", "keywords": ["exploration", "use", "provenance", "order", "user", "type", "model", "attribute", "technique", "al", "session", "function", "level", "information", "dimension", "interaction", "space", "approach", "location", "section", "work", "task", "modeling", "dataset", "map", "visualization", "probability", "based", "bias", "data", "et", "time", "point", "given", "likelihood", "distribution", "set", "analysis", "study", "click", "belief"], "document_vector": [34.118431, 35.289821], "paragraphs": [{"paragraph_vector": [-153.575393, 21.375444], "paragraph_keywords": ["data", "user", "portions", "system"]}, {"paragraph_vector": [-146.822128, 19.661146], "paragraph_keywords": ["user", "exploration", "models", "dimensions"]}, {"paragraph_vector": [-146.297973, 19.433864], "paragraph_keywords": ["modeling", "data", "provenance", "exploration"]}, {"paragraph_vector": [-154.81137, 17.548936], "paragraph_keywords": ["user", "data", "users", "visualization"]}, {"paragraph_vector": [-145.382537, 18.639848], "paragraph_keywords": ["bias", "data", "user", "models"]}, {"paragraph_vector": [-142.666824, 17.810703], "paragraph_keywords": ["level", "bias", "user", "events"]}, {"paragraph_vector": [-147.631774, 19.553298], "paragraph_keywords": ["data", "users", "provenance", "exploration"]}, {"paragraph_vector": [-148.044281, 19.65549], "paragraph_keywords": ["interaction", "user", "data", "attributes"]}, {"paragraph_vector": [-149.027954, 24.458963], "paragraph_keywords": ["exploration", "data", "models", "user"]}, {"paragraph_vector": [-146.110427, 20.736642], "paragraph_keywords": ["models", "model", "belief", "interactions"]}, {"paragraph_vector": [-144.375732, 23.754606], "paragraph_keywords": ["data", "user", "use", "models"]}, {"paragraph_vector": [-144.53572, 25.349565], "paragraph_keywords": ["distribution", "parameters", "dimensions", "distributions"]}, {"paragraph_vector": [-145.975265, 25.610383], "paragraph_keywords": ["probability", "parameter", "k", "mi"]}, {"paragraph_vector": [-146.661422, 22.839284], "paragraph_keywords": ["bias", "user", "models", "c"]}, {"paragraph_vector": [-146.414352, 20.907213], "paragraph_keywords": ["time", "distribution", "summary", "clicks"]}, {"paragraph_vector": [-144.011138, 21.661964], "paragraph_keywords": ["location", "attributes", "type", "exploration"]}, {"paragraph_vector": [-144.476638, 19.902444], "paragraph_keywords": ["user", "visualization", "dataset", "interaction"]}, {"paragraph_vector": [-141.138748, 18.052227], "paragraph_keywords": ["based", "crime", "cases", "map"]}, {"paragraph_vector": [-143.067977, 19.395862], "paragraph_keywords": ["bias", "baseline", "attribute", "distribution"]}, {"paragraph_vector": [-147.903884, 19.989433], "paragraph_keywords": ["user", "baseline", "prediction", "technique"]}, {"paragraph_vector": [-142.547653, 8.035799], "paragraph_keywords": ["visualization", "data", "board", "use"]}, {"paragraph_vector": [-143.705978, 20.359588], "paragraph_keywords": ["model", "hovers", "attributes", "study"]}, {"paragraph_vector": [-140.531066, 16.736886], "paragraph_keywords": ["model", "user", "clicks", "use"]}, {"paragraph_vector": [-147.387832, 19.219295], "paragraph_keywords": ["data", "exploration", "learn", "users"]}, {"paragraph_vector": [-147.125579, 19.734516], "paragraph_keywords": ["model", "human", "work", "learning"]}, {"paragraph_vector": [-149.904876, 21.666587], "paragraph_keywords": ["models", "data", "interactions", "level"]}, {"paragraph_vector": [-105.928291, 68.918395], "paragraph_keywords": ["ieee", "use", "republication", "redistribution"]}], "content": {}, "doi": "10.1109/TVCG.2020.3030396"}, {"uri": "63", "title": "A Survey of Text Alignment Visualization", "timestamp": "2020", "rating": "0.0", "annotation": "", "tags": [], "authors": ["Tariq Yousef", "Stefan J\u00e4nicke"], "summary": "Text alignment is one of the fundamental techniques text-related domains like natural language processing, computational linguistics, and digital humanities. It compares two or more texts with each other aiming to find similar textual patterns, or to estimate in general how different or similar the texts are. Visualizing alignment results is an essential task, because it helps researchers getting a comprehensive overview of individual findings and the overall pattern structure. Different approaches have been developed to visualize and help making sense of these patterns depending on text size, alignment methods, and, most importantly, the underlying research tasks demanding for alignment. On the basis of those tasks, we reviewed existing text alignment visualization approaches, and discuss their advantages and drawbacks. We finally derive design implications and shed light on related future challenges.", "keywords": ["use", "score", "passage", "pattern", "order", "user", "graph", "text", "heat", "collation", "number", "support", "technique", "edition", "level", "-", "word", "color", "translation", "process", "unit", "similarity", "work", "task", "token", "variant", "algorithm", "indicate", "table", "fragment", "visualization", "variation", "visualize", "tool", "figure", "form", "shakespeare", "data", "view", "aligned", "representation", "compared", "scenario", "related", "applied", "survey", "align", "example", "alignment"], "document_vector": [172.246368, 37.613182], "paragraphs": [{"paragraph_vector": [-151.976852, -23.718269], "paragraph_keywords": ["alignment", "sequence", "collation", "detect"]}, {"paragraph_vector": [-149.411117, -24.395954], "paragraph_keywords": ["text", "-", "use", "translation"]}, {"paragraph_vector": [-154.386993, -26.433887], "paragraph_keywords": ["text", "alignment", "data", "aligned"]}, {"paragraph_vector": [-154.067092, -25.783584], "paragraph_keywords": ["alignment", "text", "visualization", "survey"]}, {"paragraph_vector": [-149.51741, -24.664798], "paragraph_keywords": ["text", "alignment", "task", "process"]}, {"paragraph_vector": [-152.368515, -22.44148], "paragraph_keywords": ["collation", "text", "alignment", "use"]}, {"paragraph_vector": [-155.685943, 44.977973], "paragraph_keywords": ["text", "shakespeare", "-", "works"]}, {"paragraph_vector": [-150.433731, -25.604215], "paragraph_keywords": ["text", "-", "use", "similarity"]}, {"paragraph_vector": [-148.532272, -25.651296], "paragraph_keywords": ["text", "translation", "translations", "works"]}, {"paragraph_vector": [-148.346145, -26.024415], "paragraph_keywords": ["texts", "alignment", "translation", "text"]}, {"paragraph_vector": [-149.145965, -23.925518], "paragraph_keywords": ["text", "alignment", "units", "ieee"]}, {"paragraph_vector": [-151.64064, -28.643011], "paragraph_keywords": ["alignment", "k", "units", "text"]}, {"paragraph_vector": [-153.14685, -23.629449], "paragraph_keywords": ["text", "variation", "aligned", "units"]}, {"paragraph_vector": [-153.808654, -26.902759], "paragraph_keywords": ["text", "patterns", "alignment", "texts"]}, {"paragraph_vector": [-152.920745, -25.087812], "paragraph_keywords": ["text", "alignment", "aligned", "patterns"]}, {"paragraph_vector": [-152.874526, -26.733671], "paragraph_keywords": ["text", "texts", "use", "corpus"]}, {"paragraph_vector": [-151.131622, -23.2796], "paragraph_keywords": ["text", "texts", "aligned", "words"]}, {"paragraph_vector": [-152.197204, -24.818954], "paragraph_keywords": ["text", "color", "views", "texts"]}, {"paragraph_vector": [-153.628204, -22.605535], "paragraph_keywords": ["text", "word", "view", "visualize"]}, {"paragraph_vector": [-153.770126, -22.108514], "paragraph_keywords": ["graph", "text", "graphs", "nodes"]}, {"paragraph_vector": [-156.252532, -26.022401], "paragraph_keywords": ["alignment", "text", "users", "ieee"]}, {"paragraph_vector": [-154.751342, -27.261575], "paragraph_keywords": ["alignment", "users", "tools", "visualization"]}, {"paragraph_vector": [-152.883758, -24.280996], "paragraph_keywords": ["text", "visualization", "alignment", "techniques"]}, {"paragraph_vector": [-152.503631, -25.502794], "paragraph_keywords": ["visualization", "alignments", "techniques", "table"]}, {"paragraph_vector": [-153.065902, -29.976175], "paragraph_keywords": ["text", "alignment", "visualization", "system"]}, {"paragraph_vector": [-154.17604, -22.474607], "paragraph_keywords": ["text", "alignment", "graph", "editions"]}, {"paragraph_vector": [-154.248855, -26.997329], "paragraph_keywords": ["alignment", "text", "visualizations", "applied"]}], "content": {}, "doi": "10.1109/TVCG.2020.3030399"}, {"uri": "64", "title": "Visual Analytics for Temporal Hypergraph Model Exploration", "timestamp": "2020", "rating": "0.0", "annotation": "", "tags": [], "authors": ["Maximilian T. Fischer", "Devanshu Arya", "Dirk Streeb", "Daniel Seebacher", "Daniel A. Keim", "Marcel Worring"], "summary": "Many processes, from gene interaction in biology to computer networks to social media, can be modeled more precisely as temporal hypergraphs than by regular graphs. This is because hypergraphs generalize graphs by extending edges to connect any number of vertices, allowing complex relationships to be described more accurately and predict their behavior over time. However, the interactive exploration and seamless refinement of such hypergraph-based prediction models still pose a major challenge. We contribute HYPER-MATRIX, a novel visual analytics technique that addresses this challenge through a tight coupling between machine-learning and interactive visualizations. In particular, the technique incorporates a geometric deep learning model as a blueprint for problem-specific models while integrating visualizations for graph-based and category-based data with a novel combination of interactions for an effective user-driven exploration of hypergraph models. To eliminate demanding context switches and ensure scalability, our matrix-based visualization provides drill-down capabilities across multiple levels of semantic zoom, from an overview of model predictions down to the content. We facilitate a focused analysis of relevant connections and groups based on interactive user-steering for filtering and search tasks, a dynamically modifiable partition hierarchy, various matrix reordering techniques, and interactive model feedback. We evaluate our technique in a case study and through formative evaluation with law enforcement experts using real-world internet forum communication data. The results show that our approach surpasses existing solutions in terms of scalability and applicability, enables the incorporation of domain knowledge, and allows for fast search-space traversal. With the proposed technique, we pave the way for the visual analytics of temporal hypergraphs in a wide variety of domains.", "keywords": ["exploration", "feedback", "use", "concept", "user", "graph", "machine", "hypergraph", "showing", "search", "node", "model", "learning", "reordering", "partition", "allow", "technique", "level", "-", "information", "identify", "allows", "cf", "expert", "approach", "work", "matrix", "task", "topic", "visualization", "existing", "based", "connection", "figure", "case", "hypergraphs", "providing", "hierarchy", "data", "enabling", "change", "time", "prediction", "domain", "representation", "communication", "hyperedges", "overview", "set", "analysis", "example", "knowledge", "allowing"], "document_vector": [-154.032333, 9.433223], "paragraphs": [{"paragraph_vector": [-75.202056, -48.560863], "paragraph_keywords": ["relationships", "model", "university", "processes"]}, {"paragraph_vector": [-76.382186, -48.851623], "paragraph_keywords": ["hypergraphs", "learning", "model", "domain"]}, {"paragraph_vector": [-84.998283, -49.159366], "paragraph_keywords": ["model", "domain", "machine", "learning"]}, {"paragraph_vector": [-61.976932, -52.762905], "paragraph_keywords": ["hypergraphs", "diagrams", "change", "link"]}, {"paragraph_vector": [-61.852012, -49.663867], "paragraph_keywords": ["information", "hypergraph", "approaches", "zoom"]}, {"paragraph_vector": [-80.798645, -51.256889], "paragraph_keywords": ["hypergraphs", "model", "data", "described"]}, {"paragraph_vector": [-76.978485, -51.138687], "paragraph_keywords": ["learning", "information", "et", "networks"]}, {"paragraph_vector": [-77.299781, -48.857494], "paragraph_keywords": ["existing", "domain", "analysis", "technique"]}, {"paragraph_vector": [-69.872283, -51.111164], "paragraph_keywords": ["hypergraph", "users", "set", "v"]}, {"paragraph_vector": [-84.854225, -51.346946], "paragraph_keywords": ["model", "matrix", "feedback", "hypergraph"]}, {"paragraph_vector": [-84.892852, -47.54684], "paragraph_keywords": ["j", "model", "feedback", "matrix"]}, {"paragraph_vector": [-82.506828, -50.705844], "paragraph_keywords": ["model", "matrix", "based", "visualization"]}, {"paragraph_vector": [-88.181427, -47.039718], "paragraph_keywords": ["level", "levels", "information", "content"]}, {"paragraph_vector": [-75.004356, -54.682136], "paragraph_keywords": ["levels", "arrow", "screen", "allows"]}, {"paragraph_vector": [143.913452, -64.019325], "paragraph_keywords": ["hierarchy", "partition", "dendrogram", "search"]}, {"paragraph_vector": [74.632247, -73.369087], "paragraph_keywords": ["reordering", "sorting", "matrix", "domain"]}, {"paragraph_vector": [-174.848831, -25.157964], "paragraph_keywords": ["model", "changes", "input", "predictions"]}, {"paragraph_vector": [-88.422599, -49.698398], "paragraph_keywords": ["data", "topics", "analysis", "users"]}, {"paragraph_vector": [66.927932, -78.033859], "paragraph_keywords": ["connections", "users", "topics", "ieee"]}, {"paragraph_vector": [-75.135032, -50.396739], "paragraph_keywords": ["technique", "figure", "comparability", "topics"]}, {"paragraph_vector": [-102.554885, -45.256061], "paragraph_keywords": ["expert", "domain", "model", "allows"]}, {"paragraph_vector": [-107.811164, -40.719772], "paragraph_keywords": ["experts", "minutes", "data", "analysis"]}, {"paragraph_vector": [-79.341354, -9.960699], "paragraph_keywords": ["experts", "based", "approach", "identify"]}, {"paragraph_vector": [-67.342689, -45.918754], "paragraph_keywords": ["cf", "levels", "data", "level"]}, {"paragraph_vector": [178.567932, -58.173408], "paragraph_keywords": ["experts", "concepts", "existing", "cf"]}, {"paragraph_vector": [-106.13105, -12.730206], "paragraph_keywords": ["knowledge", "change", "note", "ieee"]}, {"paragraph_vector": [-110.372589, -41.233543], "paragraph_keywords": ["data", "users", "approach", "topics"]}, {"paragraph_vector": [109.498619, -81.047225], "paragraph_keywords": ["time", "visualization", "use", "overview"]}, {"paragraph_vector": [-82.571006, -49.175102], "paragraph_keywords": ["domain", "knowledge", "models", "model"]}, {"paragraph_vector": [-81.3479, -49.974342], "paragraph_keywords": ["model", "experts", "allowing", "knowledge"]}, {"paragraph_vector": [-97.203887, 67.618743], "paragraph_keywords": ["republication", "redistribution", "requires", "ieee"]}], "content": {}, "doi": "10.1109/TVCG.2020.3028947"}, {"uri": "65", "title": "Modeling in the Time of COVID-19: Statistical and Rule-based Mesoscale Models", "timestamp": "2020", "rating": "0.0", "annotation": "", "tags": [], "authors": ["Ngan Nguyen", "Ond\u0159ej Strnad", "Tobias Klein", "Deng Luo", "Ruwayda Alharbi", "Peter Wonka", "Martina Maritan", "Peter Mindek", "Ludovic Autin", "David S. Goodsell", "Ivan Viola"], "summary": "We present a new technique for the rapid modeling and construction of scientifically accurate mesoscale biological models. The resulting 3D models are based on a few 2D microscopy scans and the latest knowledge available about the biological entity, represented as a set of geometric relationships. Our new visual-programming technique is based on statistical and rule-based modeling approaches that are rapid to author, fast to construct, and easy to revise. From a few 2D microscopy scans, we determine the statistical properties of various structural aspects, such as the outer membrane shape, the spatial properties, and the distribution characteristics of the macromolecular elements on the membrane. This information is utilized in the construction of the 3D model. Once all the imaging evidence is incorporated into the model, additional information can be incorporated by interactively defining the rules that spatially characterize the rest of the biological entity, such as mutual interactions among macromolecules, and their distances and orientations relative to other structures. These rules are defined through an intuitive 3D interactive visualization as a visual-programming feedback loop. We demonstrate the applicability of our approach on a use case of the modeling procedure of the SARS-CoV-2 virion ultrastructure. This atomistic model, which we present here, can steer biological research to new promising directions in our efforts to fight the spread of the virus.", "keywords": ["use", "image", "element", "contour", "user", "mesoscale", "generated", "structure", "model", "system", "virion", "research", "create", "number", "presented", "shape", "placed", "level", "-", "object", "information", "size", "specified", "surface", "process", "membrane", "approach", "modeling", "distance", "algorithm", "assembly", "input", "defined", "based", "created", "figure", "form", "parent", "virus", "rna", "data", "rule", "mesh", "point", "skeleton", "distribution", "applied", "protein", "set", "geometry", "example", "sars", "instance", "specification"], "document_vector": [-107.386711, -60.684356], "paragraphs": [{"paragraph_vector": [23.258554, -29.947059], "paragraph_keywords": ["cells", "organisms", "biomolecules", "form"]}, {"paragraph_vector": [23.600168, -32.40409], "paragraph_keywords": ["modeling", "structure", "models", "data"]}, {"paragraph_vector": [25.090274, -32.841812], "paragraph_keywords": ["models", "modeling", "developed", "information"]}, {"paragraph_vector": [24.68057, -32.676174], "paragraph_keywords": ["modeling", "model", "presented", "user"]}, {"paragraph_vector": [30.209741, -35.221473], "paragraph_keywords": ["modeling", "system", "input", "defined"]}, {"paragraph_vector": [28.70413, -37.507904], "paragraph_keywords": ["model", "modeling", "requires", "assembly"]}, {"paragraph_vector": [33.683361, -38.421573], "paragraph_keywords": ["rules", "modeling", "models", "rule"]}, {"paragraph_vector": [25.288158, -34.078258], "paragraph_keywords": ["membrane", "contours", "images", "information"]}, {"paragraph_vector": [23.594705, -29.597879], "paragraph_keywords": ["surface", "proteins", "contour", "band"]}, {"paragraph_vector": [21.532749, -31.806812], "paragraph_keywords": ["proteins", "contours", "membrane", "ellipse"]}, {"paragraph_vector": [19.410463, 12.803412], "paragraph_keywords": ["contours", "contour", "angle", "points"]}, {"paragraph_vector": [22.909431, -19.151504], "paragraph_keywords": ["contour", "image", "object", "distribution"]}, {"paragraph_vector": [30.021423, -29.461595], "paragraph_keywords": ["mesh", "membrane", "distribution", "contour"]}, {"paragraph_vector": [22.557781, -30.828966], "paragraph_keywords": ["elements", "element", "geometry", "position"]}, {"paragraph_vector": [39.565254, -43.940082], "paragraph_keywords": ["rules", "elements", "model", "rule"]}, {"paragraph_vector": [44.361557, -43.72964], "paragraph_keywords": ["elements", "rules", "user", "element"]}, {"paragraph_vector": [38.782138, -43.686614], "paragraph_keywords": ["parent", "rules", "element", "distance"]}, {"paragraph_vector": [29.569715, -36.515529], "paragraph_keywords": ["rule", "elements", "polygon", "element"]}, {"paragraph_vector": [37.61098, -44.293498], "paragraph_keywords": ["element", "rule", "transformations", "rules"]}, {"paragraph_vector": [25.278606, -32.223564], "paragraph_keywords": ["viruses", "proteins", "displacement", "sars"]}, {"paragraph_vector": [23.704269, -32.035881], "paragraph_keywords": ["membrane", "model", "rna", "virus"]}, {"paragraph_vector": [25.969347, -29.627849], "paragraph_keywords": ["contour", "mesh", "instances", "protein"]}, {"paragraph_vector": [20.947406, -30.670499], "paragraph_keywords": ["protein", "rule", "skeleton", "figure"]}, {"paragraph_vector": [22.522478, -30.527618], "paragraph_keywords": ["model", "relations", "rule", "lipid"]}, {"paragraph_vector": [23.839897, -33.910537], "paragraph_keywords": ["rna", "skeleton", "rule", "line"]}, {"paragraph_vector": [24.883432, -33.673496], "paragraph_keywords": ["model", "models", "information", "modeling"]}, {"paragraph_vector": [24.711166, -32.539936], "paragraph_keywords": ["elements", "rna", "interaction", "membrane"]}, {"paragraph_vector": [24.616807, -33.156402], "paragraph_keywords": ["model", "system", "modeling", "form"]}, {"paragraph_vector": [24.781158, -32.777435], "paragraph_keywords": ["rule", "virion", "based", "specification"]}, {"paragraph_vector": [46.185775, -49.093212], "paragraph_keywords": ["kaust", "research", "ontologies", "shape"]}], "content": {}, "doi": "10.1109/TVCG.2020.3028893"}, {"uri": "66", "title": "MultiSegVA: Using Visual Analytics to Segment Biologging Time Series on Multiple Scales", "timestamp": "2020", "rating": "0.0", "annotation": "", "tags": [], "authors": ["Philipp Meschenmoser", "Juri F. Buchm\u00fcller", "Daniel Seebacher", "Martin Wikelski", "Daniel A. Keim"], "summary": "Segmenting biologging time series of animals on multiple temporal scales is an essential step that requires complex techniques with careful parameterization and possibly cross-domain expertise. Yet, there is a lack of visual-interactive tools that strongly support such multi-scale segmentation. To close this gap, we present our MultiSegVA platform for interactively defining segmentation techniques and parameters on multiple temporal scales. MultiSegVA primarily contributes tailored, visual-interactive means and visual analytics paradigms for segmenting unlabeled time series on multiple scales. Further, to flexibly compose the multi-scale segmentation, the platform contributes a new visual query language that links a variety of segmentation techniques. To illustrate our approach, we present a domain-oriented set of segmentation techniques derived in collaboration with movement ecologists. We demonstrate the applicability and usefulness of MultiSegVA in two real-world use cases from movement ecology, related to behavior analysis after environment-aware segmentation, and after progressive clustering. Expert feedback from movement ecologists shows the effectiveness of tailored visual-interactive means and visual analytics paradigms at segmenting multi-scale data, enabling them to perform semantically meaningful analyses. A third use case demonstrates that MultiSegVA is generalizable to other domains.", "keywords": ["use", "feedback", "scale", "exploration", "range", "feature", "pattern", "operator", "ecologist", "record", "vql", "anomaly", "tree", "segment", "support", "enables", "technique", "index", "series", "multisegva", "level", "segmentation", "value", "-", "dimension", "va", "acceleration", "interaction", "color", "query", "expert", "animal", "m", "section", "show", "parameter", "stripe", "movement", "requirement", "biologging", "visualization", "based", "following", "platform", "data", "window", "change", "time", "building", "domain", "given", "behavior", "point", "application", "set", "analysis", "analyst", "ecology"], "document_vector": [-105.291694, -12.888796], "paragraphs": [{"paragraph_vector": [-2.481427, -23.157783], "paragraph_keywords": ["series", "time", "scales", "segmentation"]}, {"paragraph_vector": [2.192383, -26.350175], "paragraph_keywords": ["scale", "scales", "time", "parameters"]}, {"paragraph_vector": [-2.654163, -21.09473], "paragraph_keywords": ["segmentation", "movement", "section", "domain"]}, {"paragraph_vector": [1.896605, -23.420541], "paragraph_keywords": ["time", "series", "biologging", "data"]}, {"paragraph_vector": [1.724155, -24.47903], "paragraph_keywords": ["requirements", "experts", "time", "data"]}, {"paragraph_vector": [-3.20902, -23.716451], "paragraph_keywords": ["segmentation", "analyst", "time", "series"]}, {"paragraph_vector": [-3.669696, -24.045831], "paragraph_keywords": ["time", "series", "techniques", "approaches"]}, {"paragraph_vector": [-2.458945, -23.664283], "paragraph_keywords": ["segmentation", "segments", "color", "et"]}, {"paragraph_vector": [-3.794778, -24.773862], "paragraph_keywords": ["time", "va", "series", "event"]}, {"paragraph_vector": [-0.153037, -28.03642], "paragraph_keywords": ["segmentation", "time", "analyst", "section"]}, {"paragraph_vector": [-3.449167, -24.425521], "paragraph_keywords": ["analyst", "series", "time", "dimension"]}, {"paragraph_vector": [-4.20655, -24.088937], "paragraph_keywords": ["analyst", "segment", "visualization", "icicle"]}, {"paragraph_vector": [-6.951899, -20.303546], "paragraph_keywords": ["segment", "stripe", "given", "segments"]}, {"paragraph_vector": [61.266113, 47.108123], "paragraph_keywords": ["stripe", "color", "visualization", "labels"]}, {"paragraph_vector": [-3.885478, -22.664493], "paragraph_keywords": ["stripes", "triggers", "anomalies", "analyst"]}, {"paragraph_vector": [-3.876567, -23.339685], "paragraph_keywords": ["time", "line", "density", "series"]}, {"paragraph_vector": [-3.269999, -24.846727], "paragraph_keywords": ["time", "segment", "anomaly", "use"]}, {"paragraph_vector": [-3.732552, -24.175209], "paragraph_keywords": ["map", "query", "technique", "segment"]}, {"paragraph_vector": [-3.49585, -22.362476], "paragraph_keywords": ["technique", "indices", "techniques", "operators"]}, {"paragraph_vector": [-4.774285, -25.696437], "paragraph_keywords": ["query", "building", "technique", "figure"]}, {"paragraph_vector": [-6.681104, -24.856534], "paragraph_keywords": ["operator", "template", "techniques", "operators"]}, {"paragraph_vector": [-8.41081, -25.246122], "paragraph_keywords": ["m", "indices", "operators", "operator"]}, {"paragraph_vector": [-0.865204, -22.612678], "paragraph_keywords": ["vultures", "dimensions", "conditions", "acceleration"]}, {"paragraph_vector": [-2.243542, -23.824054], "paragraph_keywords": ["analyst", "migration", "segment", "altitude"]}, {"paragraph_vector": [-1.070812, -25.410116], "paragraph_keywords": ["cats", "clustering", "hunting", "use"]}, {"paragraph_vector": [0.679685, -23.953283], "paragraph_keywords": ["segments", "time", "series", "segmentation"]}, {"paragraph_vector": [-3.038551, -23.203451], "paragraph_keywords": ["techniques", "use", "movement", "patterns"]}, {"paragraph_vector": [-2.547545, -24.05105], "paragraph_keywords": ["segmentation", "multivariate", "experts", "time"]}, {"paragraph_vector": [-1.030929, -23.044075], "paragraph_keywords": ["platform", "based", "guidance", "expert"]}, {"paragraph_vector": [0.296702, -27.387517], "paragraph_keywords": ["parameters", "technique", "building", "data"]}, {"paragraph_vector": [-1.111518, -23.729928], "paragraph_keywords": ["domain", "multisegva", "techniques", "segmentation"]}, {"paragraph_vector": [-106.856391, 71.3385], "paragraph_keywords": ["ieee", "use", "research", "republication"]}], "content": {}, "doi": "10.1109/TVCG.2020.3028955"}, {"uri": "67", "title": "QualDash: Adaptable Generation of Visualisation Dashboards for Healthcare Quality Improvement", "timestamp": "2020", "rating": "0.0", "annotation": "", "tags": [], "authors": ["Mai Elshehaly", "Rebecca Randell", "Matthew Brehmer", "Lynn McVey", "Natasha Alvarado", "Chris P. Gale", "Roy A. Ruddle"], "summary": "Adapting dashboard design to different contexts of use is an open question in visualisation research. Dashboard designers often seek to strike a balance between dashboard adaptability and ease-of-use, and in hospitals challenges arise from the vast diversity of key metrics, data models and users involved at different organizational levels. In this design study, we present QualDash, a dashboard generation engine that allows for the dynamic configuration and deployment of visualisation dashboards for healthcare quality improvement (QI). We present a rigorous task analysis based on interviews with healthcare professionals, a co-design workshop and a series of one-on-one meetings with front line analysts. From these activities we define a metric card metaphor as a unit of visual analysis in healthcare QI, using this concept as a building block for generating highly adaptable dashboards, and leading to the design of a Metric Specification Structure (MSS). Each MSS is a JSON structure which enables dashboard authors to concisely configure unit-specific variants of a metric card, while offloading common patterns that are shared across cards to be preset by the engine. We reflect on deploying and iterating the design of QualDash in cardiology wards and pediatric intensive care units of five NHS hospitals. Finally, we report evaluation results that demonstrate the adaptability, ease-of-use and usefulness of QualDash in a real-world scenario.", "keywords": ["use", "generation", "included", "hospital", "user", "type", "structure", "display", "number", "allow", "support", "qualcards", "participant", "sub", "field", "measure", "category", "focus", "level", "qualcard", "-", "information", "patient", "engine", "clinician", "context", "activity", "subsidiary", "qualdash", "question", "unit", "healthcare", "space", "section", "task", "card", "mortality", "bar", "visualisation", "include", "figure", "explained", "group", "data", "sequence", "metric", "view", "dashboard", "time", "quality", "chart", "audit", "line", "point", "entry", "design", "site", "set", "ms", "qi", "analyst"], "document_vector": [127.718376, 29.88838], "paragraphs": [{"paragraph_vector": [-56.320873, -8.658911], "paragraph_keywords": ["dashboard", "data", "monitor", "healthcare"]}, {"paragraph_vector": [-55.274314, -10.061963], "paragraph_keywords": ["design", "conducted", "use", "prototype"]}, {"paragraph_vector": [-56.139793, -11.346552], "paragraph_keywords": ["tasks", "section", "typologies", "structure"]}, {"paragraph_vector": [-57.471523, -9.156197], "paragraph_keywords": ["dashboards", "focus", "dashboard", "context"]}, {"paragraph_vector": [-55.061687, -10.573524], "paragraph_keywords": ["data", "use", "audit", "space"]}, {"paragraph_vector": [-55.21154, -9.172747], "paragraph_keywords": ["task", "tasks", "data", "audit"]}, {"paragraph_vector": [-163.733917, 3.176006], "paragraph_keywords": ["task", "participants", "information", "cards"]}, {"paragraph_vector": [-53.959182, -8.88402], "paragraph_keywords": ["tasks", "use", "task", "metrics"]}, {"paragraph_vector": [-59.321754, -6.661475], "paragraph_keywords": ["figure", "metric", "point", "tasks"]}, {"paragraph_vector": [-54.34552, -9.03098], "paragraph_keywords": ["data", "audit", "meetings", "task"]}, {"paragraph_vector": [-53.441532, -7.413937], "paragraph_keywords": ["data", "support", "measures", "qualcard"]}, {"paragraph_vector": [-58.775379, -7.37814], "paragraph_keywords": ["audit", "mss", "engine", "dashboard"]}, {"paragraph_vector": [-58.836128, -9.045036], "paragraph_keywords": ["criteria", "data", "lines", "keys"]}, {"paragraph_vector": [-57.362594, -7.907023], "paragraph_keywords": ["measures", "field", "line", "figure"]}, {"paragraph_vector": [-59.842029, -7.174334], "paragraph_keywords": ["view", "grid", "subsidiary", "tasks"]}, {"paragraph_vector": [-56.965316, -9.967235], "paragraph_keywords": ["charts", "figure", "view", "-"]}, {"paragraph_vector": [-55.168918, -8.153941], "paragraph_keywords": ["chart", "views", "measures", "bar"]}, {"paragraph_vector": [-55.832889, -8.811743], "paragraph_keywords": ["view", "measures", "use", "chart"]}, {"paragraph_vector": [-10.359339, -24.203199], "paragraph_keywords": ["view", "measures", "number", "qualcard"]}, {"paragraph_vector": [-56.041011, -8.777818], "paragraph_keywords": ["qualcards", "-", "qualcard", "sub"]}, {"paragraph_vector": [-54.463329, -10.829424], "paragraph_keywords": ["tasks", "task", "prototype", "group"]}, {"paragraph_vector": [-55.661132, -10.521744], "paragraph_keywords": ["participants", "captured", "feedback", "data"]}, {"paragraph_vector": [-54.933914, -7.232814], "paragraph_keywords": ["data", "mss", "issues", "participants"]}, {"paragraph_vector": [-56.448772, -12.334819], "paragraph_keywords": ["event", "usability", "data", "key"]}, {"paragraph_vector": [-56.086067, -8.089027], "paragraph_keywords": ["data", "qualdash", "audit", "explained"]}, {"paragraph_vector": [-54.18061, -6.476671], "paragraph_keywords": ["data", "site", "qualdash", "upload"]}, {"paragraph_vector": [-56.50027, -4.09727], "paragraph_keywords": ["patients", "medication", "metric", "drugs"]}, {"paragraph_vector": [-55.95417, -5.369851], "paragraph_keywords": ["qualdash", "mss", "tasks", "drugs"]}, {"paragraph_vector": [-56.667778, -6.971211], "paragraph_keywords": ["noted", "deaths", "smr", "pim"]}, {"paragraph_vector": [-55.182533, -9.172378], "paragraph_keywords": ["dashboard", "qualdash", "qi", "mss"]}, {"paragraph_vector": [-56.010723, -10.083881], "paragraph_keywords": ["research", "support", "ieee", "dashboard"]}], "content": {}, "doi": "10.1109/TVCG.2020.3028892"}, {"uri": "68", "title": "Introducing Layers of Meaning (LoM): A Framework to Reduce Semantic Distance of Visualization In Humanistic Research", "timestamp": "2020", "rating": "0.0", "annotation": "", "tags": [], "authors": ["Houda Lamqaddam", "Andrew Vande Moere", "Vero Vanden Abeele", "Koenraad Brosens", "Katrien Verbert"], "summary": "Information visualization (infovis) is a powerful tool for exploring rich datasets. Within humanistic research, rich qualitative data and domain culture make traditional infovis approaches appear reductive and disconnected, leading to low adoption. In this paper, we use a multi-step approach to scrutinize the relationship between infovis and the humanities and suggest new directions for it. We first look into infovis from the humanistic perspective by exploring the humanistic literature around infovis. We validate and expand those findings though a co-design workshop with humanist and infovis experts. Then, we translate our findings into guidelines for designers and conduct a design critique exercise to explore their effect on the perception of humanist researchers. Based on these steps, we introduce Layers of Meaning, a framework to reduce the semantic distance between humanist researchers and visualizations of their research material, by grounding infovis tools in time and space, physicality, terminology, nuance, and provenance.", "keywords": ["found", "element", "art", "impact", "guideline", "workshop", "expert", "section", "distance", "tool", "dh", "perceived", "time", "researcher", "set", "artefact", "instance", "humanity", "conference", "use", "literature", "order", "user", "infovis", "language", "artist", "participant", "field", "framework", "material", "process", "approach", "space", "characteristic", "centered", "history", "need", "include", "datasets", "data", "design", "knowledge", "concept", "research", "ax", "uncertainty", "information", ".", "direction", "expertise", "based", "challenge", "trust", "meaning", "address", "representation", "result", "humanist", "example", "reported", "effect", "described", "model", "theme", "dataset", "including", "visualization", "lack", "collaboration", "practice", "group", "finding", "domain", "university", "designer", "analysis", "study"], "document_vector": [83.022575, 48.586067], "paragraphs": [{"paragraph_vector": [171.666702, 19.114315], "paragraph_keywords": ["data", "research", "material", "discomfort"]}, {"paragraph_vector": [168.034866, 17.457582], "paragraph_keywords": ["data", "research", "information", "hand"]}, {"paragraph_vector": [167.848785, 18.98549], "paragraph_keywords": ["adoption", "tools", "usability", "use"]}, {"paragraph_vector": [169.297225, 16.2315], "paragraph_keywords": ["visualization", "collaborators", "field", "pitfalls"]}, {"paragraph_vector": [169.939743, 14.850784], "paragraph_keywords": ["visualization", "user", "language", "domain"]}, {"paragraph_vector": [165.298324, 14.164957], "paragraph_keywords": ["elements", "data", "effect", "practice"]}, {"paragraph_vector": [165.744583, 20.834573], "paragraph_keywords": ["visualization", "data", "practice", "information"]}, {"paragraph_vector": [166.417694, 16.328176], "paragraph_keywords": ["practice", "researchers", "data", "adoption"]}, {"paragraph_vector": [169.508941, 16.78181], "paragraph_keywords": ["participants", "workshop", "research", "art"]}, {"paragraph_vector": [163.94104, 17.642353], "paragraph_keywords": ["participants", "themes", "data", "domain"]}, {"paragraph_vector": [167.453262, 16.576589], "paragraph_keywords": ["data", "participants", "based", "artists"]}, {"paragraph_vector": [166.389343, 16.675041], "paragraph_keywords": ["data", "representation", "use", "ieee"]}, {"paragraph_vector": [171.714828, 14.530673], "paragraph_keywords": ["data", "models", "example", "novels"]}, {"paragraph_vector": [168.443344, 17.126966], "paragraph_keywords": ["visualizations", "user", "participants", "workshop"]}, {"paragraph_vector": [169.043289, 17.407196], "paragraph_keywords": ["material", "research", "humanists", "distance"]}, {"paragraph_vector": [166.464553, 15.351601], "paragraph_keywords": ["data", "axes", "research", "elements"]}, {"paragraph_vector": [167.86473, 13.688004], "paragraph_keywords": ["step", "guidelines", "design", "designs"]}, {"paragraph_vector": [167.646545, 19.023248], "paragraph_keywords": ["participants", "humanities", "university", "pairs"]}, {"paragraph_vector": [167.594451, 17.106258], "paragraph_keywords": ["uncertainty", "guidelines", "visualization", "elements"]}, {"paragraph_vector": [166.281509, 14.270886], "paragraph_keywords": ["participants", "elements", "perceived", "information"]}, {"paragraph_vector": [166.722747, 15.514283], "paragraph_keywords": ["trust", "use", "tool", "concepts"]}, {"paragraph_vector": [170.067016, 18.937261], "paragraph_keywords": ["material", "framework", "grounding", "distance"]}, {"paragraph_vector": [168.82341, 16.993497], "paragraph_keywords": ["material", "grounding", "information", "elements"]}, {"paragraph_vector": [166.941131, 14.956577], "paragraph_keywords": ["knowledge", "nuance", "data", "material"]}, {"paragraph_vector": [167.256027, 14.771286], "paragraph_keywords": ["data", "use", "users", "design"]}, {"paragraph_vector": [166.688461, 16.532329], "paragraph_keywords": ["infovis", "user", "elements", "axes"]}, {"paragraph_vector": [166.781829, 17.738227], "paragraph_keywords": ["visualization", "humanities", "tools", "design"]}, {"paragraph_vector": [176.873184, 21.481243], "paragraph_keywords": ["research", "ieee", "information", "visualization"]}, {"paragraph_vector": [177.292037, 19.811254], "paragraph_keywords": ["humanities", "visualization", "data", "research"]}, {"paragraph_vector": [175.770996, 20.160146], "paragraph_keywords": ["visualization", "visualisation", "information", "data"]}, {"paragraph_vector": [178.641693, 18.683008], "paragraph_keywords": ["visualization", "humanities", "art", "design"]}, {"paragraph_vector": [175.563949, 22.039447], "paragraph_keywords": ["design", "humanities", "visualization", "."]}, {"paragraph_vector": [177.579437, 18.545749], "paragraph_keywords": ["visualization", "data", "humanities", "ieee"]}], "content": {}, "doi": "10.1109/TVCG.2020.3028954"}, {"uri": "69", "title": "Zoomless Maps: External Labeling Methods for the Interactive Exploration of Dense Point Sets at a Fixed Map Scale", "timestamp": "2020", "rating": "0.0", "annotation": "", "tags": [], "authors": ["Sven Gedicke", "Annika Bonerath", "Benjamin Niedermann", "Jan-Henrik Haunert"], "summary": "Visualizing spatial data on small-screen devices such as smartphones and smartwatches poses new challenges in computational cartography. The current interfaces for map exploration require their users to zoom in and out frequently. Indeed, zooming and panning are tools suitable for choosing the map extent corresponding to an area of interest. They are not as suitable, however, for resolving the graphical clutter caused by a high feature density since zooming in to a large map scale leads to a loss of context. Therefore, in this paper, we present new external labeling methods that allow a user to navigate through dense sets of points of interest while keeping the current map extent fixed. We provide a unified model, in which labels are placed at the boundary of the map and visually associated with the corresponding features via connecting lines, which are called leaders. Since the screen space is limited, labeling all features at the same time is impractical. Therefore, at any time, we label a subset of the features. We offer interaction techniques to change the current selection of features systematically and, thus, give the user access to all features. We distinguish three methods, which allow the user either to slide the labels along the bottom side of the map or to browse the labels based on pages or stacks. We present a generic algorithmic framework that provides us with the possibility of expressing the different variants of interaction techniques as optimization problems in a unified way. We propose both exact algorithms and fast and simple heuristics that solve the optimization problems taking into account different criteria such as the ranking of the labels, the total leader length as well as the distance between leaders. In experiments on real-world data we evaluate these algorithms and discuss the three variants with respect to their strengths and weaknesses proving the flexibility of the presented algorithmic framework.", "keywords": ["use", "label", "feature", "consider", "study", "order", "user", "solution", "problem", "number", "state", "s", "restaurant", "information", "k", "length", "interaction", "p", "fig", "approach", "expert", "present", "crossing", "algorithm", "map", "e", "visualization", "criterion", "port", "\u03b1", "leader", "method", "time", "t", "weight", "screen", "set", "labeling", "cost"], "document_vector": [178.297851, -56.187339], "paragraphs": [{"paragraph_vector": [114.93074, -53.269592], "paragraph_keywords": ["map", "devices", "information", "user"]}, {"paragraph_vector": [108.156219, -53.309436], "paragraph_keywords": ["user", "map", "information", "overview"]}, {"paragraph_vector": [105.434951, -64.158714], "paragraph_keywords": ["labels", "labeling", "map", "user"]}, {"paragraph_vector": [129.198913, -85.988563], "paragraph_keywords": ["labels", "labeling", "stack", "label"]}, {"paragraph_vector": [65.467613, -82.697189], "paragraph_keywords": ["methods", "labeling", "use", "section"]}, {"paragraph_vector": [100.425781, -53.713199], "paragraph_keywords": ["map", "user", "zooming", "tasks"]}, {"paragraph_vector": [104.956626, -63.647392], "paragraph_keywords": ["labels", "labeling", "lens", "map"]}, {"paragraph_vector": [90.764823, -74.438766], "paragraph_keywords": ["method", "presented", "labels", "pages"]}, {"paragraph_vector": [100.865966, -70.04586], "paragraph_keywords": ["labels", "internalpaging", "restaurants", "boundsliding"]}, {"paragraph_vector": [97.850509, -76.688583], "paragraph_keywords": ["labeling", "methods", "labels", "rating"]}, {"paragraph_vector": [-16.950319, -87.586372], "paragraph_keywords": ["p", "feature", "ports", "map"]}, {"paragraph_vector": [72.069183, -89.280799], "paragraph_keywords": ["cost", "leader", "leaders", "features"]}, {"paragraph_vector": [-74.828964, -89.771942], "paragraph_keywords": ["cost", "labeling", "leader", "features"]}, {"paragraph_vector": [87.843734, -88.754394], "paragraph_keywords": ["labeling", "matching", "crossing", "set"]}, {"paragraph_vector": [93.27951, -87.226821], "paragraph_keywords": ["labeling", "approach", "labels", "state"]}, {"paragraph_vector": [-36.553993, -82.799896], "paragraph_keywords": ["problem", "labeling", "path", "use"]}, {"paragraph_vector": [-124.154373, -87.921127], "paragraph_keywords": ["t", "path", "constraint", "e"]}, {"paragraph_vector": [31.277532, -89.541229], "paragraph_keywords": ["solution", "features", "labeling", "p"]}, {"paragraph_vector": [-62.813358, -88.743293], "paragraph_keywords": ["features", "stack", "feature", "l"]}, {"paragraph_vector": [-88.726203, -89.923469], "paragraph_keywords": ["labeling", "strips", "port", "criteria"]}, {"paragraph_vector": [162.871765, -87.199363], "paragraph_keywords": ["use", "labeling", "cost", "algorithms"]}, {"paragraph_vector": [-41.45026, -89.31594], "paragraph_keywords": ["leader", "cost", "\u03b1", "labeling"]}, {"paragraph_vector": [143.182556, -87.953186], "paragraph_keywords": ["features", "use", "approach", "criterion"]}, {"paragraph_vector": [-129.321456, -89.054237], "paragraph_keywords": ["number", "cost", "leader", "crossings"]}, {"paragraph_vector": [-48.988323, -86.841499], "paragraph_keywords": ["cost", "labeling", "use", "\u03b1"]}, {"paragraph_vector": [43.426246, -87.14756], "paragraph_keywords": ["labeling", "leader", "number", "pairs"]}, {"paragraph_vector": [79.916221, -86.703208], "paragraph_keywords": ["labeling", "interaction", "running", "methods"]}, {"paragraph_vector": [105.609733, -62.334423], "paragraph_keywords": ["methods", "labeling", "algorithms", "foundation"]}], "content": {}, "doi": "10.1109/TVCG.2020.3030438"}, {"uri": "70", "title": "Towards Modeling Visualization Processes as Dynamic Bayesian Networks", "timestamp": "2020", "rating": "0.0", "annotation": "", "tags": [], "authors": ["Christian Heine"], "summary": "Visualization designs typically need to be evaluated with user studies, because their suitability for a particular task is hard to predict. What the field of visualization is currently lacking are theories and models that can be used to explain why certain designs work and others do not. This paper outlines a general framework for modeling visualization processes that can serve as the first step towards such a theory. It surveys related research in mathematical and computational psychology and argues for the use of dynamic Bayesian networks to describe these time-dependent, probabilistic processes. It is discussed how these models could be used to aid in design evaluation. The development of concrete models will be a long process. Thus, the paper outlines a research program sketching how to develop prototypes and their extensions from existing models, controlled experiments, and observational studies.", "keywords": ["use", "computer", "bayesian", "theory", "image", "require", "user", "graph", "problem", "effectiveness", "effect", "structure", "model", "system", "learning", "research", "described", "allow", "number", "eye", "network", "state", "al", "memory", "perception", "heuristic", "information", "interaction", ".", "allows", "process", "space", "paradigm", "work", "task", "simulation", "modeling", "describing", "algorithm", "indicate", "need", "confidence", "visualization", "probability", "human", "cognition", "based", "tool", "mind", "describe", "data", "variable", "error", "change", "evaluation", "address", "time", "et", "ieee", "result", "inference", "behavior", "psychology", "distribution", "design", "set", "analysis", "graphic", "knowledge", "study"], "document_vector": [37.177074, 22.257713], "paragraphs": [{"paragraph_vector": [162.791168, 39.581642], "paragraph_keywords": ["design", "space", "guidelines", "heuristics"]}, {"paragraph_vector": [151.838867, 38.950725], "paragraph_keywords": ["design", "information", "visualization", "assess"]}, {"paragraph_vector": [152.688171, 43.796733], "paragraph_keywords": ["design", "modeling", "effectiveness", "user"]}, {"paragraph_vector": [171.590057, 49.536815], "paragraph_keywords": ["models", "visualization", "use", "sect"]}, {"paragraph_vector": [174.213729, 48.574703], "paragraph_keywords": ["computer", "interaction", "models", "work"]}, {"paragraph_vector": [-179.983032, 50.453796], "paragraph_keywords": ["model", "visualization", "fixations", "eye"]}, {"paragraph_vector": [96.992164, 51.469242], "paragraph_keywords": ["processes", "visualization", "image", "human"]}, {"paragraph_vector": [178.304504, 41.40726], "paragraph_keywords": ["processes", "knowledge", "computer", "process"]}, {"paragraph_vector": [169.187927, 47.529163], "paragraph_keywords": ["data", "knowledge", "generating", "errors"]}, {"paragraph_vector": [-176.373382, 55.778495], "paragraph_keywords": ["visualization", "time", "use", "data"]}, {"paragraph_vector": [161.910202, 50.077056], "paragraph_keywords": ["user", "visualization", "model", "knowledge"]}, {"paragraph_vector": [168.458847, 50.71751], "paragraph_keywords": ["models", "processes", "modeling", "visualization"]}, {"paragraph_vector": [175.932891, 56.111289], "paragraph_keywords": ["models", "modeling", "based", "psychology"]}, {"paragraph_vector": [178.300613, 53.019641], "paragraph_keywords": ["models", "problem", "modeling", "model"]}, {"paragraph_vector": [13.476026, 86.693832], "paragraph_keywords": ["modeling", "network", "models", "systems"]}, {"paragraph_vector": [171.285903, 52.276409], "paragraph_keywords": ["models", "model", "inference", "bayesian"]}, {"paragraph_vector": [-160.448348, 53.85746], "paragraph_keywords": ["variables", "probability", "structure", "bayesian"]}, {"paragraph_vector": [-163.263992, 56.289012], "paragraph_keywords": ["variables", "probability", "probabilities", "directed"]}, {"paragraph_vector": [-177.676239, 55.464771], "paragraph_keywords": ["markov", "bayesian", "processes", "state"]}, {"paragraph_vector": [-174.18724, 48.779129], "paragraph_keywords": ["bayesian", "data", "design", "use"]}, {"paragraph_vector": [175.219863, 65.113594], "paragraph_keywords": ["bayesian", "processes", "knowledge", "models"]}, {"paragraph_vector": [173.922851, 47.952239], "paragraph_keywords": ["design", "flaws", "developed", "user"]}, {"paragraph_vector": [169.936965, 50.514366], "paragraph_keywords": ["design", "model", "use", "visualization"]}, {"paragraph_vector": [173.143875, 64.641807], "paragraph_keywords": ["confidence", "statement", "system", "visualization"]}, {"paragraph_vector": [167.127182, 51.159751], "paragraph_keywords": ["designs", "effectiveness", "design", "model"]}, {"paragraph_vector": [162.614959, 49.812511], "paragraph_keywords": ["model", "design", "variables", "algorithm"]}, {"paragraph_vector": [173.041275, 50.509368], "paragraph_keywords": ["knowledge", "algorithms", "graph", "eye"]}, {"paragraph_vector": [172.433975, 49.094604], "paragraph_keywords": ["patterns", "tasks", "model", "extended"]}, {"paragraph_vector": [171.519058, 49.987525], "paragraph_keywords": ["model", "research", "simulate", "data"]}, {"paragraph_vector": [-177.39328, 57.512176], "paragraph_keywords": ["networks", "models", "approach", "bayesian"]}, {"paragraph_vector": [171.42807, 51.299976], "paragraph_keywords": ["visualization", "knowledge", "user", "users"]}, {"paragraph_vector": [-177.372817, 51.487163], "paragraph_keywords": ["models", "use", "model", "research"]}, {"paragraph_vector": [-175.485519, 41.666847], "paragraph_keywords": ["visualization", "model", "data", "ideas"]}, {"paragraph_vector": [179.894622, 40.713447], "paragraph_keywords": ["ieee", "computer", "interaction", "visualization"]}, {"paragraph_vector": [179.075668, 40.604164], "paragraph_keywords": ["visualization", "information", "cambridge", "press"]}, {"paragraph_vector": [176.165298, 43.970603], "paragraph_keywords": ["visualization", "graphics", "computer", "."]}, {"paragraph_vector": [177.163619, 41.434429], "paragraph_keywords": ["visualization", "ieee", "science", "information"]}, {"paragraph_vector": [167.17279, 45.421173], "paragraph_keywords": ["evaluation", "methods", "information", "acm"]}], "content": {}, "doi": "10.1109/TVCG.2020.3030473"}, {"uri": "71", "title": "Retrieve-Then-Adapt: Example-based Automatic Generation for Proportion-related Infographics", "timestamp": "2020", "rating": "0.0", "annotation": "", "tags": [], "authors": ["Chunyao Qian", "Shizhao Sun", "Weiwei Cui", "Jian-Guang Lou", "Haidong Zhang", "Dongmei Zhang"], "summary": "Infographic is a data visualization technique which combines graphic and textual descriptions in an aesthetic and effective manner. Creating infographics is a difficult and time-consuming process which often requires significant attempts and adjustments even for experienced designers, not to mention novice users with limited design expertise. Recently, a few approaches have been proposed to automate the creation process by applying predefined blueprints to user information. However, predefined blueprints are often hard to create, hence limited in volume and diversity. In contrast, good infogrpahics have been created by professionals and accumulated on the Internet rapidly. These online examples often represent a wide variety of design styles, and serve as exemplars or inspiration to people who like to create their own infographics. Based on these observations, we propose to generate infographics by automatically imitating examples. We present a two-stage approach, namely retrieve-then-adapt. In the retrieval stage, we index online examples by their visual elements. For a given user information, we transform it to a concrete query by sampling from a learned distribution about visual elements, and then find appropriate examples in our example library based on the similarity between example indexes and the query. For a retrieved example, we generate an initial drafts by replacing its content with user information. However, in many cases, user information cannot be perfectly fitted to retrieved examples. Therefore, we further introduce an adaption stage. Specifically, we propose a MCMC-like approach and leverage recursive neural networks to help adjust the initial draft and improve its visual appearance iteratively, until a satisfactory result is obtained. We implement our approach on widely-used proportion-related infographics, and demonstrate its effectiveness by sample results and expert reviews.", "keywords": ["use", "generation", "element", "library", "user", "text", "generated", "choice", "type", "attribute", "number", "generate", "icon", "size", "information", "query", "approach", "process", "draft", "work", "retrieved", "infographics", "layout", "infographic", "input", "existing", "based", "position", "figure", "layer", "stage", "data", "relationship", "statement", "candidate", "quality", "chart", "result", "proportion", "adaption", "blueprint", "related", "distribution", "design", "d", "designer", "example"], "document_vector": [89.090332, -53.22192], "paragraphs": [{"paragraph_vector": [153.676223, -42.129711], "paragraph_keywords": ["infographics", "blueprints", "microsoft", "information"]}, {"paragraph_vector": [155.268981, -42.123264], "paragraph_keywords": ["examples", "infographics", "information", "results"]}, {"paragraph_vector": [153.218933, -43.68362], "paragraph_keywords": ["information", "examples", "design", "elements"]}, {"paragraph_vector": [155.632476, -42.842674], "paragraph_keywords": ["infographics", "approach", "information", "tools"]}, {"paragraph_vector": [151.110931, -43.848442], "paragraph_keywords": ["design", "examples", "existing", "data"]}, {"paragraph_vector": [-169.393463, -9.671543], "paragraph_keywords": ["methods", "layout", "generation", "layouts"]}, {"paragraph_vector": [154.444183, -42.486873], "paragraph_keywords": ["input", "information", "text", "statement"]}, {"paragraph_vector": [155.809936, -41.740142], "paragraph_keywords": ["information", "input", "refers", "emphasize"]}, {"paragraph_vector": [154.4664, -43.260143], "paragraph_keywords": ["example", "u", "examples", "based"]}, {"paragraph_vector": [152.376586, -44.028785], "paragraph_keywords": ["design", "example", "u", "elements"]}, {"paragraph_vector": [151.770614, -39.525482], "paragraph_keywords": ["examples", "sheets", "infographic", "query"]}, {"paragraph_vector": [145.129333, -45.4104], "paragraph_keywords": ["attributes", "labeling", "element", "icon"]}, {"paragraph_vector": [153.872787, -41.915802], "paragraph_keywords": ["example", "examples", "elements", "input"]}, {"paragraph_vector": [157.91304, -40.832233], "paragraph_keywords": ["elements", "choices", "statement", "design"]}, {"paragraph_vector": [153.799819, -43.152706], "paragraph_keywords": ["query", "example", "elements", "user"]}, {"paragraph_vector": [149.358657, -45.053932], "paragraph_keywords": ["example", "retrieved", "draft", "figure"]}, {"paragraph_vector": [121.56742, -52.334712], "paragraph_keywords": ["draft", "design", "candidate", "mcmc"]}, {"paragraph_vector": [145.9608, -50.148666], "paragraph_keywords": ["relationships", "d", "candidate", "design"]}, {"paragraph_vector": [151.291534, -45.871612], "paragraph_keywords": ["relationships", "design", "designs", "training"]}, {"paragraph_vector": [38.49868, 46.367115], "paragraph_keywords": ["infographic", "node", "tree", "elements"]}, {"paragraph_vector": [40.099872, 38.806388], "paragraph_keywords": ["layer", "relationships", "representing", "tree"]}, {"paragraph_vector": [158.539169, -44.291179], "paragraph_keywords": ["infographics", "figure", "generate", "provides"]}, {"paragraph_vector": [146.210388, -45.114742], "paragraph_keywords": ["example", "figure", "retrieved", "infographics"]}, {"paragraph_vector": [155.157897, -39.92382], "paragraph_keywords": ["approach", "designers", "design", "generated"]}, {"paragraph_vector": [155.664962, -41.500675], "paragraph_keywords": ["design", "approach", "users", "designers"]}, {"paragraph_vector": [149.863769, -44.76268], "paragraph_keywords": ["designers", "generated", "infographics", "elements"]}, {"paragraph_vector": [155.932281, -41.084529], "paragraph_keywords": ["infographics", "approach", "examples", "design"]}, {"paragraph_vector": [153.068527, -42.295513], "paragraph_keywords": ["example", "infographics", "design", "approach"]}, {"paragraph_vector": [149.755722, -41.412307], "paragraph_keywords": ["infographics", "adaption", "approach", "stage"]}, {"paragraph_vector": [152.018951, -43.189594], "paragraph_keywords": ["approach", "attributes", "infographics", "types"]}, {"paragraph_vector": [-139.803619, 72.331687], "paragraph_keywords": ["ieee", "use", "order", "meet"]}], "content": {}, "doi": "10.1109/TVCG.2020.3030344"}, {"uri": "72", "title": "Modeling the Influence of Visual Density on Cluster Perception in Scatterplots Using Topology", "timestamp": "2020", "rating": "0.0", "annotation": "", "tags": [], "authors": ["Ghulam Jilani Quadri", "Paul Rosen"], "summary": "Scatterplots are used for a variety of visual analytics tasks, including cluster identification, and the visual encodings used on a scatterplot play a deciding role on the level of visual separation of clusters. For visualization designers, optimizing the visual encodings is crucial to maximizing the clarity of data. This requires accurately modeling human perception of cluster separation, which remains challenging. We present a multi-stage user study focusing on four factors\u2014distribution size of clusters, number of points, size of points, and opacity of points\u2014that influence cluster identification in scatterplots. From these parameters, we have constructed two models, a distance-based model, and a density-based model, using the merge tree data structure from Topological Data Analysis. Our analysis demonstrates that these factors play an important role in the number of clusters perceived, and it verifies that the distance-based and density-based models can reasonably estimate the number of clusters a user observes. Finally, we demonstrate how these models can be used to optimize visual encodings on real-world data.", "keywords": ["use", "histogram", "density", "user", "cluster", "effect", "model", "number", "tree", "threshold", "participant", "s", "component", "merge", "perception", "size", "value", "accuracy", "clustering", "p", "stimulus", "fig", "task", "distance", "influence", "input", "visualization", "based", "scatterplots", "scatterplot", "factor", "data", "response", "opacity", "t", "point", "given", "selected", "distribution", "design", "set", "analysis", "study", "experiment"], "document_vector": [-153.197341, -40.198211], "paragraphs": [{"paragraph_vector": [17.358987, 12.345967], "paragraph_keywords": ["data", "scatterplots", "clustering", "factors"]}, {"paragraph_vector": [32.10939, 5.321179], "paragraph_keywords": ["data", "models", "clusters", "perception"]}, {"paragraph_vector": [28.531042, 17.730831], "paragraph_keywords": ["scatterplots", "dr", "clusters", "clustering"]}, {"paragraph_vector": [34.6884, -2.125024], "paragraph_keywords": ["data", "charts", "density", "influence"]}, {"paragraph_vector": [34.31858, 15.547764], "paragraph_keywords": ["points", "data", "density", "opacity"]}, {"paragraph_vector": [35.219581, 22.265731], "paragraph_keywords": ["clusters", "number", "points", "use"]}, {"paragraph_vector": [30.488246, 27.191013], "paragraph_keywords": ["selected", "points", "size", "clusters"]}, {"paragraph_vector": [26.306997, 32.390159], "paragraph_keywords": ["number", "tasks", "participant", "clusters"]}, {"paragraph_vector": [45.171627, 34.464637], "paragraph_keywords": ["clusters", "number", "responses", "use"]}, {"paragraph_vector": [37.3553, 4.228621], "paragraph_keywords": ["based", "cluster", "data", "set"]}, {"paragraph_vector": [28.162876, -43.437587], "paragraph_keywords": ["components", "merge", "graph", "balls"]}, {"paragraph_vector": [29.769037, -31.99249], "paragraph_keywords": ["density", "cells", "histogram", "model"]}, {"paragraph_vector": [29.254657, -17.322938], "paragraph_keywords": ["component", "persistence", "merge", "model"]}, {"paragraph_vector": [37.032436, 15.224546], "paragraph_keywords": ["clusters", "density", "number", "model"]}, {"paragraph_vector": [33.282913, 31.888793], "paragraph_keywords": ["experiment", "stimuli", "data", "number"]}, {"paragraph_vector": [80.355987, 41.729084], "paragraph_keywords": ["size", "responses", "user", "participants"]}, {"paragraph_vector": [38.183555, 25.118309], "paragraph_keywords": ["models", "p", "density", "factors"]}, {"paragraph_vector": [46.848682, 30.089857], "paragraph_keywords": ["differential", "model", "size", "effect"]}, {"paragraph_vector": [35.036636, 20.107166], "paragraph_keywords": ["size", "accuracy", "distribution", "effect"]}, {"paragraph_vector": [35.468029, 17.593326], "paragraph_keywords": ["density", "model", "size", "histogram"]}, {"paragraph_vector": [54.538597, 40.183071], "paragraph_keywords": ["size", "model", "interaction", "effects"]}, {"paragraph_vector": [35.940494, 20.940935], "paragraph_keywords": ["data", "responses", "participants", "point"]}, {"paragraph_vector": [28.276979, 14.591523], "paragraph_keywords": ["distribution", "models", "based", "opacity"]}, {"paragraph_vector": [33.195583, 19.991762], "paragraph_keywords": ["density", "number", "size", "points"]}, {"paragraph_vector": [33.806694, 14.800253], "paragraph_keywords": ["opacity", "visualization", "size", "data"]}, {"paragraph_vector": [22.391607, 25.500513], "paragraph_keywords": ["clusters", "data", "points", "number"]}, {"paragraph_vector": [32.319366, 15.375793], "paragraph_keywords": ["model", "size", "data", "density"]}, {"paragraph_vector": [35.976879, 16.338817], "paragraph_keywords": ["model", "requires", "guarantees", "optimization"]}], "content": {}, "doi": "10.1109/TVCG.2020.3030394"}, {"uri": "73", "title": "Bayesian-Assisted Inference from Visualized Data", "timestamp": "2020", "rating": "0.0", "annotation": "", "tags": [], "authors": ["Yea-Seul Kim", "Paula Kayongo", "Madeleine Grunde-McLaughlin", "Jessica Hullman"], "summary": "A Bayesian view of data interpretation suggests that a visualization user should update their existing beliefs about a parameter\u2019s value in accordance with the amount of information about the parameter value captured by the new observations. Extending recent work applying Bayesian models to understand and evaluate belief updating from visualizations, we show how the predictions of Bayesian inference can be used to guide more rational belief updating. We design a Bayesian inference-assisted uncertainty analogy that numerically relates uncertainty in observed data to the user\u2019s subjective uncertainty, and a posterior visualization that prescribes how a user should update their beliefs given their prior beliefs and the observed data. In a pre-registered experiment on 4,800 people, we find that when a newly observed data sample is relatively small (N=158), both techniques reliably improve people\u2019s Bayesian updating on average compared to the current best practice of visualizing uncertainty in the observed data. For large data samples (N=5208), where people\u2019s updated beliefs tend to deviate more strongly from the prescriptions of a Bayesian model, we find evidence that the effectiveness of the two forms of Bayesian assistance may depend on people\u2019s proclivity toward trusting the source of the data. We discuss how our results provide insight into individual processes of belief updating and subjective uncertainty, and how understanding these aspects of interpretation paves the way for more sophisticated interactive visualizations for analysis and communication.", "keywords": ["use", "bayesian", "sample", "user", "model", "uncertainty", "observed", "participant", "update", "size", "information", "value", "analogy", "dementia", "updating", "fig", "estimate", "people", "work", "location", "parameter", "dataset", "visualization", "probability", "based", "datasets", "bias", "posterior", "data", "proportion", "inference", "result", "assistance", "point", "likelihood", "distribution", "kld", "interval", "elicitation", "condition", "belief"], "document_vector": [7.705697, 27.356264], "paragraphs": [{"paragraph_vector": [-172.175369, 65.294158], "paragraph_keywords": ["beliefs", "data", "chance", "university"]}, {"paragraph_vector": [175.493438, 69.634681], "paragraph_keywords": ["bayesian", "beliefs", "data", "parameter"]}, {"paragraph_vector": [152.943878, 70.696952], "paragraph_keywords": ["uncertainty", "bayesian", "belief", "updating"]}, {"paragraph_vector": [167.429397, 63.390148], "paragraph_keywords": ["beliefs", "bayesian", "people", "data"]}, {"paragraph_vector": [162.439071, 67.304367], "paragraph_keywords": ["beliefs", "bayesian", "parameter", "user"]}, {"paragraph_vector": [148.759216, 75.256187], "paragraph_keywords": ["beliefs", "parameter", "data", "bayesian"]}, {"paragraph_vector": [110.413803, 73.984336], "paragraph_keywords": ["residents", "data", "events", "uncertainty"]}, {"paragraph_vector": [152.902496, 71.979911], "paragraph_keywords": ["data", "distribution", "beliefs", "information"]}, {"paragraph_vector": [149.662277, 72.593673], "paragraph_keywords": ["beliefs", "uncertainty", "distribution", "sample"]}, {"paragraph_vector": [128.704772, 73.613761], "paragraph_keywords": ["participants", "residents", "datasets", "perceived"]}, {"paragraph_vector": [111.949798, 74.89009], "paragraph_keywords": ["elicitation", "conditions", "estimate", "datasets"]}, {"paragraph_vector": [124.38623, 74.604408], "paragraph_keywords": ["interval", "participants", "handle", "visualization"]}, {"paragraph_vector": [125.528602, 67.934135], "paragraph_keywords": ["participants", "distribution", "data", "workers"]}, {"paragraph_vector": [154.195327, 71.026153], "paragraph_keywords": ["distribution", "participant", "participants", "data"]}, {"paragraph_vector": [86.563842, 78.060165], "paragraph_keywords": ["participants", "abortion", "conditions", "dataset"]}, {"paragraph_vector": [121.712356, 73.594116], "paragraph_keywords": ["participants", "bayesian", "updating", "variance"]}, {"paragraph_vector": [120.284324, 63.361896], "paragraph_keywords": ["condition", "kld", "log", "dispersion"]}, {"paragraph_vector": [128.312774, 75.045616], "paragraph_keywords": ["visualization", "condition", "location", "uncertainty"]}, {"paragraph_vector": [129.48587, 74.803718], "paragraph_keywords": ["visualization", "condition", "analogy", "fig"]}, {"paragraph_vector": [133.179809, 72.637916], "paragraph_keywords": ["bayesian", "beliefs", "estimate", "kld"]}, {"paragraph_vector": [156.862838, 69.964553], "paragraph_keywords": ["bayesian", "updating", "beliefs", "users"]}, {"paragraph_vector": [129.030715, 73.477249], "paragraph_keywords": ["bayesian", "data", "assistance", "point"]}, {"paragraph_vector": [162.994308, 65.983764], "paragraph_keywords": ["beliefs", "visualization", "uncertainty", "users"]}, {"paragraph_vector": [171.76271, 68.649795], "paragraph_keywords": ["bayesian", "data", "beliefs", "user"]}, {"paragraph_vector": [170.531524, 66.90673], "paragraph_keywords": ["bayesian", "data", "likelihood", "understanding"]}, {"paragraph_vector": [158.981994, 70.577751], "paragraph_keywords": ["bayesian", "data", "belief", "assistance"]}, {"paragraph_vector": [164.679672, 19.051935], "paragraph_keywords": ["ieee", "data", "use", "abhraneel"]}], "content": {}, "doi": "10.1109/TVCG.2020.3030381"}, {"uri": "74", "title": "QLens: Visual Analytics of Multi-step Problem-solving Behaviors for Improving Question Design", "timestamp": "2020", "rating": "0.0", "annotation": "", "tags": [], "authors": ["Meng Xia", "Reshika Palaniyappan Velumani", "Yong Wang", "Huamin Qu", "Xiaojuan Ma"], "summary": "With the rapid development of online education in recent years, there has been an increasing number of learning platforms that provide students with multi-step questions to cultivate their problem-solving skills. To guarantee the high quality of such learning materials, question designers need to inspect how students\u2019 problem-solving processes unfold step by step to infer whether students\u2019 problem-solving logic matches their design intent. They also need to compare the behaviors of different groups (e.g., students from different grades) to distribute questions to students with the right level of knowledge. The availability of fine-grained interaction data, such as mouse movement trajectories from the online platforms, provides the opportunity to analyze problem-solving behaviors. However, it is still challenging to interpret, summarize, and compare the high dimensional problem-solving sequence data. In this paper, we present a visual analytics system, QLens, to help question designers inspect detailed problem-solving trajectories, compare different student groups, distill insights for design improvements. In particular, QLens models problem-solving behavior as a hybrid state transition graph and visualizes it through a novel glyph-embedded Sankey diagram, which reflects students\u2019 problem-solving logic, engagement, and encountered difficulties. We conduct three case studies and three expert interviews to demonstrate the usefulness of QLens on real-world datasets that consist of thousands of problem-solving traces.", "keywords": ["use", "feedback", "logic", "problem", "comparison", "transition", "system", "learning", "path", "number", "qlens", "state", "level", "facilitate", "information", "interaction", "driven", "fig", "expert", "question", "process", "provide", "answer", "visualization", "based", "shown", "group", "stage", "data", "sequence", "error", "view", "time", "behavior", "event", "design", "condition", "designer", "grade", "example", "student", "mouse", "analysis", "solving", "step"], "document_vector": [51.937564, -34.528339], "paragraphs": [{"paragraph_vector": [-147.028823, 86.930824], "paragraph_keywords": ["questions", "students", "solving", "problem"]}, {"paragraph_vector": [-129.969741, 86.949249], "paragraph_keywords": ["students", "question", "solving", "problem"]}, {"paragraph_vector": [-160.754714, 89.472213], "paragraph_keywords": ["students", "solving", "problem", "level"]}, {"paragraph_vector": [-156.298614, 85.901489], "paragraph_keywords": ["students", "solving", "knowledge", "problem"]}, {"paragraph_vector": [-172.697174, 88.851654], "paragraph_keywords": ["students", "states", "transition", "visualization"]}, {"paragraph_vector": [-63.232212, -40.490898], "paragraph_keywords": ["sequences", "event", "sequence", "summarize"]}, {"paragraph_vector": [-136.604553, 86.250877], "paragraph_keywords": ["students", "question", "problem", "questions"]}, {"paragraph_vector": [-143.194473, 88.579971], "paragraph_keywords": ["data", "students", "module", "based"]}, {"paragraph_vector": [-147.120147, 86.893051], "paragraph_keywords": ["mouse", "students", "data", "question"]}, {"paragraph_vector": [-64.782676, 88.778503], "paragraph_keywords": ["rois", "mouse", "students", "roi"]}, {"paragraph_vector": [-13.535591, 89.075576], "paragraph_keywords": ["student", "stage", "step", "time"]}, {"paragraph_vector": [-166.902236, 89.343673], "paragraph_keywords": ["state", "students", "represent", "conditions"]}, {"paragraph_vector": [-157.507324, 89.296478], "paragraph_keywords": ["students", "path", "designers", "states"]}, {"paragraph_vector": [-142.590545, 87.125267], "paragraph_keywords": ["students", "question", "problem", "designers"]}, {"paragraph_vector": [59.546302, 89.846839], "paragraph_keywords": ["students", "problem", "solving", "question"]}, {"paragraph_vector": [-66.754089, 89.180824], "paragraph_keywords": ["error", "students", "number", "fig"]}, {"paragraph_vector": [-129.09436, 87.880752], "paragraph_keywords": ["conditions", "students", "answer", "fig"]}, {"paragraph_vector": [-95.446708, 89.351776], "paragraph_keywords": ["number", "students", "conditions", "stage"]}, {"paragraph_vector": [-134.795043, 89.131896], "paragraph_keywords": ["condition", "students", "number", "design"]}, {"paragraph_vector": [-104.603202, 86.910743], "paragraph_keywords": ["students", "question", "line", "step"]}, {"paragraph_vector": [-47.094806, 88.438095], "paragraph_keywords": ["students", "stage", "view", "comparison"]}, {"paragraph_vector": [-87.391197, 87.839591], "paragraph_keywords": ["question", "students", "users", "system"]}, {"paragraph_vector": [-172.558532, 88.746398], "paragraph_keywords": ["students", "fig", "stage", "conditions"]}, {"paragraph_vector": [-133.024612, 87.405494], "paragraph_keywords": ["students", "problem", "question", "condition"]}, {"paragraph_vector": [-129.663421, 85.703559], "paragraph_keywords": ["grade", "students", "question", "stage"]}, {"paragraph_vector": [-142.539581, 88.371612], "paragraph_keywords": ["students", "grade", "question", "step"]}, {"paragraph_vector": [-131.883285, 88.620368], "paragraph_keywords": ["students", "error", "feedback", "path"]}, {"paragraph_vector": [-123.068817, 86.013183], "paragraph_keywords": ["experts", "system", "question", "usability"]}, {"paragraph_vector": [174.073196, 87.532714], "paragraph_keywords": ["students", "question", "data", "help"]}, {"paragraph_vector": [146.204513, -25.53659], "paragraph_keywords": ["data", "transition", "view", "interactions"]}, {"paragraph_vector": [-153.756866, 86.313072], "paragraph_keywords": ["designers", "behaviors", "question", "players"]}, {"paragraph_vector": [-178.316055, 77.269104], "paragraph_keywords": ["ieee", "research", "kong", "hong"]}], "content": {}, "doi": "10.1109/TVCG.2020.3030402"}, {"uri": "75", "title": "PlotThread: Creating Expressive Storyline Visualizations using Reinforcement Learning", "timestamp": "2020", "rating": "0.0", "annotation": "", "tags": [], "authors": ["Tan Tang", "Renzhong Li", "Xinke Wu", "Shuhan Liu", "Johannes Knittel", "Steffen Koch", "Thomas Ertl", "Lingyun Yu", "Peiran Ren", "Yingcai Wu"], "summary": "Storyline visualizations are an effective means to present the evolution of plots and reveal the scenic interactions among characters. However, the design of storyline visualizations is a difficult task as users need to balance between aesthetic goals and narrative constraints. Despite that the optimization-based methods have been improved significantly in terms of producing aesthetic and legible layouts, the existing (semi-) automatic methods are still limited regarding 1) efficient exploration of the storyline design space and 2) flexible customization of storyline layouts. In this work, we propose a reinforcement learning framework to train an AI agent that assists users in exploring the design space efficiently and generating well-optimized storylines. Based on the framework, we introduce PlotThread, an authoring tool that integrates a set of flexible interactions to support easy customization of storyline visualizations. To seamlessly integrate the AI agent into the authoring process, we employ a mixed-initiative approach where both the agent and designers work on the same canvas to boost the collaborative design of storylines. We evaluate the reinforcement learning model through qualitative and quantitative experiments and demonstrate the usage of PlotThread using a collection of use cases.", "keywords": ["use", "implemented", "user", "rl", "learning", "model", "state", "improve", "framework", "function", "level", "constraint", "k", "agent", "interaction", "fig", "space", "process", "istoryline", "work", "layout", "ai", "action", "reinforcement", "visualization", "based", "obtain", "steppenwolf", "employ", "group", "storyline", "data", "produce", "plotthread", "time", "quality", "optimized", "character", "training", "narrative", "line", "design", "set", "story", "optimization", "step"], "document_vector": [96.632759, -33.370307], "paragraphs": [{"paragraph_vector": [-167.259643, 0.067284], "paragraph_keywords": ["university", "wu", "line", "ieee"]}, {"paragraph_vector": [-168.699005, -6.303573], "paragraph_keywords": ["storylines", "layouts", "design", "optimization"]}, {"paragraph_vector": [-171.905441, -6.551208], "paragraph_keywords": ["storylines", "learning", "agent", "users"]}, {"paragraph_vector": [-170.410903, -6.081851], "paragraph_keywords": ["visualizations", "layouts", "storyline", "storylines"]}, {"paragraph_vector": [-172.918869, -4.244863], "paragraph_keywords": ["interactions", "visualizations", "layouts", "design"]}, {"paragraph_vector": [-171.116897, -9.192923], "paragraph_keywords": ["learning", "agent", "et", "storylines"]}, {"paragraph_vector": [-170.92044, -6.129492], "paragraph_keywords": ["users", "design", "agent", "ai"]}, {"paragraph_vector": [-169.216278, -5.277493], "paragraph_keywords": ["users", "design", "agent", "storyline"]}, {"paragraph_vector": [-170.86235, -6.804308], "paragraph_keywords": ["interactions", "users", "line", "layouts"]}, {"paragraph_vector": [129.953826, -38.217796], "paragraph_keywords": ["users", "line", "lines", "characters"]}, {"paragraph_vector": [-170.699081, -7.418917], "paragraph_keywords": ["layout", "layouts", "user", "agent"]}, {"paragraph_vector": [-171.807388, -8.365718], "paragraph_keywords": ["characters", "istoryline", "agent", "constraints"]}, {"paragraph_vector": [-170.258422, -7.327276], "paragraph_keywords": ["j", "optimization", "characters", "th"]}, {"paragraph_vector": [-169.807571, -5.74222], "paragraph_keywords": ["layouts", "model", "optimization", "parameters"]}, {"paragraph_vector": [-170.308532, -5.875374], "paragraph_keywords": ["layouts", "agent", "k", "constraints"]}, {"paragraph_vector": [-171.420501, -6.76929], "paragraph_keywords": ["agent", "layout", "layouts", "action"]}, {"paragraph_vector": [-170.003036, -9.29162], "paragraph_keywords": ["layout", "step", "alignment", "function"]}, {"paragraph_vector": [-172.511962, -6.429829], "paragraph_keywords": ["state", "action", "agent", "actor"]}, {"paragraph_vector": [157.691482, -22.745021], "paragraph_keywords": ["learning", "actor", "implemented", "model"]}, {"paragraph_vector": [-171.703887, -8.216656], "paragraph_keywords": ["layouts", "user", "agent", "model"]}, {"paragraph_vector": [-171.113525, -6.715994], "paragraph_keywords": ["agent", "use", "justice", "league"]}, {"paragraph_vector": [87.250221, 11.727629], "paragraph_keywords": ["steppenwolf", "fig", "superheroes", "step"]}, {"paragraph_vector": [-168.916702, -3.899631], "paragraph_keywords": ["plotthread", "storylines", "fig", "expert"]}, {"paragraph_vector": [-168.458206, -5.061782], "paragraph_keywords": ["ai", "storylines", "agent", "users"]}, {"paragraph_vector": [-171.191543, -5.507247], "paragraph_keywords": ["users", "plotthread", "storylines", "design"]}, {"paragraph_vector": [-171.551651, -7.036337], "paragraph_keywords": ["design", "storylines", "learning", "visualizations"]}, {"paragraph_vector": [-171.184448, -6.951137], "paragraph_keywords": ["design", "agent", "users", "plan"]}, {"paragraph_vector": [-106.664375, 69.846855], "paragraph_keywords": ["ieee", "research", "use", "funded"]}], "content": {}, "doi": "10.1109/TVCG.2020.3030436"}, {"uri": "76", "title": "Objective Observer-Relative Flow Visualization in Curved Spaces for Unsteady 2D Geophysical Flows", "timestamp": "2020", "rating": "0.0", "annotation": "", "tags": [], "authors": ["Peter Rautek", "Matej Mlejnek", "Johanna Beyer", "Jakob Troidl", "Hanspeter Pfister", "Thomas Theu\u00dfl", "Markus Hadwiger"], "summary": "Computing and visualizing features in fluid flow often depends on the observer, or reference frame, relative to which the input velocity field is given. A desired property of feature detectors is therefore that they are objective, meaning independent of the input reference frame. However, the standard definition of objectivity is only given for Euclidean domains and cannot be applied in curved spaces. We build on methods from mathematical physics and Riemannian geometry to generalize objectivity to curved spaces, using the powerful notion of symmetry groups as the basis for definition. From this, we develop a general mathematical framework for the objective computation of observer fields for curved spaces, relative to which other computed measures become objective. An important property of our framework is that it works intrinsically in 2D, instead of in the 3D ambient space. This enables a direct generalization of the 2D computation via optimization of observer fields in flat space to curved domains, without having to perform optimization in 3D. We specifically develop the case of unsteady 2D geophysical flows given on spheres, such as the Earth. Our observer fields in curved spaces then enable objective feature computation as well as the visualization of the time evolution of scalar and vector fields, such that the automatically computed reference frames follow moving structures like vortices in a way that makes them appear to be steady.", "keywords": ["flow", "use", "reference", "computed", "motion", "derivative", "transformation", "sphere", "g", "compute", "respect", "path", "component", "field", "killing", "framework", "eq", "j", "space", "m", "matrix", "definition", "observer", "vector", "visualization", "tensor", "computation", "group", "isometry", "time", "chart", "t", "given", "computing", "point", "v", "hurricane", "u", "optimization", "lie"], "document_vector": [-0.536497, -41.419811], "paragraphs": [{"paragraph_vector": [56.671558, 29.743839], "paragraph_keywords": ["time", "frame", "university", "velocity"]}, {"paragraph_vector": [62.96474, 26.295015], "paragraph_keywords": ["reference", "flow", "earth", "frame"]}, {"paragraph_vector": [65.522827, 24.278741], "paragraph_keywords": ["observer", "time", "field", "motion"]}, {"paragraph_vector": [61.324764, 24.159732], "paragraph_keywords": ["fields", "observer", "field", "spaces"]}, {"paragraph_vector": [61.96471, 26.492052], "paragraph_keywords": ["flow", "space", "visualization", "techniques"]}, {"paragraph_vector": [56.726615, 15.343234], "paragraph_keywords": ["use", "work", "surfaces", "isometries"]}, {"paragraph_vector": [63.112247, 26.712667], "paragraph_keywords": ["flow", "methods", "fields", "computations"]}, {"paragraph_vector": [62.989295, 26.257188], "paragraph_keywords": ["vector", "basis", "vectors", "m"]}, {"paragraph_vector": [61.893321, 24.861152], "paragraph_keywords": ["use", "field", "v", "vector"]}, {"paragraph_vector": [61.630142, 27.070514], "paragraph_keywords": ["m", "time", "observer", "g"]}, {"paragraph_vector": [61.005477, 27.235492], "paragraph_keywords": ["field", "vector", "killing", "isometry"]}, {"paragraph_vector": [62.943389, 26.979797], "paragraph_keywords": ["field", "observer", "hurricane", "concept"]}, {"paragraph_vector": [61.115779, 25.061208], "paragraph_keywords": ["definition", "respect", "tensor", "group"]}, {"paragraph_vector": [59.940364, 24.345342], "paragraph_keywords": ["group", "g", "fields", "transformation"]}, {"paragraph_vector": [61.616043, 25.112955], "paragraph_keywords": ["group", "m", "defined", "symmetry"]}, {"paragraph_vector": [62.199562, 27.595703], "paragraph_keywords": ["eq", "g", "objectivity", "field"]}, {"paragraph_vector": [63.115318, 27.82521], "paragraph_keywords": ["vector", "tensor", "m", "field"]}, {"paragraph_vector": [62.412021, 27.264055], "paragraph_keywords": ["m", "jk", "j", "\u03b3i"]}, {"paragraph_vector": [62.389488, 26.461343], "paragraph_keywords": ["m", "g", "vector", "given"]}, {"paragraph_vector": [62.746398, 27.43951], "paragraph_keywords": ["killing", "equation", "field", "components"]}, {"paragraph_vector": [61.635066, 27.926612], "paragraph_keywords": ["field", "t", "tensor", "killing"]}, {"paragraph_vector": [60.443798, 27.233619], "paragraph_keywords": ["m", "derivatives", "u", "lu"]}, {"paragraph_vector": [60.650772, 23.957523], "paragraph_keywords": ["matrix", "stored", "eq", "computed"]}, {"paragraph_vector": [54.930213, 18.721298], "paragraph_keywords": ["j", "tensor", "given", "t"]}, {"paragraph_vector": [61.751056, 26.090656], "paragraph_keywords": ["field", "chart", "time", "isometry"]}, {"paragraph_vector": [62.283123, 23.714027], "paragraph_keywords": ["use", "data", "computations", "given"]}, {"paragraph_vector": [63.346809, 25.783174], "paragraph_keywords": ["field", "observer", "fields", "sphere"]}, {"paragraph_vector": [64.848823, 25.366081], "paragraph_keywords": ["field", "optimization", "data", "observer"]}, {"paragraph_vector": [63.307685, 24.094709], "paragraph_keywords": ["time", "framework", "hurricane", "features"]}, {"paragraph_vector": [54.382808, 29.2616], "paragraph_keywords": ["kaust", "university", "ieee", "use"]}], "content": {}, "doi": "10.1109/TVCG.2020.3030363"}, {"uri": "77", "title": "TaxThemis: Interactive Mining and Exploration of Suspicious Tax Evasion Groups", "timestamp": "2020", "rating": "0.0", "annotation": "", "tags": [], "authors": ["Yating Lin", "Kamkwai Wong", "Yong Wang", "Rong Zhang", "Bo Dong", "Huamin Qu", "Qinghua Zheng"], "summary": "Tax evasion is a serious economic problem for many countries, as it can undermine the government\u2019s tax system and lead to an unfair business competition environment. Recent research has applied data analytics techniques to analyze and detect tax evasion behaviors of individual taxpayers. However, they have failed to support the analysis and exploration of the related party transaction tax evasion (RPTTE) behaviors (e.g., transfer pricing), where a group of taxpayers is involved. In this paper, we present TaxThemis, an interactive visual analytics system to help tax officers mine and explore suspicious tax evasion groups through analyzing heterogeneous tax-related data. A taxpayer network is constructed and fused with the respective trade network to detect suspicious RPTTE groups. Rich visualizations are designed to facilitate the exploration and investigation of suspicious transactions between related taxpayers with profit and topological data analysis. Specifically, we propose a calendar heatmap with a carefullydesigned encoding scheme to intuitively show the evidence of transferring revenue through related party transactions. We demonstrate the usefulness and effectiveness of TaxThemis through two case studies on real-world tax-related data and interviews with domain experts.", "keywords": ["exploration", "use", "feature", "pattern", "taxpayer", "user", "party", "graph", "officer", "record", "status", "node", "system", "number", "taxthemis", "support", "network", "trade", "al", "control", "information", "analytics", "color", "fig", "expert", "rptte", "transaction", "algorithm", "help", "requirement", "provide", "visualization", "based", "explore", "tax", "group", "profit", "data", "relationship", "inspection", "view", "detection", "time", "trading", "chart", "method", "et", "administration", "audit", "line", "related", "period", "encoding", "design", "proposed", "analysis", "indicator", "investment", "evasion"], "document_vector": [-85.992378, 29.430675], "paragraphs": [{"paragraph_vector": [-134.471923, -21.338376], "paragraph_keywords": ["tax", "gap", "evasion", "ieee"]}, {"paragraph_vector": [-130.036254, -23.788463], "paragraph_keywords": ["tax", "evasion", "data", "officers"]}, {"paragraph_vector": [-124.6874, -27.246782], "paragraph_keywords": ["tax", "groups", "group", "network"]}, {"paragraph_vector": [-127.375968, -25.4346], "paragraph_keywords": ["tax", "based", "methods", "evasion"]}, {"paragraph_vector": [-123.135444, -28.128021], "paragraph_keywords": ["positives", "licensed", "groups", "identified"]}, {"paragraph_vector": [-127.777976, -29.527214], "paragraph_keywords": ["visualization", "trade", "et", "network"]}, {"paragraph_vector": [-127.577201, -28.97573], "paragraph_keywords": ["support", "tax", "system", "fraud"]}, {"paragraph_vector": [-128.971984, -24.86835], "paragraph_keywords": ["tax", "taxpayers", "information", "taxpayer"]}, {"paragraph_vector": [-132.557617, -24.284355], "paragraph_keywords": ["tax", "taxpayers", "audit", "evasion"]}, {"paragraph_vector": [-129.333511, -26.545335], "paragraph_keywords": ["tax", "evasion", "officers", "experts"]}, {"paragraph_vector": [-127.749649, -23.693134], "paragraph_keywords": ["groups", "tax", "users", "taxpayers"]}, {"paragraph_vector": [-129.101364, -24.599145], "paragraph_keywords": ["data", "tax", "taxpayers", "transactions"]}, {"paragraph_vector": [-129.696075, -27.106838], "paragraph_keywords": ["information", "data", "taxpayers", "groups"]}, {"paragraph_vector": [-124.024787, -24.693819], "paragraph_keywords": ["network", "taxpayer", "party", "taxpayers"]}, {"paragraph_vector": [-126.071151, -25.450256], "paragraph_keywords": ["party", "network", "taxpayers", "rptte"]}, {"paragraph_vector": [-125.035667, -24.748222], "paragraph_keywords": ["profit", "rptte", "party", "selected"]}, {"paragraph_vector": [-125.704154, -25.008424], "paragraph_keywords": ["users", "groups", "rptte", "tax"]}, {"paragraph_vector": [-124.970687, -26.500658], "paragraph_keywords": ["fig", "transactions", "flow", "party"]}, {"paragraph_vector": [-126.009162, -24.86333], "paragraph_keywords": ["node", "groups", "color", "group"]}, {"paragraph_vector": [-125.058876, -24.946619], "paragraph_keywords": ["users", "party", "design", "profit"]}, {"paragraph_vector": [-127.996574, -27.020784], "paragraph_keywords": ["users", "profit", "transaction", "chart"]}, {"paragraph_vector": [-127.438888, -27.791477], "paragraph_keywords": ["line", "taxthemis", "chart", "y"]}, {"paragraph_vector": [-122.946983, -23.828433], "paragraph_keywords": ["fig", "taxpayers", "group", "profit"]}, {"paragraph_vector": [-126.378593, -25.209308], "paragraph_keywords": ["group", "tax", "groups", "transaction"]}, {"paragraph_vector": [-129.165695, -25.675441], "paragraph_keywords": ["tax", "profit", "minutes", "conducted"]}, {"paragraph_vector": [-127.249488, -27.334651], "paragraph_keywords": ["system", "use", "tax", "groups"]}, {"paragraph_vector": [-128.747497, -25.345802], "paragraph_keywords": ["system", "tax", "experts", "positives"]}, {"paragraph_vector": [-127.470924, -27.467294], "paragraph_keywords": ["tax", "taxthemis", "users", "groups"]}, {"paragraph_vector": [-125.995407, -26.415061], "paragraph_keywords": ["tax", "design", "data", "taxthemis"]}, {"paragraph_vector": [-126.064956, -27.685684], "paragraph_keywords": ["china", "grant", "servyou", "research"]}], "content": {}, "doi": "10.1109/TVCG.2020.3030452"}, {"uri": "78", "title": "Truth or Square: Aspect Ratio Biases Recall of Position Encodings", "timestamp": "2020", "rating": "0.0", "annotation": "", "tags": [], "authors": ["Cristina R. Ceja", "Caitlyn M. McColeman", "Cindy Xiong", "Steven L. Franconeri"], "summary": "Bar charts are among the most frequently used visualizations, in part because their position encoding leads them to convey data values precisely. Yet reproductions of single bars or groups of bars within a graph can be biased. Curiously, some previous work found that this bias resulted in an overestimation of reproduced data values, while other work found an underestimation. Across three empirical studies, we offer an explanation for these conflicting findings: this discrepancy is a consequence of the differing aspect ratios of the tested bar marks. Viewers are biased to remember a bar mark as being more similar to a prototypical square, leading to an overestimation of bars with a wide aspect ratio, and an underestimation of bars with a tall aspect ratio. Experiments 1 and 2 showed that the aspect ratio of the bar marks indeed influenced the direction of this bias. Experiment 3 confirmed that this pattern of misestimation bias was present for reproductions from memory, suggesting that this bias may arise when comparing values across sequential displays or views. We describe additional visualization designs that might be prone to this bias beyond bar charts (e.g., Mekko charts and treemaps), and speculate that other visual channels might hold similar biases toward prototypical values.", "keywords": ["found", "height", "effect", "aspect", "ratio", "participant", "memory", "control", "perception", "value", "work", "mark", "bar", "position", "figure", "shown", "bias", "data", "error", "viewer", "area", "chart", "response", "representation", "encoding", "replication", "condition", "screen", "study", "experiment"], "document_vector": [42.681434, -3.753246], "paragraphs": [{"paragraph_vector": [106.715217, 5.498375], "paragraph_keywords": ["bar", "university", "position", "aspect"]}, {"paragraph_vector": [109.675773, 0.646839], "paragraph_keywords": ["position", "data", "biases", "bar"]}, {"paragraph_vector": [106.826828, 5.751379], "paragraph_keywords": ["bar", "position", "aspect", "ratios"]}, {"paragraph_vector": [98.370941, -45.012905], "paragraph_keywords": ["ratio", "aspect", "data", "slope"]}, {"paragraph_vector": [105.925552, 13.607101], "paragraph_keywords": ["aspect", "ratio", "bar", "bias"]}, {"paragraph_vector": [105.218536, 16.318634], "paragraph_keywords": ["color", "ratio", "bias", "memory"]}, {"paragraph_vector": [104.53688, 16.754974], "paragraph_keywords": ["aspect", "ratio", "bars", "position"]}, {"paragraph_vector": [104.719085, 15.217701], "paragraph_keywords": ["response", "bar", "aspect", "position"]}, {"paragraph_vector": [104.494445, 17.166732], "paragraph_keywords": ["aspect", "condition", "height", "trials"]}, {"paragraph_vector": [105.169448, 17.01198], "paragraph_keywords": ["bar", "position", "experiment", "aspect"]}, {"paragraph_vector": [103.705566, 16.314077], "paragraph_keywords": ["bias", "p", "ratio", "aspect"]}, {"paragraph_vector": [106.829353, 13.612107], "paragraph_keywords": ["aspect", "bar", "ratios", "mark"]}, {"paragraph_vector": [105.028121, 12.943609], "paragraph_keywords": ["area", "marks", "aspect", "bar"]}, {"paragraph_vector": [106.329856, 16.054681], "paragraph_keywords": ["area", "condition", "replication", "control"]}, {"paragraph_vector": [105.458312, 17.100137], "paragraph_keywords": ["area", "aspect", "model", "condition"]}, {"paragraph_vector": [105.499488, 15.025848], "paragraph_keywords": ["condition", "area", "aspect", "responses"]}, {"paragraph_vector": [107.439407, 14.210685], "paragraph_keywords": ["aspect", "ratios", "bar", "marks"]}, {"paragraph_vector": [105.314079, 15.337844], "paragraph_keywords": ["bar", "memory", "response", "screen"]}, {"paragraph_vector": [106.380409, 16.296552], "paragraph_keywords": ["conditions", "memory", "error", "tracing"]}, {"paragraph_vector": [104.58493, 16.441249], "paragraph_keywords": ["bars", "aspect", "memory", "condition"]}, {"paragraph_vector": [104.904548, 15.044303], "paragraph_keywords": ["position", "aspect", "responses", "ratios"]}, {"paragraph_vector": [106.929023, 12.036122], "paragraph_keywords": ["bias", "aspect", "position", "ratios"]}, {"paragraph_vector": [106.16365, 12.705488], "paragraph_keywords": ["position", "mark", "values", "aspect"]}, {"paragraph_vector": [104.770828, 14.623823], "paragraph_keywords": ["perception", "position", "reproduction", "error"]}, {"paragraph_vector": [106.324989, 6.294243], "paragraph_keywords": ["position", "bar", "representation", "reporting"]}, {"paragraph_vector": [-125.864021, 70.705833], "paragraph_keywords": ["grant", "belkind", "data", "maksim"]}], "content": {}, "doi": "10.1109/TVCG.2020.3030384"}, {"uri": "79", "title": "Auditing the Sensitivity of Graph-based Ranking with Visual Analytics", "timestamp": "2020", "rating": "0.0", "annotation": "", "tags": [], "authors": ["Tiankai Xie", "Yuxin Ma", "Hanghang Tong", "My T. Thai", "Ross Maciejewski"], "summary": "Graph mining plays a pivotal role across a number of disciplines, and a variety of algorithms have been developed to answer who/what type questions. For example, what items shall we recommend to a given user on an e-commerce platform? The answers to such questions are typically returned in the form of a ranked list, and graph-based ranking methods are widely used in industrial information retrieval settings. However, these ranking algorithms have a variety of sensitivities, and even small changes in rank can lead to vast reductions in product sales and page hits. As such, there is a need for tools and methods that can help model developers and analysts explore the sensitivities of graph ranking algorithms with respect to perturbations within the graph structure. In this paper, we present a visual analytics framework for explaining and exploring the sensitivity of any graph-based ranking algorithm by performing perturbation-based what-if analysis. We demonstrate our framework through three case studies inspecting the sensitivity of two classic graph-based ranking algorithms (PageRank and HITS) as applied to rankings in political news media and social networks.", "keywords": ["use", "sensitivity", "removal", "rank", "graph", "node", "structure", "system", "effect", "search", "add", "auditing", "support", "community", "network", "index", "framework", "-", "constraint", "k", "identify", "ranking", "removing", "analyst", "task", "influence", "vector", "algorithm", "edge", "pagerank", "based", "explore", "figure", "r", "data", "change", "view", "method", "blog", "result", "increased", "distribution", "perturbation", "analysis", "influenced"], "document_vector": [-140.352645, 14.368162], "paragraphs": [{"paragraph_vector": [-134.820007, -1.40913], "paragraph_keywords": ["graph", "ranking", "algorithms", "algorithm"]}, {"paragraph_vector": [-136.239288, -5.098761], "paragraph_keywords": ["graph", "ranking", "based", "sensitivities"]}, {"paragraph_vector": [-134.464599, -10.307488], "paragraph_keywords": ["node", "pagerank", "graph", "nodes"]}, {"paragraph_vector": [-135.516143, -4.881342], "paragraph_keywords": ["graph", "matrix", "methods", "pagerank"]}, {"paragraph_vector": [-137.082351, -3.828522], "paragraph_keywords": ["graph", "ranking", "use", "changes"]}, {"paragraph_vector": [-136.338378, -3.645801], "paragraph_keywords": ["ranking", "analysts", "data", "graph"]}, {"paragraph_vector": [-135.238677, -4.450616], "paragraph_keywords": ["graph", "ranking", "changes", "node"]}, {"paragraph_vector": [-136.191879, -3.573879], "paragraph_keywords": ["perturbation", "view", "nodes", "ranking"]}, {"paragraph_vector": [-136.022552, -1.063727], "paragraph_keywords": ["analyst", "sensitivity", "nodes", "graph"]}, {"paragraph_vector": [-134.796112, -3.075554], "paragraph_keywords": ["sensitivity", "node", "ranking", "graph"]}, {"paragraph_vector": [-135.231582, -2.409658], "paragraph_keywords": ["node", "index", "sensitivity", "ranking"]}, {"paragraph_vector": [-137.316177, -3.415951], "paragraph_keywords": ["changes", "perturbation", "ranking", "node"]}, {"paragraph_vector": [-135.696166, -5.00988], "paragraph_keywords": ["ranking", "node", "graph", "k"]}, {"paragraph_vector": [-138.077911, -4.619465], "paragraph_keywords": ["influence", "node", "nodes", "influenced"]}, {"paragraph_vector": [-19.821023, -54.468463], "paragraph_keywords": ["node", "graph", "nodes", "influenced"]}, {"paragraph_vector": [-136.554107, -4.562235], "paragraph_keywords": ["nodes", "ranking", "node", "search"]}, {"paragraph_vector": [-137.196395, -5.080852], "paragraph_keywords": ["analyst", "nodes", "constraints", "add"]}, {"paragraph_vector": [-138.636901, -6.441154], "paragraph_keywords": ["graph", "sensitivity", "ranking", "network"]}, {"paragraph_vector": [-136.489395, -3.754995], "paragraph_keywords": ["user", "nodes", "perturbation", "ranking"]}, {"paragraph_vector": [-138.746398, -3.623582], "paragraph_keywords": ["community", "nodes", "subreddit", "communities"]}, {"paragraph_vector": [-138.905364, -3.077139], "paragraph_keywords": ["r", "sensitivity", "subreddits", "sports"]}, {"paragraph_vector": [-137.845489, -4.227324], "paragraph_keywords": ["ranking", "graph", "rankings", "sports"]}, {"paragraph_vector": [-136.687377, -4.025061], "paragraph_keywords": ["ranking", "results", "sensitivity", "nodes"]}, {"paragraph_vector": [-137.410949, -2.201447], "paragraph_keywords": ["nodes", "blog", "ranking", "column"]}, {"paragraph_vector": [-137.354705, -3.662644], "paragraph_keywords": ["blogs", "ranking", "interview", "results"]}, {"paragraph_vector": [-136.63002, -3.661332], "paragraph_keywords": ["framework", "ranking", "graph", "sensitivity"]}, {"paragraph_vector": [-135.373245, -4.899053], "paragraph_keywords": ["sensitivity", "nodes", "-", "dataset"]}, {"paragraph_vector": [-135.33908, -4.62577], "paragraph_keywords": ["perturbation", "adding", "graph", "ieee"]}], "content": {}, "doi": "10.1109/TVCG.2020.3030333"}, {"uri": "80", "title": "Investigating Visual Analysis of Differentially Private Data", "timestamp": "2020", "rating": "0.0", "annotation": "", "tags": [], "authors": ["Dan Zhang", "Ali Sarvghad", "Gerome Miklau"], "summary": "Differential Privacy is an emerging privacy model with increasing popularity in many domains. It functions by adding carefully calibrated noise to data that blurs information about individuals while preserving overall statistics about the population. Theoretically, it is possible to produce robust privacy-preserving visualizations by plotting differentially private data. However, noise-induced data perturbations can alter visual patterns and impact the utility of a private visualization. We still know little about the challenges and opportunities for visual data exploration and analysis using private visualizations. As a first step towards filling this gap, we conducted a crowdsourced experiment, measuring participants\u2019 performance under three levels of privacy (high, low, non-private) for combinations of eight analysis tasks and four visualization types (bar chart, pie chart, line chart, scatter plot). Our findings show that for participants\u2019 accuracy for summary tasks (e.g., find clusters in data) was higher that value tasks (e.g., retrieve a certain value). We also found that under DP, pie chart and line chart offer similar or better accuracy than bar chart. In this work, we contribute the results of our empirical study, investigating the task-based effectiveness of basic private visualizations, a dichotomous model for defining and measuring user success in performing visual analysis tasks under DP, and a set of distribution metrics for tuning the injection to improve the utility of private visualizations.", "keywords": ["use", "histogram", "range", "user", "injection", "mechanism", "effectiveness", "cluster", "uncertainty", "utility", "participant", "al", "\u03b5", "level", "success", "value", "information", "accuracy", "approach", "question", "investigate", "work", "privacy", "task", "output", "algorithm", "noise", "input", "visualization", "answer", "bar", "based", "preserving", "summary", "rate", "data", "performing", "metric", "finding", "time", "et", "chart", "result", "performance", "laplace", "point", "given", "loss", "distribution", "budget", "find", "perturbation", "analysis", "example", "knowledge", "study"], "document_vector": [-83.890541, 7.755263], "paragraphs": [{"paragraph_vector": [-12.295332, 12.882567], "paragraph_keywords": ["data", "privacy", "analysis", "success"]}, {"paragraph_vector": [-15.705342, 11.908164], "paragraph_keywords": ["analysis", "data", "privacy", "user"]}, {"paragraph_vector": [-17.341964, 11.480438], "paragraph_keywords": ["noise", "tasks", "injection", "task"]}, {"paragraph_vector": [-13.939257, 12.862063], "paragraph_keywords": ["privacy", "work", "data", "algorithm"]}, {"paragraph_vector": [-14.337918, 11.27491], "paragraph_keywords": ["\u03b5", "output", "privacy", "query"]}, {"paragraph_vector": [-19.089767, 15.894677], "paragraph_keywords": ["function", "laplace", "\u03b5", "noise"]}, {"paragraph_vector": [-16.592136, 11.657401], "paragraph_keywords": ["visualization", "data", "algorithms", "privacy"]}, {"paragraph_vector": [-15.171721, 5.75795], "paragraph_keywords": ["privacy", "data", "visualization", "visualizations"]}, {"paragraph_vector": [-14.270013, 6.017838], "paragraph_keywords": ["privacy", "utility", "visualization", "data"]}, {"paragraph_vector": [-11.481895, 12.153301], "paragraph_keywords": ["data", "privacy", "uncertainty", "visualization"]}, {"paragraph_vector": [-13.15801, 14.630927], "paragraph_keywords": ["uncertainty", "data", "study", "information"]}, {"paragraph_vector": [-16.638605, 10.117458], "paragraph_keywords": ["privacy", "laplace", "utility", "data"]}, {"paragraph_vector": [-11.761708, 12.260029], "paragraph_keywords": ["tasks", "\u03b5", "visualizations", "noise"]}, {"paragraph_vector": [12.039245, 16.139497], "paragraph_keywords": ["visualization", "group", "asked", "participants"]}, {"paragraph_vector": [-15.005774, 12.21745], "paragraph_keywords": ["visualization", "study", "participant", "chart"]}, {"paragraph_vector": [40.231884, 61.466899], "paragraph_keywords": ["questions", "responses", "question", "worker"]}, {"paragraph_vector": [-11.853011, 11.413419], "paragraph_keywords": ["task", "accuracy", "data", "success"]}, {"paragraph_vector": [-13.036484, 13.414448], "paragraph_keywords": ["data", "tasks", "task", "value"]}, {"paragraph_vector": [-13.513905, 10.377901], "paragraph_keywords": ["perturbation", "task", "accuracy", "analysis"]}, {"paragraph_vector": [-14.058297, 11.562748], "paragraph_keywords": ["tasks", "noise", "privacy", "accuracy"]}, {"paragraph_vector": [-13.90684, 10.858473], "paragraph_keywords": ["noise", "distribution", "data", "injection"]}, {"paragraph_vector": [5.772156, 13.315008], "paragraph_keywords": ["point", "data", "cluster", "clustering"]}, {"paragraph_vector": [-13.341179, 12.65728], "paragraph_keywords": ["data", "distribution", "algorithm", "algorithms"]}, {"paragraph_vector": [-14.964354, 11.121852], "paragraph_keywords": ["accuracy", "task", "noise", "perturbation"]}, {"paragraph_vector": [-14.977282, 13.419114], "paragraph_keywords": ["privacy", "accuracy", "task", "queries"]}, {"paragraph_vector": [-15.636085, 8.531316], "paragraph_keywords": ["algorithms", "data", "privacy", "visualizations"]}, {"paragraph_vector": [-13.185476, 11.119516], "paragraph_keywords": ["metrics", "dp", "work", "privacy"]}, {"paragraph_vector": [-19.008529, 7.061144], "paragraph_keywords": ["visualization", "data", "privacy", "analysis"]}, {"paragraph_vector": [-20.615503, 16.131603], "paragraph_keywords": ["data", "privacy", "conference", "smith"]}, {"paragraph_vector": [-21.10095, 6.596008], "paragraph_keywords": ["data", "privacy", "visualization", "conference"]}, {"paragraph_vector": [-17.629989, 5.245535], "paragraph_keywords": ["visualization", "ieee", "data", "use"]}, {"paragraph_vector": [-17.059555, 8.912544], "paragraph_keywords": ["data", "hay", "miklau", "challenges"]}], "content": {}, "doi": "10.1109/TVCG.2020.3030389"}, {"uri": "81", "title": "LineSmooth: An Analytical Framework for Evaluating the Effectiveness of Smoothing Techniques on Line Charts", "timestamp": "2020", "rating": "0.0", "annotation": "", "tags": [], "authors": ["Paul Rosen", "Ghulam Jilani Quadri"], "summary": "We present a comprehensive framework for evaluating line chart smoothing methods under a variety of visual analytics tasks. Line charts are commonly used to visualize a series of data samples. When the number of samples is large, or the data are noisy, smoothing can be applied to make the signal more apparent. However, there are a wide variety of smoothing techniques available, and the effectiveness of each depends upon both nature of the data and the visual analytics task at hand. To date, the visualization community lacks a summary work for analyzing and classifying the various smoothing methods available. In this paper, we establish a framework, based on 8 measures of the line smoothing effectiveness tied to 8 low-level visual analytics tasks. We then analyze 12 methods coming from 4 commonly used classes of line chart smoothing\u2014rank filters, convolutional filters, frequency domain filters, and subsampling. The results show that while no method is ideal for all situations, certain methods, such as GAUSSIAN filters and TOPOLOGY-based subsampling, perform well in general. Other methods, such as low-pass CUTOFF filters and DOUGLAS-PEUCKER subsampling, perform well for specific visual analytics tasks. Almost as importantly, our framework demonstrates that several methods, including the commonly used UNIFORM subsampling, produce low-quality results, and should, therefore, be avoided, if possible.", "keywords": ["use", "filter", "subsampling", "rank", "sample", "user", "cutoff", "stock", "number", "frequency", "technique", "framework", "measure", "value", "fig", "output", "task", "difference", "input", "visualization", "gaussian", "case", "smoothing", "datasets", "peak", "data", "error", "window", "method", "performance", "chart", "result", "domain", "point", "line", "trend", "topology"], "document_vector": [-61.221202, -28.084047], "paragraphs": [{"paragraph_vector": [1.766284, 12.278068], "paragraph_keywords": ["data", "fig", "information", "charts"]}, {"paragraph_vector": [3.228804, 15.197901], "paragraph_keywords": ["smoothing", "line", "tasks", "technique"]}, {"paragraph_vector": [4.265705, 16.76472], "paragraph_keywords": ["charts", "line", "data", "tasks"]}, {"paragraph_vector": [113.37519, -26.209302], "paragraph_keywords": ["line", "charts", "data", "shown"]}, {"paragraph_vector": [4.471965, 14.935827], "paragraph_keywords": ["smoothing", "data", "techniques", "distortion"]}, {"paragraph_vector": [2.603675, 24.746131], "paragraph_keywords": ["window", "filter", "ieee", "input"]}, {"paragraph_vector": [0.796489, 30.290542], "paragraph_keywords": ["filter", "window", "data", "input"]}, {"paragraph_vector": [-0.572096, 32.919498], "paragraph_keywords": ["cutoff", "output", "filter", "domain"]}, {"paragraph_vector": [5.244246, 19.214246], "paragraph_keywords": ["points", "data", "input", "subsampling"]}, {"paragraph_vector": [4.49438, 16.048997], "paragraph_keywords": ["measures", "smoothing", "data", "difference"]}, {"paragraph_vector": [3.038752, 18.718395], "paragraph_keywords": ["data", "difference", "y", "peaks"]}, {"paragraph_vector": [3.921696, 18.088378], "paragraph_keywords": ["points", "data", "correlation", "relationship"]}, {"paragraph_vector": [3.484355, 16.068943], "paragraph_keywords": ["data", "value", "task", "values"]}, {"paragraph_vector": [2.082774, 1.087032], "paragraph_keywords": ["data", "peaks", "anomalies", "task"]}, {"paragraph_vector": [3.392772, 15.340153], "paragraph_keywords": ["values", "data", "task", "order"]}, {"paragraph_vector": [5.822052, 15.395069], "paragraph_keywords": ["smoothing", "tasks", "techniques", "apex"]}, {"paragraph_vector": [3.424745, 16.297058], "paragraph_keywords": ["smoothing", "ieee", "apex", "value"]}, {"paragraph_vector": [-0.43371, 17.352376], "paragraph_keywords": ["data", "rank", "datasets", "area"]}, {"paragraph_vector": [-21.560932, 70.173789], "paragraph_keywords": ["samples", "data", "climate", "ieee"]}, {"paragraph_vector": [1.096428, 17.41361], "paragraph_keywords": ["data", "fig", "rank", "techniques"]}, {"paragraph_vector": [1.446, 17.811229], "paragraph_keywords": ["topology", "techniques", "datasets", "use"]}, {"paragraph_vector": [2.08124, 15.182669], "paragraph_keywords": ["data", "method", "topology", "gaussian"]}, {"paragraph_vector": [5.191401, 17.855283], "paragraph_keywords": ["smoothing", "study", "line", "framework"]}, {"paragraph_vector": [141.924484, -5.10848], "paragraph_keywords": ["ieee", "science", "publication", "visualization"]}], "content": {}, "doi": "10.1109/TVCG.2020.3030353"}, {"uri": "82", "title": "MetroSets: Visualizing Sets as Metro Maps", "timestamp": "2020", "rating": "0.0", "annotation": "", "tags": [], "authors": ["Ben Jacobsen", "Markus Wallinger", "Stephen Kobourov", "Martin N\u00f6llenburg"], "summary": "We propose MetroSets, a new, flexible online tool for visualizing set systems using the metro map metaphor. We model a given set system as a hypergraph H = (V,S), consisting of a set V of vertices and a set S, which contains subsets of V called hyperedges. Our system then computes a metro map representation of H, where each hyperedge E in S corresponds to a metro line and each vertex corresponds to a metro station. Vertices that appear in two or more hyperedges are drawn as interchanges in the metro map, connecting the different sets. MetroSets is based on a modular 4-step pipeline which constructs and optimizes a path-based hypergraph support, which is then drawn and schematized using metro map layout algorithms. We propose and implement multiple algorithms for each step of the MetroSet pipeline and provide a functional prototype with easy-to-use preset configurations. Furthermore, using several real-world datasets, we perform an extensive quantitative evaluation of the impact of different pipeline stages on desirable properties of the generated maps, such as octolinearity, monotonicity, and edge uniformity.", "keywords": ["use", "property", "goal", "spring", "label", "element", "metaphor", "problem", "graph", "hypergraph", "tsp", "system", "schematization", "path", "number", "opt", "support", "station", "pipeline", "diagram", "one", "-", "crossing", "fig", "space", "drawing", "layout", "task", "algorithm", "vertex", "map", "e", "provide", "input", "edge", "visualization", "drawn", "based", "data", "hyperedge", "finding", "method", "time", "optimizing", "metrosets", "line", "design", "hyperedges", "set", "metro", "step"], "document_vector": [-175.206329, -41.674804], "paragraphs": [{"paragraph_vector": [40.799003, -72.622131], "paragraph_keywords": ["metro", "set", "sets", "e"]}, {"paragraph_vector": [30.530452, -65.430274], "paragraph_keywords": ["metrosets", "sets", "visualization", "metro"]}, {"paragraph_vector": [37.455757, -68.597465], "paragraph_keywords": ["sets", "set", "diagrams", "visualization"]}, {"paragraph_vector": [41.55698, -65.6529], "paragraph_keywords": ["set", "sets", "elements", "bubble"]}, {"paragraph_vector": [8.855531, -69.910476], "paragraph_keywords": ["set", "supports", "elements", "sets"]}, {"paragraph_vector": [52.852703, -60.601615], "paragraph_keywords": ["metro", "map", "layout", "maps"]}, {"paragraph_vector": [-59.592128, -47.311916], "paragraph_keywords": ["metro", "layout", "vertices", "set"]}, {"paragraph_vector": [25.773937, -76.072715], "paragraph_keywords": ["support", "metro", "design", "step"]}, {"paragraph_vector": [51.234039, -63.753944], "paragraph_keywords": ["stations", "lines", "pipeline", "metro"]}, {"paragraph_vector": [-10.84704, -79.76374], "paragraph_keywords": ["vertices", "hyperedges", "path", "opt"]}, {"paragraph_vector": [-36.186832, -83.059249], "paragraph_keywords": ["path", "opt", "vertices", "use"]}, {"paragraph_vector": [-17.699399, -80.503372], "paragraph_keywords": ["vertex", "property", "ones", "vertices"]}, {"paragraph_vector": [29.283243, -78.968093], "paragraph_keywords": ["vertices", "graph", "metro", "map"]}, {"paragraph_vector": [9.753619, -81.552314], "paragraph_keywords": ["vertices", "graph", "path", "edge"]}, {"paragraph_vector": [-22.266801, -79.920959], "paragraph_keywords": ["paths", "edge", "graph", "self"]}, {"paragraph_vector": [-19.878261, -87.137962], "paragraph_keywords": ["edge", "vertex", "error", "angle"]}, {"paragraph_vector": [-41.021041, -89.071563], "paragraph_keywords": ["spring", "stage", "lines", "apply"]}, {"paragraph_vector": [49.059425, -75.902687], "paragraph_keywords": ["terminator", "label", "line", "crossings"]}, {"paragraph_vector": [28.444826, -77.204933], "paragraph_keywords": ["labels", "algorithm", "labeling", "pipeline"]}, {"paragraph_vector": [15.834494, -69.708702], "paragraph_keywords": ["lines", "map", "algorithm", "use"]}, {"paragraph_vector": [177.765533, -68.846305], "paragraph_keywords": ["set", "tasks", "focus", "visualization"]}, {"paragraph_vector": [31.459693, -66.060974], "paragraph_keywords": ["union", "hyperedges", "mode", "vertices"]}, {"paragraph_vector": [51.928817, -56.022132], "paragraph_keywords": ["edge", "line", "monotonicity", "number"]}, {"paragraph_vector": [30.601419, -78.796302], "paragraph_keywords": ["hyperedges", "edge", "number", "design"]}, {"paragraph_vector": [33.40406, -63.67313], "paragraph_keywords": ["number", "ieee", "statistics", "test"]}, {"paragraph_vector": [23.964889, -52.790496], "paragraph_keywords": ["pipeline", "number", "vertices", "time"]}, {"paragraph_vector": [30.343149, -69.290802], "paragraph_keywords": ["data", "schematization", "ieee", "visualization"]}, {"paragraph_vector": [41.007034, -68.164955], "paragraph_keywords": ["metrosets", "step", "data", "system"]}, {"paragraph_vector": [14.206544, -57.52288], "paragraph_keywords": ["thank", "ieee", "use", "vienna"]}], "content": {}, "doi": "10.1109/TVCG.2020.3030431"}, {"uri": "83", "title": "Responsive Matrix Cells: A Focus+Context Approach for Exploring and Editing Multivariate Graphs", "timestamp": "2020", "rating": "0.0", "annotation": "", "tags": [], "authors": ["Tom Horak", "Philip Berger", "Heidrun Schumann", "Raimund Dachselt", "Christian Tominski"], "summary": "Matrix visualizations are a useful tool to provide a general overview of a graph\u2019s structure. For multivariate graphs, a remaining challenge is to cope with the attributes that are associated with nodes and edges. Addressing this challenge, we propose responsive matrix cells as a focus+context approach for embedding additional interactive views into a matrix. Responsive matrix cells are local zoomable regions of interest that provide auxiliary data exploration and editing facilities for multivariate graphs. They behave responsively by adapting their visual contents to the cell location, the available display space, and the user task. Responsive matrix cells enable users to reveal details about the graph, compare node and edge attributes, and edit data values directly in a matrix without resorting to external views or tools. We report the general design considerations for responsive matrix cells covering the visual and interactive means necessary to support a seamless data exploration and editing. Responsive matrix cells have been implemented in a web-based prototype based on which we demonstrate the utility of our approach. We describe a walk-through for the use case of analyzing a graph of soccer players and report on insights from a preliminary user feedback session.", "keywords": ["label", "rmc", "user", "cell", "graph", "node", "attribute", "embedded", "aspect", "player", "detail", "level", "object", "value", "information", "size", "-", "rmcs", "interaction", "color", "fig", "approach", "space", "unit", "similarity", "star", "work", "layout", "matrix", "mark", "zooming", "plot", "bar", "visualization", "edge", "based", "shown", "data", "view", "chart", "edit", "overview", "set", "analysis", "example", "analyst", "multivariate", "editing", "interest"], "document_vector": [135.979721, -31.156959], "paragraphs": [{"paragraph_vector": [127.326667, -60.542434], "paragraph_keywords": ["plants", "data", "nodes", "power"]}, {"paragraph_vector": [133.140609, -58.228683], "paragraph_keywords": ["matrix", "editing", "data", "views"]}, {"paragraph_vector": [131.630096, -59.309658], "paragraph_keywords": ["graph", "layout", "attributes", "graphs"]}, {"paragraph_vector": [125.463844, -59.043766], "paragraph_keywords": ["matrix", "techniques", "nodes", "lens"]}, {"paragraph_vector": [138.415817, -37.851474], "paragraph_keywords": ["visualization", "visualizations", "level", "data"]}, {"paragraph_vector": [131.392807, -58.375587], "paragraph_keywords": ["data", "editing", "node", "edit"]}, {"paragraph_vector": [128.747299, -57.724266], "paragraph_keywords": ["approach", "rmcs", "multivariate", "data"]}, {"paragraph_vector": [131.843338, -58.204055], "paragraph_keywords": ["users", "matrix", "details", "cells"]}, {"paragraph_vector": [125.659019, -59.45436], "paragraph_keywords": ["matrix", "nodes", "edges", "rmc"]}, {"paragraph_vector": [77.067512, -41.27412], "paragraph_keywords": ["group", "objects", "cell", "sub"]}, {"paragraph_vector": [127.168807, -41.796489], "paragraph_keywords": ["visualizations", "cells", "unit", "objects"]}, {"paragraph_vector": [127.436691, -46.211975], "paragraph_keywords": ["color", "details", "visualization", "background"]}, {"paragraph_vector": [113.229621, -48.774528], "paragraph_keywords": ["object", "size", "bar", "plots"]}, {"paragraph_vector": [108.602699, -47.358604], "paragraph_keywords": ["objects", "labels", "attribute", "comparison"]}, {"paragraph_vector": [103.594932, -48.436058], "paragraph_keywords": ["cells", "attribute", "plots", "aspect"]}, {"paragraph_vector": [106.219619, -63.272201], "paragraph_keywords": ["number", "aspects", "cells", "users"]}, {"paragraph_vector": [123.682746, -61.491817], "paragraph_keywords": ["rmcs", "visualizations", "use", "degree"]}, {"paragraph_vector": [122.353492, -53.928565], "paragraph_keywords": ["rmcs", "matrix", "embedded", "create"]}, {"paragraph_vector": [122.697624, -49.486511], "paragraph_keywords": ["rmcs", "details", "rmc", "zooming"]}, {"paragraph_vector": [125.385856, -52.079704], "paragraph_keywords": ["menu", "rmcs", "visualization", "details"]}, {"paragraph_vector": [129.96762, -58.290935], "paragraph_keywords": ["values", "editing", "value", "attribute"]}, {"paragraph_vector": [87.502738, -0.064436], "paragraph_keywords": ["data", "players", "editing", "soccer"]}, {"paragraph_vector": [87.082572, 10.668259], "paragraph_keywords": ["players", "analyst", "similarity", "cells"]}, {"paragraph_vector": [86.432586, 6.921528], "paragraph_keywords": ["rl", "lm", "analyst", "number"]}, {"paragraph_vector": [86.595108, 8.227824], "paragraph_keywords": ["analyst", "players", "matrix", "value"]}, {"paragraph_vector": [124.208984, -61.244632], "paragraph_keywords": ["participants", "labels", "approach", "visualization"]}, {"paragraph_vector": [138.952636, -45.528709], "paragraph_keywords": ["rmcs", "based", "visualization", "adaptations"]}, {"paragraph_vector": [159.095184, -40.71807], "paragraph_keywords": ["editing", "visualizations", "plots", "cells"]}, {"paragraph_vector": [131.653564, -58.273368], "paragraph_keywords": ["rmcs", "studies", "approach", "visualization"]}, {"paragraph_vector": [132.786026, -57.506412], "paragraph_keywords": ["matrix", "rmcs", "table", "approach"]}, {"paragraph_vector": [133.900115, -56.698032], "paragraph_keywords": ["multivariate", "details", "feedback", "visualizations"]}], "content": {}, "doi": "10.1109/TVCG.2020.3030415"}, {"uri": "84", "title": "Ray Tracing Structured AMR Data Using ExaBricks", "timestamp": "2020", "rating": "0.0", "annotation": "", "tags": [], "authors": ["Ingo Wald", "Stefan Zellmann", "Will Usher", "Nate Morrical", "Ulrich Lang", "Valerio Pascucci"], "summary": "Structured Adaptive Mesh Refinement (Structured AMR) enables simulations to adapt the domain resolution to save computation and storage, and has become one of the dominant data representations used by scientific simulations; however, efficiently rendering such data remains a challenge. We present an efficient approach for volumeand iso-surface ray tracing of Structured AMR data on GPU-equipped workstations, using a combination of two different data structures. Together, these data structures allow a ray tracing based renderer to quickly determine which segments along the ray need to be integrated and at what frequency, while also providing quick access to all data values required for a smooth sample reconstruction kernel. Our method makes use of the RTX ray tracing hardware for surface rendering, ray marching, space skipping, and adaptive sampling; and allows for interactive changes to the transfer function and implicit iso-surfacing thresholds. We demonstrate that our method achieves high performance with little memory overhead, enabling interactive high quality rendering of complex AMR data sets on individual GPU workstations.", "keywords": ["use", "region", "require", "sample", "cell", "computed", "ospray", "structure", "model", "hardware", "compute", "number", "tree", "support", "bvh", "reconstruction", "al", "memory", "level", "rtx", "boundary", "iso", "size", "k", "surface", "space", "approach", "work", "simulation", "rendering", "figure", "volume", "skipping", "data", "method", "et", "quality", "basis", "performance", "mesh", "opacity", "requires", "ray", "brick", "proposed", "sampling", "amr", "set"], "document_vector": [-83.555068, -55.904411], "paragraphs": [{"paragraph_vector": [53.42575, -15.031749], "paragraph_keywords": ["scale", "amr", "data", "university"]}, {"paragraph_vector": [52.644786, -13.56196], "paragraph_keywords": ["data", "amr", "cell", "reconstruction"]}, {"paragraph_vector": [52.022369, -15.541036], "paragraph_keywords": ["data", "amr", "method", "proposed"]}, {"paragraph_vector": [50.186557, -17.262847], "paragraph_keywords": ["amr", "rendering", "method", "sampling"]}, {"paragraph_vector": [51.92823, -15.816443], "paragraph_keywords": ["amr", "rendering", "interpolation", "support"]}, {"paragraph_vector": [49.098091, -16.554719], "paragraph_keywords": ["regions", "bricks", "data", "cells"]}, {"paragraph_vector": [51.035037, -15.535005], "paragraph_keywords": ["cells", "level", "amr", "use"]}, {"paragraph_vector": [50.402713, -15.227032], "paragraph_keywords": ["cells", "bricks", "amr", "node"]}, {"paragraph_vector": [51.341381, -16.297494], "paragraph_keywords": ["reconstruction", "sample", "cells", "basis"]}, {"paragraph_vector": [44.235942, -27.375162], "paragraph_keywords": ["brick", "region", "space", "supports"]}, {"paragraph_vector": [44.527343, -24.776702], "paragraph_keywords": ["region", "brick", "regions", "support"]}, {"paragraph_vector": [51.848758, -15.453671], "paragraph_keywords": ["region", "ray", "regions", "tree"]}, {"paragraph_vector": [54.648635, -12.173231], "paragraph_keywords": ["bvh", "region", "ray", "regions"]}, {"paragraph_vector": [48.930385, -17.133224], "paragraph_keywords": ["sampling", "use", "rate", "region"]}, {"paragraph_vector": [52.668384, -13.156383], "paragraph_keywords": ["sample", "opacity", "regions", "artifacts"]}, {"paragraph_vector": [50.254665, -16.346385], "paragraph_keywords": ["sample", "region", "gradients", "computed"]}, {"paragraph_vector": [52.481582, -15.491203], "paragraph_keywords": ["volume", "iso", "figure", "ray"]}, {"paragraph_vector": [56.404293, -16.476036], "paragraph_keywords": ["benchmarks", "surface", "computed", "iso"]}, {"paragraph_vector": [53.49887, -14.924536], "paragraph_keywords": ["simulation", "amr", "set", "data"]}, {"paragraph_vector": [54.291233, -14.9333], "paragraph_keywords": ["memory", "data", "models", "regions"]}, {"paragraph_vector": [53.166122, -14.466701], "paragraph_keywords": ["performance", "regions", "models", "rendering"]}, {"paragraph_vector": [49.549591, -15.238213], "paragraph_keywords": ["method", "reconstruction", "basis", "cell"]}, {"paragraph_vector": [54.813694, -15.028075], "paragraph_keywords": ["ospray", "method", "use", "rendering"]}, {"paragraph_vector": [52.466045, -17.381732], "paragraph_keywords": ["data", "bricks", "regions", "amr"]}, {"paragraph_vector": [53.768638, -14.474529], "paragraph_keywords": ["rendering", "amr", "integration", "implementation"]}, {"paragraph_vector": [51.255706, -17.317564], "paragraph_keywords": ["doe", "work", "ieee", "courtesy"]}], "content": {}, "doi": "10.1109/TVCG.2020.3030454"}, {"uri": "85", "title": "A Simple Pipeline for Coherent Grid Maps", "timestamp": "2020", "rating": "0.0", "annotation": "", "tags": [], "authors": ["Wouter Meulemans", "Max Sondag", "Bettina Speckmann"], "summary": "Grid maps are spatial arrangements of simple tiles (often squares or hexagons), each of which represents a spatial element. They are an established, effective way to show complex data per spatial element, using visual encodings within each tile ranging from simple coloring to nested small-multiples visualizations. An effective grid map is coherent with the underlying geographic space: the tiles maintain the contiguity, neighborhoods and identifiability of the corresponding spatial elements, while the grid map as a whole maintains the global shape of the input. Of particular importance are salient local features of the global shape which need to be represented by tiles assigned to the appropriate spatial elements. State-of-the-art techniques can adequately deal only with simple cases, such as close-to-uniform spatial distributions or global shapes that have few characteristic features. We introduce a simple fully-automated 3-step pipeline for computing coherent grid maps. Each step is a well-studied problem: shape decomposition based on salient features, tile-based Mosaic Cartograms, and point-set matching. Our pipeline is a seamless composition of existing techniques for these problems and results in high-quality grid maps. We provide an implementation, demonstrate the efficacy of our approach on various complex datasets, and compare it to the state-of-the-art.", "keywords": ["use", "element", "feature", "region", "municipality", "adjacency", "compute", "number", "decomposition", "pipeline", "cartogram", "shape", "cartograms", "tile", "part", "p", "fig", "arrangement", "section", "work", "uk", "map", "contiguity", "assignment", "direction", "input", "based", "case", "coherence", "data", "candidate", "time", "containing", "result", "cut", "set", "grid", "example", "topology", "step", "facet"], "document_vector": [-50.77494, -78.900535], "paragraphs": [{"paragraph_vector": [75.212471, -48.62218], "paragraph_keywords": ["data", "dimension", "map", "grid"]}, {"paragraph_vector": [60.271713, -50.210597], "paragraph_keywords": ["grid", "map", "elements", "space"]}, {"paragraph_vector": [53.021865, -51.888233], "paragraph_keywords": ["section", "grid", "shape", "elements"]}, {"paragraph_vector": [58.148353, -50.202819], "paragraph_keywords": ["topology", "tile", "facets", "elements"]}, {"paragraph_vector": [56.368747, -50.131233], "paragraph_keywords": ["shape", "tile", "map", "elements"]}, {"paragraph_vector": [54.283077, -50.82632], "paragraph_keywords": ["shape", "tiles", "facets", "contiguity"]}, {"paragraph_vector": [51.342914, -52.240531], "paragraph_keywords": ["grid", "shape", "example", "containing"]}, {"paragraph_vector": [54.646331, -49.217292], "paragraph_keywords": ["tile", "fig", "use", "density"]}, {"paragraph_vector": [50.131958, -49.879028], "paragraph_keywords": ["shape", "regions", "eroded", "ieee"]}, {"paragraph_vector": [53.562328, -51.216819], "paragraph_keywords": ["grid", "cartogram", "maps", "shape"]}, {"paragraph_vector": [53.373325, -52.138568], "paragraph_keywords": ["pipeline", "step", "shape", "compute"]}, {"paragraph_vector": [44.774063, -46.33654], "paragraph_keywords": ["shape", "decomposition", "cuts", "cut"]}, {"paragraph_vector": [52.83329, -52.605163], "paragraph_keywords": ["cuts", "candidate", "p", "cut"]}, {"paragraph_vector": [56.754692, -53.15995], "paragraph_keywords": ["cut", "cuts", "elements", "time"]}, {"paragraph_vector": [52.068195, -51.343574], "paragraph_keywords": ["parts", "tiles", "contiguity", "shape"]}, {"paragraph_vector": [58.752956, -53.474151], "paragraph_keywords": ["tiles", "step", "number", "adjacencies"]}, {"paragraph_vector": [55.246665, -49.866214], "paragraph_keywords": ["piece", "tile", "elements", "mainland"]}, {"paragraph_vector": [51.964229, -48.074993], "paragraph_keywords": ["results", "step", "pipeline", "set"]}, {"paragraph_vector": [55.936931, -52.010429], "paragraph_keywords": ["parts", "shape", "places", "dilation"]}, {"paragraph_vector": [58.626644, -49.410888], "paragraph_keywords": ["pipeline", "shape", "grid", "use"]}, {"paragraph_vector": [55.028236, -50.798339], "paragraph_keywords": ["seconds", "result", "ieee", "requires"]}, {"paragraph_vector": [56.711559, -49.797885], "paragraph_keywords": ["provinces", "shape", "province", "containing"]}, {"paragraph_vector": [55.312732, -50.726547], "paragraph_keywords": ["shape", "parts", "tile", "decomposition"]}, {"paragraph_vector": [53.282199, -51.29549], "paragraph_keywords": ["step", "results", "distortion", "cartogram"]}, {"paragraph_vector": [56.735233, -51.057117], "paragraph_keywords": ["ieee", "measure", "use", "scale"]}], "content": {}, "doi": "10.1109/TVCG.2020.3030407"}]